---
layout : post
title : ML 심화 1주차
categories : ML 심화
---

**퍼셉트론 : 가장 간단한 인공 신경망 구조. 인공 뉴런의 한 종류 (TLU 또는 LTU 등 인공 뉴런 기반)**

**퍼셉트론의 기본 구조**

(1) 입력과 가중치

![image](https://github.com/user-attachments/assets/ae0a582c-df51-49b4-9cfc-cc68f20dfcc3)

입력의 선형 함수 z 계산 후, 결과에 **계단 함수** 적용 (**헤비사이드 계단 함수**)

(2) 활성화 함수 (계단 함수)

![image](https://github.com/user-attachments/assets/94e1e0d1-259a-40c9-b112-e4986e9dfbbd)

z>=0 : 출력 1 (양성클래스) / z<0 : 출력 0 (음성 클래스)

**퍼셉트론의 학습 과정**
주어진 데이터에 대해 가중치를 조정하는 과정을 통해 학습 (퍼셉트론 학습 규칙 사용)
(1) 가중치 업데이트
예측값과 실제값이 다를 경우에만 가중치를 업데이트함

![image](https://github.com/user-attachments/assets/61d67fcb-c3f6-470a-810e-59139a98d1c2)

(2) 학습 알고리즘

-가중치 w와 편향 b를 초기화 

-데이터 샘플을 하나씩 가져와서 예측값 계산

-예측값과 실제값이 다르면 가중치 업데이트

-모든 데이터 반복하며 학습 진행

![image](https://github.com/user-attachments/assets/3fdd0648-2299-42f0-ac3a-0f9ec413ff45)

※ 각 출력 뉴런의 결정 경계 : 선형. 훈련 샘플이 선형적으로 구분될 수 있다면 알고리즘이 정답에 수렴함. (**퍼셉트론 수렴 이론**)

사이킷런 Perceptron 클래스를 통해 붓꽃 데이터셋을 적용하는 과정
```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris (as_frame = True)
X = iris.data[['petal length (cm)', 'petal width (cm)']].values
y = (iris.target == 0)

per_clf = Perceptron(random_state=42)
per_clf.fit(X,y)

X_new = [[2,0.5],[3,1]]
y_pred = per_clf.predict(X_new)
```
**퍼셉트론의 한계와 보완**

(1) 선형 분리 문제

단순한 선형 결정 경계 특성을 가진 퍼셉트론 ☞ XOR (배타적 논리합) 문제와 같은 **비선형 문제** 해결 불가

(2) 다층 퍼셉트론 (MLP) 등장 ☞ 복잡한 문제 해결 가능

(다중 퍼셉트론이 XOR을 해결하는 과정)

![image](https://github.com/user-attachments/assets/2e3ccd5d-e236-4d3e-8315-dfaf7e9161bc)

**역전파** : 오차를 출력층에서 입력층으로 전파하여 가중치를 조정하는 알고리즘 (MLP 및 신경망 학습의 핵심 알고리즘. 경사 하강법 이용하여 가중치 최적화)

**후진 모드 자동 미분 & 경사 하강법을 결합한 알고리즘**

(1) 역전파가 필요한 이유

은닉층이 존재하는 MLP의 특성으로 인해 직접적인 오차 계산 어려움 ☞ **역전파를 통해 출력층에서 발생한 오차를 은닉층 바향으로 거꾸로 전파하면서 가중치 업데이트**

(2) 역전파의 주요 과정

▶ 입력값 받아 순차적으로 예측을 만듦 (정방향 계산)

▶ 역방향으로 각 층 거치면서 각 연결이 오차에 기여한 정도 측정 (역방향 계산)

▶ 오차 감소를 위해 가중치와 편향 조정 (경사 하강법)

***※ 신경망으로 무엇을 할 수 있는가? : 회귀, 분류 등***

**회귀를 위한 다층 퍼셉트론**

▶ 입력층 - 은닉층 - 출력층 거쳐 연속적인 값 예측

▶ 출력층에서 활성화 함수 사용 X (연속적이며 자유로운 값 보장) 

▶ 손실 함수로 MSE 사용. 훈련 세트에 이상치가 많은 경우 평균 절대 오차 대신 사용 또는 두 가지를 조합한 **후버 손실** 사용 가능

사이킷런 MLPRegressor 클래스를 통해 캘리포니아 주택 데이터셋 훈련
```python
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler 

housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state = 42)

mlp_reg = MLPRegressor(hidden_layer_sizes=[50,50,50],random_state=42)
pipeline = make_pipeline(StandardScaler(), mlp_reg)
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_valid)
mse = mean_squared_error(y_valid, y_pred) # 약 0.505 랜덤 포레스트 분류기 결괏값과 유사
```

**분류를 위한 다층 퍼셉트론**

▶ 입력층 - 은닉층 - 출력층 거쳐 클래스 확률 예측

▶ 출력층에서 활성화 함수 (Sigmoid, Softmax) 적용하여 분류 문제를 해결

이진분류 : 시그모이드 활성화 함수 사용. 다중 분류 : 소프트맥스 활성화 함수

▶ 손실함수로 크로스 엔트로피 사용. 역전파를 통해 가중치 업데이트.

※ 회귀에서 출력층에 활성화 함수를 사용하지 않는 이유 : 출력값이 연속적인 실수 값이므로 특정 범위로 제한하면 안 됨. 분류는 특정 범위 (0~1)로 변환하여 확률 형태로 해석할 수 있어야 함. (가장 높은 확률 가진 클래스 선택)

**케라스로 다층 퍼셉트론 구현하기**

케라스 : 모든 종류의 신경망을 쉽게 만들고 훈련, 평가, 실행할 수 있는 텐서플로의 고수준 딥러닝 API





