---
layout: post
title:  "01. 분류"
date:   2025-01-19
categories: ['ML기초']
---
# 쿠다 ML 기초 1주차

이 장에서는 분류 시스템을 집중적으로 다루어 보려고 한다. 

**(사실대로 말하면 사이킷런에 있는 데이터를 불러올 수 없어서 교재에 나온 코드를 보완하여 정리하려고 한다.)**
```python
import sys
assert sys.version_info >= (3, 7)
import matplotlib.pyplot as plt
plt.rc('font', size=14)
plt.rc('axes', labelsize=14, titlesize=14)
plt.rc('legend', fontsize=14)
plt.rc('xtick', labelsize=10)
plt.rc('ytick', labelsize=10)
from pathlib import Path

IMAGES_PATH = Path() / "images" / "classification"
IMAGES_PATH.mkdir(parents=True, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = IMAGES_PATH / f"{fig_id}.{fig_extension}"
    #images/classification 폴더가 없다면 이 폴더를 만들고 고해상도 이미지 저장을 위해 노트북에서 사용할 save_fig() 함수를 정의한다.
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)
```

## 3.1 MNIST

MNIST 데이터셋이란 고등학생과 미국 인구 조사국 직원들이 손으로 쓴 70000개의 작은 숫자 이미지이다. 각 이미지에는 어떤 숫자를 나타내는지 레이블이 되어있고, 머신러닝 학습을 위한 유용한 데이터로 활용된다.
이는 사이킷런에서 제공하는 여러 헬퍼 함수를 사용하여 잘 알려진 데이터셋을 내려받을 수 있다.
다음은 OpenML.org에서 MNIST 데이터셋을 내려받는 코드이다.

```python
import tensorflow as tf
import numpy as np
#사이킷런에 있는 데이터 대신 텐서플로우의 숫자 분류 데이터를 쓰려고 한다.
(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

x_all = (np.concatenate([x_train, x_test], axis=0))
y = np.concatenate([y_train, y_test], axis=0)

X = x_all.reshape(x_all.shape[0], -1)

print("전체 데이터 (2D 변환) 크기:", X.shape)
print("전체 라벨 크기: ", y.shape)
```

X와 y의 데이터 배열을 살펴보면 array([0., 0., ..., 0., 0., 0.], ...)과 같은 형태가 나온다. 이게 뭘 의미할까?

각 이미지에는 784개의 특성이 있다. 이미지가 28X28 픽셀이기 때문이다.

각각의 특성은 단순히 0(흰색)부터 255(검은색)까지의 픽셀 강도를 나타낸다.

X[0] 데이터인 숫자 5를 출력해보자.

```python
import matplotlib.pyplot as plt

def plot_digit(image_data):
    image = image_data.reshape(28, 28)
    plt.imshow(image, cmap="binary")
    plt.axis("off")

some_digit = X[0]
plot_digit(some_digit)
save_fig("some_digit_plot")  # 추가 코드
plt.show()
```

![some_digit_plot](https://github.com/user-attachments/assets/61d1f72c-db35-4df6-b0c3-3534745d6c58)

이 그림은 숫자 5로 보인다. 실제 레이블을 확인하면

```python
print (y[0])
```
이에 대한 결괏값이 5로 출력됨을 알 수 있다.

이번에는 이미지 샘플을 더 확인해보자.
```python
plt.figure(figsize=(9, 9))
for idx, image_data in enumerate(X[:100]):
    plt.subplot(10, 10, idx + 1)
    plot_digit(image_data)
plt.subplots_adjust(wspace=0, hspace=0)
save_fig("more_digits_plot", tight_layout=False)
plt.show()
```
이에 대한 결괏값은 다음과 같다.

![download](https://github.com/user-attachments/assets/8d8067ea-5fe7-4082-b7c5-0420ec82f41d)

**데이터 섞기는 모델 학습에서 매우 중요한 과정이다.** 데이터를 섞지 않으면 훈련 세트와 검증 세트에 특정 클래스(레이블)가 편향될 가능성이 있다. 예를 들어, MNIST 데이터셋에서 0-9 숫자의 샘플이 순차적으로 나열되어 있다면 초반 샘플에는 주로 0,1이 많고 후반에는 8,9가 몰려 있을 수 있다. 따라서 교차 검증에서 각 폴드에 모든 클래스가 골고루 포함되도록 데이터를 섞어 공정성을 확보해야 한다. 또한, 데이터를 섞지 않고 폴드를 나눈다면 특정 폴드에 어떤 클래스가 전혀 포함되어 있지 않아 모델이 학습할 기회를 갖지 못해 부정확한 결과를 낼 수 있다. 
***그러므로, 데이터(데이터셋)를 섞어 클래스 분포를 균일하게 만들고 훈련 안정성을 증가시켜야 한다.***

MNIST 데이터에서는 훈련 세트 (앞쪽 60000개 이미지)와 테스트 세트 (뒤쪽 10000개 이미지)로 구성되어 있다. 
```python
X_train, X_test, y_train, y_test = X[:60000], X[60000:], y[:60000], y[60000:]
```

|용어|용도|
|---|---|
|훈련세트|모델을 학습시키기 위해 사용|
|테스트 세트|모델이 학습하지 않은 데이터로 성능 평가|

그러나 모든 데이터가 이렇게 나누어져 있는 것은 아니기에, 데이터 조사 전에 항상 테스트 세트를 만들고 따로 떼어놓아야 한다. 그리하여 모델이 새로운 데이터에 대해 얼마나 일반화할 수 있는지 평가 가능하도록 만들어야 한다.

## 3.2 이진 분류기 훈련
**이진 분류기 훈련은 True와 False, 0과1 등을 구분할 수 있는 분류기이다.** 분류 작업을 위해 타깃 벡터를 우선 만들고, 분류 모델을 선택하여 훈련을 시켜보려고 한다. **확률적 경사 하강법(SGD)은 이진 분류기를 학습시키는 데 사용되는 최적화 도구이다.**

다음 코드를 통해 살펴보자.
```python
y_train_5 = (y_train == '5')
y_test_5 = (y_test == '5')
from sklearn.linear_model import SGDClassifier
sgd_clf = SGDClassifier(random_state = 42)
sgd_clf.fit(X_train, y_train_5)
# 괄호 안의 첫 번째 인자는 모든 훈련 데이터의 입력(특징 데이터)를 의미한다. 두 번째 인자는 레이블 데이터로, 여기서는 숫자 5에 해당하는 샘플만을 골라내어 T/F로 구성된 이진 레이블 데이터이다.
# 모델은 X_train의 각 샘플을 보고 그것이 '5'인지 아닌지 (y_train_5)를 학습한다.
'''
![image](https://github.com/user-attachments/assets/784a1d66-7184-48ef-b293-cd06251279d7)

이제 이 모델을 사용하여 숫자 5의 이미지를 감지해보자.
```python
sgd_clf.predict([some_digit])
```
![image](https://github.com/user-attachments/assets/74ea1a55-5539-4a5d-ab96-81b198d5aef9)

분류기는 이 이미지가 5를 나타낸다고 추측하였다.

## 3.3 성능 측정
### 3.3.1 교차 검증을 사용한 정확도 측정
cross_val_score () 함수로 폴드가 3개인 k-폴드 교차 검증을 사용해 SGDClassifier 모델을 평가해보려고 한다.

k-폴드 교차 검증 : 훈련 세트를 k개의 폴드로 나누고, 평가를 위해 매번 다른 폴드를 떼어놓고 모델을 k번 훈련함
```python
from sklearn.model_selection import cross_val_score
cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring="accuracy")
```
이에 대한 결괏값은 0.90 이상이다.

가끔 교차 검증 과정을 더 많이 제어해야 할 때는 교차 검증 기능을 직접 구현하면 된다.
```python
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

skfolds = StratifiedKFold(n_splits=3)  # 데이터셋이 미리 섞여 있지 않다면
                                       # shuffle=True를 추가하세요.
for train_index, test_index in skfolds.split(X_train, y_train_5):
    clone_clf = clone(sgd_clf)
    X_train_folds = X_train[train_index]
    y_train_folds = y_train_5[train_index]
    X_test_fold = X_train[test_index]
    y_test_fold = y_train_5[test_index]

    clone_clf.fit(X_train_folds, y_train_folds)
    y_pred = clone_clf.predict(X_test_fold)
    n_correct = sum(y_pred == y_test_fold)
    print(n_correct / len(y_pred))
```
이렇게 해도 결괏값이 95% 이상이 된다. 

이번에는 가장 많이 등장하는 클래스 (이 자료에서는 음성 클래스, 즉 '5 아님')로 분류하는 더미 분류기를 만들어 비교해보자.
```python
from sklearn.dummy import DummyClassifier

dummy_clf = DummyClassifier()
dummy_clf.fit(X_train, y_train_5)
print(any(dummy_clf.predict(X_train)))

cross_val_score(dummy_clf, X_train, y_train_5, cv=3, scoring="accuracy")
```
이 역시 결과가 90% 이상이 나온다.

위 과정은 분류기의 성능 측정 지표로 정확성을 지정한 것을 나타낸다. 그러나, 정확성을 지표로 사용하면 문제가 생긴다.

예를 들어보자. 특정 집단 중 대다수가 암에 걸리지 않은 환자이고, 소수가 암에 걸린 환자인 데이터가 존재한다고 가정한다. 그렇다면, 정확도를 기준으로 했을 때 대다수에 초점을 맞춘 예측값은 암에 걸린 환자를 반영하지 못하는 불균형이 생길 수 있다. 

그렇기에, **분류기의 성능을 평가하는 데 더 좋은 방법은 오차 행렬을 조사하는 것이다.**

### 3.3.2 오차 행렬
오차 행렬의 기본 아이디어는 모든 A/B 쌍에 대해 클래스 A의 샘플이 클래스 B로 분류된 횟수를 세는 것이다.\

예를 들어보자. 실제 레이블이 [5,0,0,5,0,5,0,5,5,0] 이고 예측 레이블이 [5,0,0,5,0,5,0,5,5,0]이라고 가정해보자.

이진 분류에는 (숫자 5 or not) 2개의 클래스가 있다. Positive Class (숫자 5), Negatie Class (숫자 5가 아님)

오차 행렬은 다음과 같이 구성된다. 

|실제/예측|숫자 5(P)|숫자 5 아님(N)|
|---|---|---|
|숫자 5(P)|True Positive|False Negative(FN)|
|숫자 5 아님(N)|False Positive|True Negative|

TP : 실제 숫자 5인데, 모델도 5로 예측한 경우 => TP = 3
FN : 실제 숫자 5인데, 모델이 5가 아니라고 예측한 경우 => FN = 1
FP : 실제 숫자 5가 아닌데, 모델이 5라고 예측한 경우 => FP = 1
TN : 실제 숫자 5가 아니고, 모델도 5가 아니라고 예측한 경우 => TN = 4

이를 통해 정확도, 정밀도, 재현율 등 성능 지표를 계산하는 것을 의미한다.

위의 예시처럼 오차 행렬을 만들려면 실제 타깃과 비교할 수 있도록 예측값을 만들어야 한다. cross_val_predict() 함수를 사용하여 예측을 만들어보자.

```python
from sklearn.model_selection import cross_val_predict
y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=3)
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_train_5, y_train_pred)
```
이에 대한 결괏값은 다음과 같다.

![image](https://github.com/user-attachments/assets/e5e86621-a967-4d9c-850d-80a4c30566e0)

오차 행렬의 행은 실제 클래스를, 열은 예측 클래스를 나타낸다. 이 행렬의 1행은 '5 아님' 이미지 (음성 클래스)에 대해 53892개를 '5 아님'으로 정확히 분류했고 (TN), 687개는 5라고 잘못 분류했다. (FP 또는 1종 오류) 두 번째 행은 5 이미지에 대해 1891개를 5 아님으로 잘못 분류했고 (FN 또는 2종 오류) 나머지 3230개를 정확히 5라고 분류했다. (TP)

완벽한 분류기라면 진짜 양성(TP)과 진짜 음성 (TN)만 가지고 있을 것이므로 오차 행렬의 주대각선만 0이 아닌 값이 된다.

```python
y_train_perfect_predictions = y_train_5  # 완벽한 분류기일 경우
confusion_matrix(y_train_5, y_train_perfect_predictions)
```
이에 대한 결괏값은 다음과 같다.

![image](https://github.com/user-attachments/assets/b5106812-46e0-4024-a8c1-351ffe64e171)

오차 행렬의 정보들 중 더 요약된 지표를 알아보자.

**정밀도 : 양성 예측의 정확도**
정밀도 = TP/TP+FP (TP : 진짜 양성의 수, FP : 거짓 양성의 수)

정밀도 100% 만드는 방법은 간단하다. 제일 확신이 높은 샘플에 대해 양성 예측을 하고 나머지는 모두 음성 예측을 하는 분류기를 만들면 된다. (의미 없음)
정밀도는 재현율이라는 또 다른 지표와 같이 사용하는 것이 일반적이다.

**재현율 : 분류기가 정확하게 감지한 양성 샘플의 비율 (=민감도, 진짜 양성 비율(TPR))**
재현율 = TP/TP+FN (FN : 거짓 음성의 수)

### 3.3.3 정밀도와 재현율
사이킷런은 정밀도와 재현율을 포함하여 분류기의 지표를 계산하는 여러 함수를 제공한다.
이를 코드로 구현해보자.
```python
from sklearn.metrics import precision_score, recall_score
precision_score (y_train_5, y_train_pred) # == 3530 / (687 + 3530)
# 정밀도 점수는 위에서 5라고 예측한 것들 중에서 진짜 양성인 비율을 따지려는 것
recall_score (y_train_5, y_train_pred) # 3530 / (1891 + 3530)
# 재현율 점수는 5인 데이터들 중 얼마나 진짜 양성을 재현했는지를 따지려는 것
```
이를 돌려보면 각각 83%의 정밀도와 65%의 재현율이라는 결과가 나온다.

정밀도와 재현율을 F1점수라고 하는 하나의 숫자로 만들면 편리하다.
특히 두 분류기를 비교할 때 그렇다. 예를 들어, 분류기 1은 정밀도가 높은 모델이고, 분류기 2는 재현율이 높은 모델이라고 가정하자. 그럼 F1 점수를 통해 더 균형 잡힌 성능을 보이는 분류기가 무엇인지 확인 가능하다. F1 점수가 도대체 뭘까?

**F1 score : 정밀도와 재현율의 조화 평균**
![image](https://github.com/user-attachments/assets/05425ab4-d1b4-406f-a09a-7a5a4b0ab92d)

우리는 조화 평균 공식이 위와 같다는 것을 알고 있다. x자리에 정밀도를, y 자리에 재현율을 집어 넣어보자.

![image](https://github.com/user-attachments/assets/22c1d51c-4a79-4d75-87a4-d6c3c81e6dd6)

오차행렬의 구성 요소를 이용하여 정리하면 다음과 같다.

![image](https://github.com/user-attachments/assets/156f36e0-ec4a-4723-9225-635aa1fdf4b8)

자, 이제 f1_score() 함수를 이용하여 점수를 계산해보자.
```python
from sklearn.metrics import fl_score
fl_score (y_train_5, y_train_pred)
```
이 결과 73%가 나온다. 정밀도와 재현율이 비슷한 분류기에서는 위의 F1 점수가 높다. 하지만 이게 과연 바람직할까?
아니다. 상황에 따라 정밀도가 중요할 수도 있고, 재현율이 중요할 수도 있다. 
정밀도와 재현율 모두 얻을 수는 없다. 정밀도를 올리면 재현율이 줄고 반대도 마찬가지다. 이를 **정밀도/재현율 트레이드오프**라고 한다.

###3.3.4 정밀도/재현율 트레이드오프
**분류기란, 데이터를 입력받아 특정 클래스(Label)에 속하는지 예측하는 모델이다. 예를 들어, 이메일이 스팸인지 아닌지, 숫자 이미지를 0-9중 하나로 분류하는 작업 등이 있다.** SGDClassifier으로 분류를 어떻게 결정하는지 살펴보며 이를 이해해보자.이 분류기는 **결정함수**를 사용하여 각 샘플의 점수를 계산한다. 즉, 점수를 기반으로 데이터가 특정 클래스에 속할 확률을 예측하거나, 점수가 0보다 큰지 작은지로 클래스(양성/음성)을 판별한다. 점수가 임곗값보다 크면 샘플을 양성 클래스에 할당하고 그렇지 않으면 음성 클래스에 할당한다.

![image](https://github.com/user-attachments/assets/c7a76317-7cbf-4080-be4b-88e3805979d1)

이처럼 가장 낮은 점수부터 가장 높은 점수까지 몇 개의 숫자를 나열했다. **결정 임곗값이 가운데 화살표라고 가정해보자.** 임곗값 오른쪽에 4개의 TP와 하나의 FP가 있다. 그렇기에 임곗값에서 정밀도는 4/5인 80%이다. 하지만 실제 숫자 5는 6개이고 분류기는 4개만 감지했으므로, 재현율은 4/6인 67%이다. **임곗값을 높여 오른쪽 화살표로 옮겨보자.** 이렇게 되면 TP가 TN이 되어 정밀도가 높아진다. 3/3인 100%가 된다. 하지만 TP 하나가 FN이 되었으므로 3/6인 50%가 재현율이 된다. 이런식으로 임곗값을 높이고 내리면 재현율과 정밀도에 변화가 생긴다.

사이킷런에서 임곗값을 직접 지정할 수는 없지만 예측에 사용한 점수는 확인할 수 있다. 분류기의 predict() 메소드 대신 decision_function() 메소드를 호출하면 각 샘플의 점수를 얻을 수 있다. 이 점수를 기반으로 원하는 임곗값을 정해 예측을 만들 수 있다.
```python
y_scores = sgd_clf.decision_function([some_digit])
# y_scores는 array([2164.22030239])로 출력된다.
threshold = 0
y_some_digit_pred = (y_scores > threshold)
```
위 코드에서는 분류기의 임곗값이 0이므로 predict() 메소드와 같은 결과를 반환한다. (True) 이번엔 임곗값을 높여보자.
```python
threshold = 3000
y_some_digit_pred = (y_scores > threshold)
```
이렇게 되면 False가 나온다. 즉, 재현율이 줄어든다. 

적절한 임곗값을 어떻게 정할 수 있을까? 먼저 cross_val_predict () 함수를 이용하여 훈련 세트에 있는 모든 샘플의 점수를 구해야 한다. 하지만 이번에는 '교차 검증을 통해 예측 결과가 아닌 결정 점수를 반환하도록 지정해야 한다.
**결정점수 : 샘플이 양성 클래스에 속할 가능성을 나타내는 점수 (양수일수록 양성 가능성 높고, 음수일수록 음성 가능성 높음)**
```y_scores = cross_val_predict (sgd_clf, X-train, y_train_5, cv=3, method = 'decision_function')
# method 부분은 결정 점수를 반환하라는 뜻이다.
# y_scores는 각 샘플에 대해 분류기가 계산한 결정 점수를 담고 있다. 
```
이 점수로 precision_recall_curve() 함수를 사용하여 결정 점수에 따라 가능한 모든 임곗값에 대해 정밀도와 재현율을 계산할 수 있다.
이 함수는 임곗값을 조정하면서 각각의 정밀도와 재현율 값을 반환한다.
```python
from sklearn.metrics import precision_recall_curve
precisions, recalls, thresholds = precision_recall_curve(y_train_5, y_scores)
```
**precisions: 각 임곗값에서의 정밀도 값 리스트.**
**recalls: 각 임곗값에서의 재현율 값 리스트.**
**thresholds: 임곗값 리스트.**

결괏값 중 thresholds와 precisions를 살펴보면 다음과 같다.

(array([-146348.56726174, -142300.00705404, -137588.97581744, ...,
          38871.26391927,   42216.05562787,   49441.43765905]),
 array([0.09035   , 0.09035151, 0.09035301, ..., 1.        , 1.        ,
        1.        ]))

이제 이를 바탕으로 임곗값의 함수로 정밀도와 재현율을 시각화해보자. 임곗값이 3000일때의 그래프를 그려보려고 한다.
```python
plt.figure(figsize=(8, 4))  # 추가 코드
plt.plot(thresholds, precisions[:-1], "b--", label="Precision", linewidth=2)
plt.plot(thresholds, recalls[:-1], "g-", label="Recall", linewidth=2)
plt.vlines(threshold, 0, 1.0, "k", "dotted", label="threshold")

# 추가 코드 – 그림 3–5를 그리고 저장합니다
idx = (thresholds >= threshold).argmax()  # 첫 번째 index ≥ threshold
plt.plot(thresholds[idx], precisions[idx], "bo")
plt.plot(thresholds[idx], recalls[idx], "go")
plt.axis([-50000, 50000, 0, 1])
plt.grid()
plt.xlabel("Threshold")
plt.legend(loc="center right")
save_fig("precision_recall_vs_threshold_plot")

plt.show()
```
이에 대한 결괏값은 다음과 같다.
![download](https://github.com/user-attachments/assets/bfbd1ef2-582f-4bd2-801e-4faa86fe1796)

정밀도 곡선이 왜 재현율 곡선보다 울퉁불퉁할까? 이는 임곗값을 올리더라도 정밀도가 낮아질 때가 가끔 있기 때문이다. 
이 임곗값에서 정밀도는 약 90%이고, 재현율은 약 50%이다. 좋은 정밀도/재현율 트레이드오프를 선택하는 다른 방법은 재현율에 대한 정밀도 곡선을 그리는 것이다. 살펴보자.
```python
import matplotlib.patches as patches  # 추가 코드 – 구부러진 화살표를 그리기 위해서

plt.figure(figsize=(6, 5))  # 추가 코드

plt.plot(recalls, precisions, linewidth=2, label="Precision/Recall curve")

# extra code – just beautifies and saves Figure 3–6
plt.plot([recalls[idx], recalls[idx]], [0., precisions[idx]], "k:")
plt.plot([0.0, recalls[idx]], [precisions[idx], precisions[idx]], "k:")
plt.plot([recalls[idx]], [precisions[idx]], "ko",
         label="Point at threshold 3,000")
plt.gca().add_patch(patches.FancyArrowPatch(
    (0.79, 0.60), (0.61, 0.78),
    connectionstyle="arc3,rad=.2",
    arrowstyle="Simple, tail_width=1.5, head_width=8, head_length=10",
    color="#444444"))
plt.text(0.56, 0.62, "Higher\nthreshold", color="#333333")
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.axis([0, 1, 0, 1])
plt.grid()
plt.legend(loc="lower left")
save_fig("precision_vs_recall_plot")

plt.show()
```
![download](https://github.com/user-attachments/assets/65fcb33e-49bb-4f12-a3c4-86b85434368b)

재현율 80% 근처에서 정밀도가 급격하게 줄어들기 시작한다. 이 하강점 직전을 정밀도/재현율 트레이드오프로 선택하는 것이 좋다. (ex : 재현율 60%인 지점, 프로젝트에 따라 상이)

**정밀도, 재현율과 임곗값의 관계**
임곗값이 높아질수록:
모델이 양성으로 예측하는 데이터가 줄어들기 때문에 정밀도는 증가하지만, 재현율은 감소

임곗값이 낮아질수록:
모델이 더 많은 데이터를 양성으로 예측하므로 재현율은 증가하지만, 정밀도는 감소

그렇다면, 정밀도 90%를 달성하는 유용한 지점이 어디인지를 알아보자. 즉, 정밀도가 최소 90%가 되는 가장 낮은 임곗값을 찾는 것이다. (재현율을 최대한으로 맞춰보자.) 이를 위해 넘파이 배열의 argmax(0 메소드를 사용한다. 이는 최댓값의 첫 번째 인덱스를 반환한다. (첫 번째 True)
```python
idx_for_90_precision = (precisions >= 0.90).argmax()
threshold_for_90_precision = thresholds[idx_for_90_precision]
threshold_for_90_precision
```
결과는 3370.019... 

이번에는 훈련 세트에 대한 에측을 만들고, 이 예측에 대한 정밀도와 재현율을 확인해보자.
```python
y_train_pred_90 = (y_scores >= threshold_for_90_precision)
precision_score(y_train_5, y_train_pred_90)
recall_at_90_precision = recall_score(y_train_5, y
_train_pred_90)
```
정밀도와 재현율은 각각 90%, 48%가 나온다.
***임곗값을 충분히 크게 지정하면 거의 모든 정밀도의 분류기를 손쉽게 만들 수 있다. 그러나 재현율이 너무 낮다면 높은 정밀도의 분류기는 유용하지 않다.*** 48%? 썩 좋은 값은 아니다.

###3.3.5 ROC 곡선
ROC 곡선은 거짓 양성 비율(FPR)에 대한 진짜 양성 비율(TPR)의 곡선이다. 개념부터 살펴보자.
|용어|정의|식|
|---|---|---|
|FPR|음성을 얼마나 자주 잘못 양성으로 예측했는가|![image](https://github.com/user-attachments/assets/76a10b94-f0ae-4e5b-95c6-f83ad74db42c)|
|TPR|양성 데이터를 얼마나 잘 탐지했는가 (재현율)|![image](https://github.com/user-attachments/assets/71d23232-7990-4005-a3a0-56ced1ada026)|




