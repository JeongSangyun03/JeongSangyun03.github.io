---
layout : posts
title : 쿠다 ML 심화 6주차
categories : 쿠다 ML 심화
---
# 13. 텍스트 유사성 측정
본 장에서는 두 텍스트의 유사도를 측정하는 기본적인 NLP 문제에 대하여 다루고자 한다.
## 13.1 간단한 텍스트 비교
3개의 텍스트가 주어졌다고 가정할 때, 텍스트 간 차이를 정량화 해야 함
```python
text1 = 'She sells seashells by the seashore.'
text2 = '"Seashells! The seashells are on sale! By the seashore."'
text3 = 'She sells 3 seashells to John, who lives by the lake.'
```

**자카드 유사도** : 두 텍스트의 유사성을 평가하는 간단한 지표 (전체 단어 수에 대한 교집합 비율)

**자카드 유사도의 작동 방식** :
1. 두 텍스트가 주어졌을 때 각 텍스트에서 단어 리스트 추출
2. 텍스트 간 공유되는 고유 단어 수를 셈
3. 공유 단어 수를 두 텍스트의 총 고유 단어 수로 나눔

![image](https://github.com/user-attachments/assets/067336d0-8919-4465-9b35-6ec04fcf3ead)

※ 그림에서 자카드 유사도 : 4/9

### 13.1.1 자카드 유사도 탐색
자카드 유사도가 텍스트 유사성을 합리적으로 측정하는 척도인 이유

▶ 텍스트 간 겹치는 단어와 그렇지 않은 단어 함께 고려

▶ 항상 0과 1 사이로 표현됨 + 해석 용이 (0 : 공유 단어 無 | 0.5 : 공유 단어 半 | 1 : 모든 단어 공유)

### 13.1.2 단어를 숫자 값으로 바꾸기
**텍스트의 벡터화** : 딕셔너리에 각 고유 단어 정보 저장 후, 어휘가 주어지면 1차원 숫자 배열인 벡터로 변환 (해당 단어 텍스트가 존재하면 벡터느 1로, 존재하지 않으면 0으로 변환됨)

벡터의 내적을 이용하여 공유하는 단어 수를 구할 수 있음 → 벡터 연산만으로 자카드 유사도를 계산할 수 있음
```python
assert vector.dot(vector2) == shared_word_count
assert vector @ vector2 == shared_word_count
```
이와 같이 벡터화된 텍스트로 자카드 유사도를 구현한 것 : **타니모토 유사도**

▶ 이진 벡터 또는 비이진 벡터 ([5,2],[5,3])을 비교하여 유사도를 측정

## 13.2 단어 수를 사용하여 텍스트 벡터화하기
텍스트 상황 설정

텍스트 A: "오리" 61회, "거위" 2회 등장 → 벡터 A = [61, 2]

텍스트 B: "오리" 1회, "거위" 71회 등장 → 벡터 B = [1, 71]

이전의 이진 벡터는 단지 '등장 여부'만 판단하기 때문에 두 텍스트 모두 "오리", "거위"가 포함되므로 유사도가 1에 가까워짐

***하지만, 실제로 등장 빈도 다름***

```python
similarity = tanimoto_similarity(np.array([61, 2]), np.array([1, 71]))
print(f"텍스트 간 유사도는 약 {similarity:.3f}입니다")
```
각 인덱스에 등장 빈도 값을 할당하여 유사도를 측정할 수 있음

→ 단어 수 벡터 생성 (용어 빈도 벡터 또는 TF 벡터)

※ TF 벡터화는 두 텍스트의 선호도를 더욱 뚜렷하게 만든다.

**TF 벡터의 장점과 한계**

*장점* : 단어 수 차이에 민감하기에 향상된 비교 결과 제공 

*단점* : 길이가 다른 텍스트 비교 시 불리

### 13.2.1 정규화로 TF 벡터 유사도 개선하기
텍스트 사용 예시 

제목 A : "Pepperoni Pizza! Pepperoni Pizza! Pepperoni Pizza!"

제목 B ; "Pepperoni"

검색엔진에 쿼리를 입력 → 이 쿼리를 TF 벡터로 변환하고, 저장된 문서들의 제목과 유사도 비교 (관련도 계산)

문제점 : TF 벡터의 크기 (스케일)에 따른 유사도 왜곡 발생

이유 → 제목 A의 단어 등장 횟수가 많아 Tanimoto 유사도에서는 오히려 제목 B의 점수가 더 높음 (Tanimoto 유사도는 벡터의 ‘방향’보다 ‘크기’에 더 민감하기 때문)

제목A(3,3)의 벡터는 크기가 커서 내적이 높지만, 분모가 더 커져서 유사도는 낮아짐

제목B(1,1)는 내적이 작지만 분모도 작아서 비율이 더 높음

![image](https://github.com/user-attachments/assets/d55e7d92-84e5-48f0-9b49-a5142727dbef)

해결 방법 : 정규화 (크기 통일)

크기 차이(스케일 차이)를 없애기 위해 나누어서 크기 제거

![image](https://github.com/user-attachments/assets/aebd40c9-aa7c-41e2-bf9e-09a41b38bee2)

(여기서 쿼리 벡터가 A의 정규화된 벡터를 의미)

여기서 query vector을 1로 정규화 시 단어의 수 차이는 중요해지지 않음 (=단위벡터이므로 방향에 따라서만 결정)

위 작업을 거치면 쿼리 벡터와 정규화된 제목 A 벡터를 구분할 수 있음

### 13.2.2 단위 벡터 내적으로 관련성 지표 간 변환하기
코사인 유사도 : 두 단위 벡터의 내적 

코사인 유사도는 각도 기반 내적에서 유도되며,

이를 이용해 유클리드 거리 및 타니모토 유사도로 변환이 가능함

ex)

코사인 유사도 계산 → 유클리드 거리로 변환 → 클러스터링에 사용

코사인 유사도 계산 → 타니모토 유사도로 변환 → 분류 기준으로 사용

🔹 타니모토 유사도 변환의 유용성

유클리드 거리로 변환 가능:

타니모토 유사도를 유클리드 거리로 변환하면,

→ 텍스트 데이터에 대해 K-평균 클러스터링 적용이 가능

코사인 유사도로 변환 가능:

타니모토 유사도는 코사인 유사도로도 변환 가능하며,

→ 모든 계산을 단순한 내적 연산으로 축소 가능

🔹 정규화를 통해 얻는 이점

**텍스트 길이에 따른 차별성 제거**

→ 긴 텍스트와 짧은 텍스트 간의 비교가 공정해짐

**단일 내적만으로 효율적 계산 가능**

→ 불필요한 연산 감소

**전체 벡터 쌍 간 유사도 계산 가능**

→ 전체 유사도 (all by all) 계산 가능

**내적 연산을 행렬 곱셈(matrix multiplication)으로 처리 가능**

1차원 벡터 내적을 2차원 벡터 행렬 연산으로 일반화 가능

→ 모든 텍스트 쌍의 유사도를 벡터화된 방식으로 효율적으로 계산 가능

## 13.3 효율적인 유사도 계산을 위한 행렬 곱셈

![image](https://github.com/user-attachments/assets/8e4ba45f-fb74-4d35-b4ef-fcff97d15622)

세 문장을 대상으로 각 문장에 등장한 단어들을 기반으로 벡터 생성

모든 단어의 집합을 기준으로 각 문장을 정규화된 벡터로 표현

```less
예: "She sells seashells by the seashore." → [1, 1, 0, 0, ..., 1]
```

위쪽 그림 : 빈도 벡터 / 아래쪽 그림 : (cosine 유사도를 기반으로) 정규화된 실수 벡터

(ex : [0,0,1,1,0] 크기 루트2 → [0,0,1/루트2, 1/루트2] 크기 1) = 각 벡터를 vector / ||vector||로 정규화한 결과

![image](https://github.com/user-attachments/assets/7b1ac960-077a-40a8-b0ba-873087133411)

※ 텍스트 간 정규화된 타니모토 유사도 테이블 

:text1과 text2의 유사도가 가장 높고, text2와 text3는 유사도가 가장 낮다.

문장을 벡터화하고, 유사도 계산을 행렬로 처리하면 텍스트 유사도 파악이 효율적이다!

### 13.3.1 기본 행렬 연산
**넘파이 행렬 산술 연산**
🔹 넘파이의 산술적 유연성은 판다스보다 뛰어나다 (ex : similarities 행렬에 사칙연산 적용 가능!)

🔹 행렬 연산을 통해 행렬 유형을 쉽게 변환할 수 있다 (ex : 타니모토 행렬을 2 * similarities / (1+similarities)로 실행하여 코사인 유사도 행렬로 변환)

**넘파이 행렬의 행과 열 연산**
🔹 넘파이는 훨씬 더 간단히 인덱스로 행과 열에 접근할 수 있다.
```python
for i in range (num_rows) :
  for j in range (num_columns) :
      row = similarities[i]
      column = unit_vectors[:,j]
      dot_product = row @ column
```
(이건 비효율적이다 ㅋㅋ)

![image](https://github.com/user-attachments/assets/4e7cbb27-c6da-4a48-9351-3cf4c6e615bf)

**넘파이 행렬의 곱**
NumPy에서는 2D 행렬 @ 2D 행렬을 곱하면, 모든 내적이 자동으로 계산됨 

(=즉, 위처럼 이중 반복문을 실행할 필요가 없다.)

단, 앞 행렬의 열 개수와, 뒤 행렬의 행 개수가 다르면 계산 불가

(넘파이 최고)

### 13.3.2 전체 행렬에 대한 유사도 계산하기
