---
layout : posts
title : 쿠다 ML 심화 6주차
categories : 쿠다 ML 심화
---
# 13. 텍스트 유사성 측정
본 장에서는 두 텍스트의 유사도를 측정하는 기본적인 NLP 문제에 대하여 다루고자 한다.
## 13.1 간단한 텍스트 비교
3개의 텍스트가 주어졌다고 가정할 때, 텍스트 간 차이를 정량화 해야 함
```python
text1 = 'She sells seashells by the seashore.'
text2 = '"Seashells! The seashells are on sale! By the seashore."'
text3 = 'She sells 3 seashells to John, who lives by the lake.'
```

**자카드 유사도** : 두 텍스트의 유사성을 평가하는 간단한 지표 (전체 단어 수에 대한 교집합 비율)

**자카드 유사도의 작동 방식** :
1. 두 텍스트가 주어졌을 때 각 텍스트에서 단어 리스트 추출
2. 텍스트 간 공유되는 고유 단어 수를 셈
3. 공유 단어 수를 두 텍스트의 총 고유 단어 수로 나눔

![image](https://github.com/user-attachments/assets/067336d0-8919-4465-9b35-6ec04fcf3ead)

※ 그림에서 자카드 유사도 : 4/9

### 13.1.1 자카드 유사도 탐색
자카드 유사도가 텍스트 유사성을 합리적으로 측정하는 척도인 이유

▶ 텍스트 간 겹치는 단어와 그렇지 않은 단어 함께 고려

▶ 항상 0과 1 사이로 표현됨 + 해석 용이 (0 : 공유 단어 無 | 0.5 : 공유 단어 半 | 1 : 모든 단어 공유)

### 13.1.2 단어를 숫자 값으로 바꾸기
**텍스트의 벡터화** : 딕셔너리에 각 고유 단어 정보 저장 후, 어휘가 주어지면 1차원 숫자 배열인 벡터로 변환 (해당 단어 텍스트가 존재하면 벡터느 1로, 존재하지 않으면 0으로 변환됨)

벡터의 내적을 이용하여 공유하는 단어 수를 구할 수 있음 → 벡터 연산만으로 자카드 유사도를 계산할 수 있음
```python
assert vector.dot(vector2) == shared_word_count
assert vector @ vector2 == shared_word_count
```
이와 같이 벡터화된 텍스트로 자카드 유사도를 구현한 것 : **타니모토 유사도**

▶ 이진 벡터 또는 비이진 벡터 ([5,2],[5,3])을 비교하여 유사도를 측정

## 13.2 단어 수를 사용하여 텍스트 벡터화하기
텍스트 상황 설정

텍스트 A: "오리" 61회, "거위" 2회 등장 → 벡터 A = [61, 2]

텍스트 B: "오리" 1회, "거위" 71회 등장 → 벡터 B = [1, 71]

이전의 이진 벡터는 단지 '등장 여부'만 판단하기 때문에 두 텍스트 모두 "오리", "거위"가 포함되므로 유사도가 1에 가까워짐

***하지만, 실제로 등장 빈도 다름***

```python
similarity = tanimoto_similarity(np.array([61, 2]), np.array([1, 71]))
print(f"텍스트 간 유사도는 약 {similarity:.3f}입니다")
```
각 인덱스에 등장 빈도 값을 할당하여 유사도를 측정할 수 있음

→ 단어 수 벡터 생성 (용어 빈도 벡터 또는 TF 벡터)

※ TF 벡터화는 두 텍스트의 선호도를 더욱 뚜렷하게 만든다.

**TF 벡터의 장점과 한계**

*장점* : 단어 수 차이에 민감하기에 향상된 비교 결과 제공 

*단점* : 길이가 다른 텍스트 비교 시 불리

### 13.2.1 정규화로 TF 벡터 유사도 개선하기
텍스트 사용 예시 

제목 A : "Pepperoni Pizza! Pepperoni Pizza! Pepperoni Pizza!"

제목 B ; "Pepperoni"

검색엔진에 쿼리를 입력 → 이 쿼리를 TF 벡터로 변환하고, 저장된 문서들의 제목과 유사도 비교 (관련도 계산)

문제점 : TF 벡터의 크기 (스케일)에 따른 유사도 왜곡 발생

이유 → 제목 A의 단어 등장 횟수가 많아 Tanimoto 유사도에서는 오히려 제목 B의 점수가 더 높음 (Tanimoto 유사도는 벡터의 ‘방향’보다 ‘크기’에 더 민감하기 때문)

제목A(3,3)의 벡터는 크기가 커서 내적이 높지만, 분모가 더 커져서 유사도는 낮아짐

제목B(1,1)는 내적이 작지만 분모도 작아서 비율이 더 높음

![image](https://github.com/user-attachments/assets/d55e7d92-84e5-48f0-9b49-a5142727dbef)

해결 방법 : 정규화 (크기 통일)

크기 차이(스케일 차이)를 없애기 위해 나누어서 크기 제거

![image](https://github.com/user-attachments/assets/aebd40c9-aa7c-41e2-bf9e-09a41b38bee2)

(여기서 쿼리 벡터가 A의 정규화된 벡터를 의미)

여기서 query vector을 1로 정규화 시 단어의 수 차이는 중요해지지 않음 (=단위벡터이므로 방향에 따라서만 결정)

위 작업을 거치면 쿼리 벡터와 정규화된 제목 A 벡터를 구분할 수 있음

### 13.2.2 단위 벡터 내적으로 관련성 지표 간 변환하기
코사인 유사도 : 두 단위 벡터의 내적 

코사인 유사도는 각도 기반 내적에서 유도되며,

이를 이용해 유클리드 거리 및 타니모토 유사도로 변환이 가능함

ex)

코사인 유사도 계산 → 유클리드 거리로 변환 → 클러스터링에 사용

코사인 유사도 계산 → 타니모토 유사도로 변환 → 분류 기준으로 사용

🔹 타니모토 유사도 변환의 유용성

유클리드 거리로 변환 가능:

타니모토 유사도를 유클리드 거리로 변환하면,

→ 텍스트 데이터에 대해 K-평균 클러스터링 적용이 가능

코사인 유사도로 변환 가능:

타니모토 유사도는 코사인 유사도로도 변환 가능하며,

→ 모든 계산을 단순한 내적 연산으로 축소 가능

🔹 정규화를 통해 얻는 이점

**텍스트 길이에 따른 차별성 제거**

→ 긴 텍스트와 짧은 텍스트 간의 비교가 공정해짐

**단일 내적만으로 효율적 계산 가능**

→ 불필요한 연산 감소

**전체 벡터 쌍 간 유사도 계산 가능**

→ 전체 유사도 (all by all) 계산 가능

**내적 연산을 행렬 곱셈(matrix multiplication)으로 처리 가능**

1차원 벡터 내적을 2차원 벡터 행렬 연산으로 일반화 가능

→ 모든 텍스트 쌍의 유사도를 벡터화된 방식으로 효율적으로 계산 가능

## 13.3 효율적인 유사도 계산을 위한 행렬 곱셈

![image](https://github.com/user-attachments/assets/8e4ba45f-fb74-4d35-b4ef-fcff97d15622)

세 문장을 대상으로 각 문장에 등장한 단어들을 기반으로 벡터 생성

모든 단어의 집합을 기준으로 각 문장을 정규화된 벡터로 표현

```less
예: "She sells seashells by the seashore." → [1, 1, 0, 0, ..., 1]
```

위쪽 그림 : 빈도 벡터 / 아래쪽 그림 : (cosine 유사도를 기반으로) 정규화된 실수 벡터

(ex : [0,0,1,1,0] 크기 루트2 → [0,0,1/루트2, 1/루트2] 크기 1) = 각 벡터를 vector / ||vector||로 정규화한 결과

![image](https://github.com/user-attachments/assets/7b1ac960-077a-40a8-b0ba-873087133411)

※ 텍스트 간 정규화된 타니모토 유사도 테이블 

:text1과 text2의 유사도가 가장 높고, text2와 text3는 유사도가 가장 낮다.

문장을 벡터화하고, 유사도 계산을 행렬로 처리하면 텍스트 유사도 파악이 효율적이다!

### 13.3.1 기본 행렬 연산
**넘파이 행렬 산술 연산**

🔹 넘파이의 산술적 유연성은 판다스보다 뛰어나다 (ex : similarities 행렬에 사칙연산 적용 가능!)

🔹 행렬 연산을 통해 행렬 유형을 쉽게 변환할 수 있다 (ex : 타니모토 행렬을 2 * similarities / (1+similarities)로 실행하여 코사인 유사도 행렬로 변환) 

왜?

![image](https://github.com/user-attachments/assets/624150f9-afa6-42ef-9ad1-08208e4ccc23)



**넘파이 행렬의 행과 열 연산**

🔹 넘파이는 훨씬 더 간단히 인덱스로 행과 열에 접근할 수 있다.
```python
for i in range (num_rows) :
  for j in range (num_columns) :
      row = similarities[i]
      column = unit_vectors[:,j]
      dot_product = row @ column
```
(이건 비효율적이다 ㅋㅋ)

![image](https://github.com/user-attachments/assets/4e7cbb27-c6da-4a48-9351-3cf4c6e615bf)

**넘파이 행렬의 곱**

NumPy에서는 2D 행렬 @ 2D 행렬을 곱하면, 모든 내적이 자동으로 계산됨 

(=즉, 위처럼 이중 반복문을 실행할 필요가 없다.)

단, 앞 행렬의 열 개수와, 뒤 행렬의 행 개수가 다르면 계산 불가

(넘파이 최고)

### 13.3.2 전체 행렬에 대한 유사도 계산하기
전치를 이용해야만 이중 반복문(for i, for j)으로 일일이 계산한 결과와 완전히 똑같은 코사인 유사도 행렬이 나옴

```python
cosine_matrix = unit_vectors @ unit_vectors.T
assert np.allclose(cosine_matrix, cosine_similarities)
```

그 후, 코사인 유사도 기반으로 타니모토 유사도 행렬로 변환

## 13.4 행렬 곱셈의 계산 한계
```nginx
행렬곱셉의 계산 속도 ∝ 행렬 크기
```
책과 같이 단어의 수가 커지면 시간이 정말 많이 소요된다.

→ 행렬 크기를 줄여야 함

# 14. 행렬 데이터의 차원 감소
차원 축소 : 데이터 정보 내용을 유지한 채 데이터를 축소하는 기술

차원 축소 데이터의 장점

🔹 압축된 데이터는 전송과 저장 용이

🔹 적은 데이터에 대한 알고리즘 수행 시간 단축

🔹 불필요한 노이즈 정보를 제거하여 복잡함 간소화

## 14.1 2D 데이터를 단일 차원으로 그룹화하기
책 예제에서는 온라인 의류 매장 관리 시나리오를 예시로 들었다.

목적 

: 크기에 따라 고객을 세 그룹으로 나누고, 나뉜 그룹을 대상으로 한 가능한 모델 구축하여 신규 고객 위한 의류 크기 유형을 결정 (단순하게)

여기서, 차원축소를 통해 단순성을 확보할 수 있다.

키-몸무게 사이의 선형관계 그래프를 회전 → 그룹/클러스터 구분 가능 (K-평균 or 단순 회전)

x축과 평행하게 그래프를 기울이면 y축 값 제거 (2차원에서 1차원으로 데이터 축소)

![image](https://github.com/user-attachments/assets/efbf7fd1-2a9f-47d1-9c41-be92cd8b423a)

![image](https://github.com/user-attachments/assets/9249a17b-4b76-4078-a3a2-80e0dc453f95)

### 14.1.1 회전으로 차원 줄이기
데이터를 뒤집기 위해 두 가지를 단계별로 실행해야 한다.

1단계: 데이터 중심 정렬

모든 데이터를 평균 기준으로 이동해 중심을 (0,0)에 맞춤

2단계: 회전을 통해 정보 압축

데이터를 회전시켜 x축 분산이 최대가 되도록 함 → 정보가 x축에 몰리게 만듦

이러한 이동을 정량화 할 방법 : 페널티 점수 (x축을 향해 회전함에 따라 점수 감소)

수학적으로: 페널티 점수 ≒ y값 제곱 평균 ≒ y축 분산

1. 최적 회전각(예: 78.3도)으로 데이터를 회전 → 대부분 정보가 x축에 포함됨 (분산의 99.08%)

![image](https://github.com/user-attachments/assets/41591ec9-9a0d-4c1e-bbb4-1fa21f9b16f2)

y축 제거 후 1D x값만으로 데이터 세분화 가능

2. 데이터는 임계값을 사용하여 세분화 한다.

![image](https://github.com/user-attachments/assets/045e7dc4-f3c3-4cbb-a040-7b3c5f6a0313)

3. 1D → 2D로 재구성 (역회전)
 
x값과 y=0을 이용해 역회전하면 2D로 다시 복원 가능 

복원된 데이터는 원본 데이터와 거의 동일한 분포 (99.08% 재현)

**전체 흐름**

중심 이동 → 회전 → x축 정렬

x값 기반 구간 나누기 (고객 분류)

필요하면 역회전해서 2D로 복원
(분석, 시각화, 또는 기존 좌표계 기준의 비교 목적)


