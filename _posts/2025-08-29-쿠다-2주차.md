---
layout : post
title : 쿠다2주차
categories : 데이터비즈니스
---
## 회귀와 예측
**단순 선형회귀**
# 상관관계 vs 회귀

| 구분 | 상관관계 (Correlation) | 회귀 (Regression) |
|------|------------------------|-------------------|
| 정의 | 두 변수 사이의 **관련 강도**를 측정 | 한 변수가 다른 변수에 **얼마나 영향을 주는지 정량화** |
| 해석 | “두 변수가 얼마나 같이 움직이는가?” | “X가 1 단위 변하면 Y는 평균적으로 얼마 변하는가?” |

**회귀식** : 독립변수가 변할 때 종속변수가 어떻게, 얼마나 변하는지를 수식으로 나타낸 관계식

<img width="400" height="482" alt="image" src="https://github.com/user-attachments/assets/86216e20-8d9b-43b8-9739-de749adc38ff" />

▶ 면전에 노출된 연수와 폐활량의 관계를 파악하기 위해 단순선형회귀 이용

<img width="400" height="482" alt="image" src="https://github.com/user-attachments/assets/f7b80979-8718-4765-ab72-cc28863d25ca" />

▶ 단순선형회귀는 예측변수 Exposure에 대한 함수로 응답변수 PEFR을 예측하기 위한 가장 최선의 직선을 찾으려고 시도

**적합값과 잔차**

| 기호 | 이름 | 정의 | 의미 |
|------|------|------|------|
| $$\( Y_i \)$$ | 실제값 (Observed value) | 관측된 종속변수 값 | 우리가 데이터에서 실제로 얻은 값 |
| $$\( \hat{Y}_i = \hat{b}_0 + \hat{b}_1 X_i \)$$ | 적합값 (Fitted value, 예측값) | 추정된 회귀식으로 계산된 값 | 독립변수$$ \(X_i\)$$에 따라 모델이 예측한 종속변수 |
|$$ \( e_i \) $$| 오차항 (Error term) |$$ \( Y_i = b_0 + b_1 X_i + e_i \)$$ | 실제 모형에서 설명되지 않는 부분 (이론적 개념) |
| $$\( \hat{e}_i = Y_i - \hat{Y}_i \)$$ | 잔차 (Residual) | 실제값 - 적합값 | 데이터에서 실제로 계산 가능한 예측 오차 |
| $$\( \hat{b}_0, \hat{b}_1 \) $$| 추정된 회귀계수 (Estimated coefficients) | 표본으로부터 추정된 값 | 모집단의 진짜 계수 \(b_0, b_1\)의 추정치 |

<img width="400" height="482" alt="image" src="https://github.com/user-attachments/assets/65de1f2e-10a0-42dd-b743-d8bc78c7a9fb" />

▶ 폐활량에 대한 회귀선으로부터 얻은 잔차 (수직으로 그은 점선들 = 잔차)

**최소제곱** : 잔차제곱을 최소화하는 방법

```
$$​min(b0,b1)_​i=1∑n​(yi​−y^​i​)2$$
```
**예측 대 설명 (프로파일링)**

| 구분 | 예측 (Prediction) | 설명 (Explanation / Profiling) |
|------|-------------------|--------------------------------|
| 목적 | 새로운 \(X\) 값으로 \(Y\)를 **정확히 맞추는 것** | \(X\)가 \(Y\)에 **어떻게, 왜 영향을 주는지 해석** |
| 관심사 | 수학적/통계적 관계식 자체 (\(Y = b_0 + b_1X\)) | 회귀계수의 의미, 변수 간 **인과·맥락 해석** |
| 평가 | RMSE, MAE, R² 같은 **예측 정확도 지표** | Adjusted R², 설명력, 인과적 타당성 |
| 특징 | 패턴 포착이 핵심, 인과 여부는 중요치 않음 | 단순 수치 이상으로 **도메인 지식** 필요 |
| 예시 | “광고비 X로 다음 달 매출 Y 예측하기” | “광고비가 매출에 영향을 주는 이유를 설명하기” |

**다중선형회귀**

*회귀식*

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \varepsilon
$$

*적합값*

$$
\hat{Y}_i \= \hat{\beta}_0 + \hat{\beta}_1 X_{i1} + \hat{\beta}_2 X_{i2} + \cdots + \hat{\beta}_p X_{ip}
$$

*잔차*

$$
\hat{e}_i = Y_i - \hat{Y}_i
$$

R² (결정계수)와 "변동을 설명한다"는 의미

1. 기본 개념
- **총 변동 (SST, Total Variation)**  
  데이터 \(y_i\)가 평균 \(\bar{y}\) 주위에서 얼마나 퍼져 있는가  
  \[
  SST = \sum (y_i - \bar{y})^2
  \]

- **설명된 변동 (SSR, Explained Variation)**  
  모델 예측값 \(\hat{y}_i\)가 평균 \(\bar{y}\)와 얼마나 다른가 → 모델이 잡아낸 패턴  
  \[
  SSR = \sum (\hat{y}_i - \bar{y})^2
  \]

- **잔차 변동 (SSE, Unexplained Variation)**  
  실제값과 예측값의 차이 → 모델이 설명 못한 부분  
  \[
  SSE = \sum (y_i - \hat{y}_i)^2
  \]

---

2. R² 정의
\[
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
\]

- \(R^2 = 0.8\) → 데이터 변동의 **80%를 모델이 설명**  
- \(R^2 = 0.2\) → 데이터 변동의 **20%만 모델이 설명**, 나머지는 잡지 못함  

---

3. "변동을 설명한다"는 직관적 의미
- 데이터는 평균에서 들쭉날쭉(=변동) 퍼져 있다.  
- 모델이 잡은 직선(혹은 곡선)이 점들의 **패턴**을 얼마나 잘 따라가는지가 "설명력".  
- 설명력이 높다 = 점들의 흩어진 이유를 모델이 잘 포착했다는 뜻.  

**모델 평가와 홀드아웃 샘플의 문제점**

1. 기존 지표의 한계
- R², F 통계량, p값 등은 **훈련 데이터 기준**으로 계산됨.
- 따라서 **새로운 데이터에서의 성능(일반화 능력)** 은 보장하지 못함.

2. 해결책: 홀드아웃(hold-out)
- 데이터를 **훈련(train) / 테스트(test)** 로 분리.
- 훈련 데이터로 모델을 만들고, 테스트 데이터로 성능을 검증.

3. 문제점
- **테스트 세트가 작으면** 샘플 편차가 커짐 → 성능 지표가 불안정.
- 즉, 모델의 **진짜 성능과 측정된 성능 간 오차(불확실성)** 이 커질 수 있음.

4. 대안
- 교차검증(K-fold CV) : 데이터를 k개로 나누고, 각 부분을 번갈아가며 Test set로 활용하는 것을 의미한다.

**모형 선택 및 단계적 회귀** : 많은 변수를 추가한다고 해서 꼭 더 좋은 모델을 얻는 것인가? (X)

비교
| 기준 | R² | 수정 R² | AIC |
|------|-----|---------|-----|
| 의미 | 설명된 변동 비율 | R² + 자유도 보정 | 적합도 vs 복잡도 균형 |
| 해석 | 값 ↑ → 좋음 | 값 ↑ → 좋음 | 값 ↓ → 좋음 |
| 변수 추가 효과 | 항상 좋아짐 | 유효한 변수만 좋아짐 | 복잡도에 따라 불이익 |
| 한계 | 과적합 착시 | 여전히 훈련 데이터 기반 | 절대적 의미 없음, 비교만 가능 |

---

- **R²**: 단순 설명력 (변수 추가하면 착시 발생)  
- **수정 R²**: 변수 추가에 패널티 반영, 유효 변수만 인정  
- **AIC**: 잔차 크기와 변수 수를 함께 고려, 값이 작을수록 좋은 모델  

AIC를 최소로 하거나, 수정 R 제곱을 최대로 하는 모델을 어떻게 찾을 수 있을까?

1. 부분집합회귀 : 모든 가능한 모델을 검색 -> 계산 비용 많이 소모 + 대용량 데이터와 다변수 문제에 적합하지 않음

2. 단계적 회귀 (대안적 회귀방법)

(1) 전진 선택 (Forward Selection)
- 처음에 변수 없음 → 하나씩 추가  
- 기준: p값, F-test, AIC/BIC 등  

(2) 후진 제거 (Backward Elimination)
- 모든 변수를 포함한 모델에서 시작 → 덜 중요한 변수 제거  
- 기준: p값, F-test 등  

(3) 단계적 선택 (Stepwise Selection)
- 전진 선택 + 후진 제거 혼합  
- 변수를 추가하면서, 기존 변수가 유의하지 않으면 제거  
- **유의미한 모델이 될 때까지 변수 집합을 조정**  
- 한계: 데이터 샘플에 민감, 변수 많을 때 불안정  

---

3. 벌점 회귀 (Penalized Regression)
- 아이디어: 변수를 빼는 대신 **벌점(Penalty)** 으로 중요도 조절
$$\[
\min_\beta \; RSS(\beta) + \lambda \cdot P(\beta)
\] $$
- Ridge (L2): $$\(\lambda \sum \beta_j^2\$$) → 계수 크기 축소, 다중공선성 완화  
- Lasso (L1): $$\(\lambda \sum |\beta_j|\$$) → 일부 계수 0, 변수 선택 효과  

4. 교차검증 (Cross Validation, CV)
- 데이터를 반복 분할하여 훈련/검증 → **새로운 데이터에서의 성능** 추정  
- 과적합/과소적합 여부를 확인 가능  
- 벌점 회귀의 λ 같은 하이퍼파라미터 선택에도 사용

**회귀분석 → (단계적 회귀 또는 벌점 회귀)로 변수 조정 → 교차검증으로 과적합 여부 확인** 

이 과정을 통해 가장 신뢰할 수 있는 모델을 선택할 수 있다.

---

가중회귀 : 방정식 피팅할 때 레코드별로 가중치를 주기 위해 사용 : 각 관측값의 중요도나 신뢰도가 다르다고 판단될 때, 가중치(weight) 를 곱해 잔차 제곱합을 계산

---

**회귀에서의 요인변수** : 개수가 제한된 이산값을 취하는 변수들

용어 정리

1. 가변수 (Dummy Variable)
- **정의**: 회귀나 다른 모델에서 요인 데이터를 사용하기 위해 0과 1의 이진변수로 부호화한 변수.

2. 기준 부호화 (Reference Coding)
- **정의**: 통계학자들이 많이 사용하는 부호화 형태.  
  한 요인을 **기준(reference)** 으로 하고, 다른 요인들이 이 기준에 따라 비교될 수 있도록 표현.  
- **유의어**: 처리 부호화 (Treatment Coding)

3. 원-핫 인코딩 (One-Hot Encoding)
- **정의**: 머신러닝 분야에서 많이 사용되는 부호화 방식.  
  모든 요인 수준이 각각 독립된 변수로 표현됨.  
- **특징**: 머신러닝 알고리즘에는 유용하나, 다중선형회귀에는 적합하지 않음 (다중공선성 문제 발생).

4. 편차 부호화 (Deviation Coding)
- **정의**: 기준 수준과는 반대로, **전체 평균**에 대해 각 수준을 비교하는 부호화 방식.  
- **유의어**: 총합 대비 (Sum Contrast)

---

요인변수는 회귀를 위해 수치형 변수로 변환해야 함.

다수의 수준을 갖는 요인변수의 경우 : 더 적은 수의 수준을 갖는 변수가 되도록 수준들을 통합해야 함 (과적합 & 차원의 저주 방지)

순서를 갖는 요인변수의 경우 : 수치형 변수로 변환하여 사용할 수 있음 (EX : A,B,C 등급)

**회귀방정식 해석** : X,Y 간 관계의 본질을 이해하기 위해 방정식 자체로부터 통찰을 얻는 방법.

1. 예측변수 간 상관 

2. 다중공선성 : 한 예측 변수가 다른 변수들의 선형결합으로 표현됨을 의미

원인 :

- 오류로 인해 한 변수가 여러 번 포함된 경우
- 요인변수로부터 P-1개가 아닌 P개의 가변수가 만들어진 경우
- 두 변수가 서로 거의 완벽하게 상관성이 있는 경우

-> 다중공선성이 사라질 때까지 회귀분석에서 변수를 제거해야 함. (완전 다중공선성이 존재하는 상황에서는 회귀를 통해 제대로 된 답을 도출하기 어려움)

| 구분 | 완전 다중공선성                   | 불완전 다중공선성                        |
| -- | -------------------------- | -------------------------------- |
| 정의 | 한 변수가 다른 변수들의 **정확한 선형결합** | 강한 상관관계는 있으나, 완전 결합은 아님          |
| 결과 | 회귀계수 추정 자체 불가능             | 추정은 가능하지만, 불안정하고 표준오차 큼          |
| 원인 | 가변수 함정(dummy trap), 중복된 변수 | 높은 상관관계 변수 동시 포함                 |
| 해결 | 변수 제거, 기준 범주 삭제            | 변수 선택, 차원 축소(PCA, Ridge/Lasso 등) |

※ 트리, 클러스터링, 최근접 이웃 알고리즘 등 비선형회귀 유형이 아닌 방법에서는 다중공선성이 크게 문제가 되지 않는다. 그럼에도 불구하고 다중공선성은 줄이는 게 좋다.

3. 교란변수 : 회귀방정식에 중요한 변수가 포함되지 못해서 생기는 누락의 문제

4. 상호작용과 주효과 : 두 변수의 결합 효과, 즉 한 변수의 효과가 다른 변수 값에 따라 달라지는 현상 -> 변수의 결과가 서로 의존적일 때 고려할 필요 있음 (공선성은 다른 문제다)

※ 다수의 변수가 존재하는 문제의 경우, 어떤 상호작용을 고려해야 할지 결정하기 어려움

해결 : 

- 사전 지식, 직관을 통한 상호작용 결정
- 단계적 선택을 통한 제거
- 벌점 회귀 등을 통한 상호작용 가려내기
- 랜덤 포레스트, 그레이디언트 부스팅 트리와 같은 트리모델은 자동적으로 최적의 상호작용 항을 걸러냄

**회귀 진단**

용어 정리

1. 표준화잔차 (Standardized Residual)
- 잔차를 표준오차로 나눈 값 : 특잇값 발견 가능

2. 특잇값 (Outlier)
- 나머지 데이터(혹은 예측값)와 멀리 떨어진 레코드(혹은 출력값)

3. 영향값 (Influential Value)
- 있을 때와 없을 때 회귀방정식에 큰 차이를 보이는 값 혹은 레코드

4. 지렛대 (레버리지, Leverage)
- 회귀식에 한 레코드가 미치는 영향력의 정도  
- **유의어**: 햇값 (hat-value)

5. 비정규 잔차 (Non-normal Residual)
- 정규분포를 따르지 않는 잔차는 회귀분석의 요건을 무효로 만들 수 있음  
- 데이터 과학에서는 별로 중요하게 다루지 않음

6. 이분산성 (Heteroskedasticity)
- 어떤 범위 내 출력값의 잔차가 매우 넓은 분산을 보이는 경향  
- (어떤 예측변수를 회귀식에 놓치고 있다는 것을 의미할 수 있음)

7. 편차그림 (Partial Residual Plot)
- 결과변수와 특정 예측변수 사이의 관계를 진단하는 그림  
- **유의어**: 추가변수그림 (Added Variable Plot)

# 회귀모형 진단과 비선형성 요약

## 4.6.3 이분산성, 비정규성, 오차 간 상관
- 잔차 분포는 추정치 신뢰구간과 관련 있음 → 데이터 과학자는 크게 신경 안 씀.
- **이분산성**: 예측값에 따라 잔차 분산이 일정하지 않은 현상.
  - 예: 값이 낮은 집일수록 잔차 분산이 큼.
  - 시각화(잔차 산점도)로 쉽게 확인 가능.
- **비정규성**: 표준화 잔차가 정규분포를 따르지 않음 → 긴 꼬리 분포 발생.
- **오차 간 상관**: 시계열·공간 데이터에서 자주 발생 → 더빈-왓슨 통계량 등으로 진단.

---

편잔차그림과 비선형성
- **편잔차그림(partial residual plot)**: 특정 변수와 종속변수 간의 관계를 시각화.
- 잔차에 해당 변수 효과를 더해 그 변수와의 관계만 따로 확인.
- 예: `SqFtTotLiving` 변수 → 주택가격과의 관계가 단순선형이 아닌 비선형임을 보여줌.

---

다항회귀와 스플라인 회귀
다항식
- 응답변수와 설명변수 관계가 비선형일 때, 다항식을 사용.
- 예: `Y = b0 + b1X + b2X^2 + e`
- 한계: 고차항 추가 시 과적합 위험 존재.

스플라인
- 변수 구간을 나누어 각각 다른 다항식을 적합 → 곡선을 더 유연하게 표현.
- 국소적 적합으로 전체적인 곡선 형태를 잘 반영.
- R: `splines::bs()`, Python: `patsy`의 `bs()` 등 사용.

일반화가법모형(GAM)
- 스플라인을 자동으로 찾아주는 확장된 모형.
- `s(X)` 형태로 특정 변수의 비선형 효과를 반영.
- 유연하면서도 해석이 가능.

# Chapter 5 분류 요약

나이브 베이즈
- **개념**: 조건부 확률을 이용하여 결과를 예측하는 분류 알고리즘.
- **절차**
  1. 각 레코드에 대해 모든 레코드들과 비교.
  2. 가장 유사한 레코드 선택.
  3. 해당 레코드의 결과로 분류.
- **특징**
  - 조건부 독립 가정(단순화된 가정).
  - 계산 효율적, 적은 데이터에도 동작 가능.
- **한계**
  - 현실에서는 변수 간 독립이 성립하지 않는 경우가 많음 → 정확성 낮을 수 있음.
- **변형**: 나이브한 가정을 완화한 **나이브한 해법** 존재.
- **응용**: 텍스트 분류, 스팸메일 탐지, 감성 분석.

---

판별분석
- **개념**: 관측치를 소속 집단(클래스)으로 분류하는 기법.
- **대표 방법**: 선형판별분석(LDA, Linear Discriminant Analysis)
  - 클래스 간 분산을 최대화, 클래스 내 분산을 최소화.
  - 다차원 데이터를 선형 조합하여 판별 축 생성.
- **절차**
  1. 클래스별 평균, 공분산 계산.
  2. 분산비율 극대화하는 선형결합 찾기.
  3. 새로운 관측치 분류.
- **유형**
  - **공분산행렬 동일 가정**: LDA.
  - **공분산행렬 다름 허용**: QDA(Quadratic Discriminant Analysis).
- **적용 분야**: 의학 통계, 얼굴 인식, 마케팅 세분화.

---

로지스틱 회귀
- **개념**: 이진(0/1) 분류 문제에서 특정 클래스에 속할 확률을 추정.
- **핵심 함수**: 로지스틱 반응 함수 (시그모이드 함수).
  \[
  p = \frac{1}{1 + e^{-(\beta_0 + \beta_1 X_1 + ... + \beta_k X_k)}}
  \]
- **특징**
  - 출력은 0~1 사이 확률값.
  - 결과는 로짓(logit, log-odds) 변환을 통해 선형 회귀처럼 해석 가능.
- **GLM(Generalized Linear Model)**의 특수한 경우.
- **해석**: 회귀계수 → 오즈비(odds ratio)로 해석 가능.
  - \( \text{오즈비} > 1 \): 설명변수가 결과 발생 확률을 높임.
  - \( \text{오즈비} < 1 \): 결과 발생 확률을 낮춤.

로지스틱 반응 함수와 로짓
- 확률을 오즈(odds)로 변환 후 로그취한 값이 선형결합으로 표현됨.
- 직관적으로 해석 가능: 변수 1 단위 증가 시 오즈비 변화.

GLM으로서의 로지스틱 회귀
- `family = binomial`을 사용하여 GLM의 한 형태로 구현.
- Python: `LogisticRegression`, R: `glm()`.

일반화선형모형(GLM)
- 다양한 링크 함수와 분포를 통해 회귀 모델을 일반화.
- 로지스틱 회귀는 GLM의 대표적 사례.

예측값
- 로짓을 확률로 변환하여 예측값 도출.
- `predict_proba`로 클래스별 확률 추정 가능.

계수와 오즈비
- 회귀계수는 곧 오즈비로 해석.
- 예: 특정 변수의 계수 β = 0.2 → 오즈비는 exp(0.2) ≈ 1.22 → 결과 발생 확률이 약 22% 증가.

---


- **나이브 베이즈**: 조건부 독립 가정, 계산 효율적.
- **판별분석**: LDA, QDA → 클래스 분리 기반.
- **로지스틱 회귀**: 확률 기반 분류, 오즈비 해석 가능.

선형회귀와 로지스틱 회귀: 유사점과 차이점
- 둘 다 종속변수를 예측하기 위해 설명변수를 선형 결합.
- **차이점**: 선형회귀는 연속형 변수 예측, 로지스틱 회귀는 이진 분류 확률 예측.
- 로지스틱 회귀는 오즈비(log odds) 변환을 통해 확률을 선형모형처럼 다룸.

---

모델 평가
- 모델의 성능은 새로운 데이터에서 얼마나 정확히 분류하는지로 평가.
- 주요 지표:
  - **계수의 통계적 유의성** (p-value, 표준오차 SE).
  - **분류 성능 지표** (혼동행렬, 정확도, 재현율, 특이도 등).

---

잔차 분석
- 선형회귀와 달리 로지스틱 회귀에서는 잔차 해석이 까다로움.
- 편잔차(Partial Residual)를 사용하여 설명변수 효과 확인.
- 패턴이 비선형이면 스플라인, 다항식 회귀 등 고려.

---

 분류 모델 평가하기
분류 모델의 예측력을 평가하는 여러 지표.

 혼동행렬
- 실제값과 예측값을 교차시킨 표.
- 구성:
  - **TP**: 실제 1, 예측 1
  - **TN**: 실제 0, 예측 0
  - **FP**: 실제 0, 예측 1 (거짓 양성)
  - **FN**: 실제 1, 예측 0 (거짓 음성)

 희귀 클래스 문제
- 클래스 비율이 불균형하면 단순 정확도는 무의미.
- 예: 사기 거래 예측 → 대부분 정상 거래라면 정확도는 높지만 의미 없음.
- 해결: 재표본추출, 가중치 부여, ROC 곡선 등 사용.

정밀도, 재현율, 특이도
- **정밀도(Precision)**: 예측한 양성 중 실제 양성 비율.
- **재현율(Recall, 민감도)**: 실제 양성 중 올바르게 예측한 비율.
- **특이도(Specificity)**: 실제 음성 중 올바르게 예측한 비율.

 ROC 곡선
- X축: 특이도(1 - FPR), Y축: 재현율(민감도).
- 여러 임곗값 변화에 따른 모델 성능을 시각화.
- 완벽한 분류기는 좌상단(0,1)에 가까운 곡선.

AUC
- ROC 곡선 아래 면적.
- 0.5: 무작위 예측, 1.0: 완벽 예측.
- 일반적으로 0.7 이상이면 준수한 성능, 0.9 이상이면 우수.

리프트
- 특정 구간에서 모델이 무작위 예측 대비 얼마나 더 잘 분류하는지 평가.
- 리프트 곡선으로 시각화.
- 마케팅 캠페인 등에서 고객 반응 예측에 자주 활용.

---

 불균형 데이터 처리
과소표본추출 (Undersampling)
- 다수 클래스 데이터를 줄여 클래스 비율을 맞추는 방법.

과잉표본추출 (Oversampling)
- 소수 클래스 데이터를 복제 또는 합성하여 데이터 균형 맞춤.
- **SMOTE** 기법: 소수 클래스 데이터를 합성해 샘플 생성.

데이터 생성
- 기존 데이터에서 새로운 데이터를 합성.
- SMOTE, ADASYN 등이 대표적.

비용 기반 분류
- 잘못 분류했을 때의 비용을 고려하여 모델 학습.
- 예: 사기 탐지에서 False Negative 비용이 큰 경우 가중치를 더 부여.

예측 결과 분석
- 여러 분류 모델(LDA, 로지스틱, 트리, GAM 등)의 성능을 비교.
- AUC, ROC, F1-score 등을 활용해 평가.








