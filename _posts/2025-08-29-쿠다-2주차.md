---
layout : post
title : 쿠다2주차
categories : 데이터비즈니스
---
## 회귀와 예측
**단순 선형회귀**
# 상관관계 vs 회귀

| 구분 | 상관관계 (Correlation) | 회귀 (Regression) |
|------|------------------------|-------------------|
| 정의 | 두 변수 사이의 **관련 강도**를 측정 | 한 변수가 다른 변수에 **얼마나 영향을 주는지 정량화** |
| 해석 | “두 변수가 얼마나 같이 움직이는가?” | “X가 1 단위 변하면 Y는 평균적으로 얼마 변하는가?” |

**회귀식** : 독립변수가 변할 때 종속변수가 어떻게, 얼마나 변하는지를 수식으로 나타낸 관계식

<img width="400" height="482" alt="image" src="https://github.com/user-attachments/assets/86216e20-8d9b-43b8-9739-de749adc38ff" />

▶ 면전에 노출된 연수와 폐활량의 관계를 파악하기 위해 단순선형회귀 이용

<img width="400" height="482" alt="image" src="https://github.com/user-attachments/assets/f7b80979-8718-4765-ab72-cc28863d25ca" />

▶ 단순선형회귀는 예측변수 Exposure에 대한 함수로 응답변수 PEFR을 예측하기 위한 가장 최선의 직선을 찾으려고 시도

**적합값과 잔차**

| 기호 | 이름 | 정의 | 의미 |
|------|------|------|------|
| $$\( Y_i \)$$ | 실제값 (Observed value) | 관측된 종속변수 값 | 우리가 데이터에서 실제로 얻은 값 |
| $$\( \hat{Y}_i = \hat{b}_0 + \hat{b}_1 X_i \)$$ | 적합값 (Fitted value, 예측값) | 추정된 회귀식으로 계산된 값 | 독립변수$$ \(X_i\)$$에 따라 모델이 예측한 종속변수 |
|$$ \( e_i \) $$| 오차항 (Error term) |$$ \( Y_i = b_0 + b_1 X_i + e_i \)$$ | 실제 모형에서 설명되지 않는 부분 (이론적 개념) |
| $$\( \hat{e}_i = Y_i - \hat{Y}_i \)$$ | 잔차 (Residual) | 실제값 - 적합값 | 데이터에서 실제로 계산 가능한 예측 오차 |
| $$\( \hat{b}_0, \hat{b}_1 \) $$| 추정된 회귀계수 (Estimated coefficients) | 표본으로부터 추정된 값 | 모집단의 진짜 계수 \(b_0, b_1\)의 추정치 |

<img width="400" height="482" alt="image" src="https://github.com/user-attachments/assets/65de1f2e-10a0-42dd-b743-d8bc78c7a9fb" />

▶ 폐활량에 대한 회귀선으로부터 얻은 잔차 (수직으로 그은 점선들 = 잔차)

**최소제곱** : 잔차제곱을 최소화하는 방법

```
$$​min(b0,b1)_​i=1∑n​(yi​−y^​i​)2$$
```
**예측 대 설명 (프로파일링)**

| 구분 | 예측 (Prediction) | 설명 (Explanation / Profiling) |
|------|-------------------|--------------------------------|
| 목적 | 새로운 \(X\) 값으로 \(Y\)를 **정확히 맞추는 것** | \(X\)가 \(Y\)에 **어떻게, 왜 영향을 주는지 해석** |
| 관심사 | 수학적/통계적 관계식 자체 (\(Y = b_0 + b_1X\)) | 회귀계수의 의미, 변수 간 **인과·맥락 해석** |
| 평가 | RMSE, MAE, R² 같은 **예측 정확도 지표** | Adjusted R², 설명력, 인과적 타당성 |
| 특징 | 패턴 포착이 핵심, 인과 여부는 중요치 않음 | 단순 수치 이상으로 **도메인 지식** 필요 |
| 예시 | “광고비 X로 다음 달 매출 Y 예측하기” | “광고비가 매출에 영향을 주는 이유를 설명하기” |

**다중선형회귀**

*회귀식*

$$
Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_p X_p + \varepsilon
$$

*적합값*

$$
\hat{Y}_i \= \hat{\beta}_0 + \hat{\beta}_1 X_{i1} + \hat{\beta}_2 X_{i2} + \cdots + \hat{\beta}_p X_{ip}
$$

*잔차*

$$
\hat{e}_i = Y_i - \hat{Y}_i
$$

R² (결정계수)와 "변동을 설명한다"는 의미

1. 기본 개념
- **총 변동 (SST, Total Variation)**  
  데이터 \(y_i\)가 평균 \(\bar{y}\) 주위에서 얼마나 퍼져 있는가  
  \[
  SST = \sum (y_i - \bar{y})^2
  \]

- **설명된 변동 (SSR, Explained Variation)**  
  모델 예측값 \(\hat{y}_i\)가 평균 \(\bar{y}\)와 얼마나 다른가 → 모델이 잡아낸 패턴  
  \[
  SSR = \sum (\hat{y}_i - \bar{y})^2
  \]

- **잔차 변동 (SSE, Unexplained Variation)**  
  실제값과 예측값의 차이 → 모델이 설명 못한 부분  
  \[
  SSE = \sum (y_i - \hat{y}_i)^2
  \]

---

2. R² 정의
\[
R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}
\]

- \(R^2 = 0.8\) → 데이터 변동의 **80%를 모델이 설명**  
- \(R^2 = 0.2\) → 데이터 변동의 **20%만 모델이 설명**, 나머지는 잡지 못함  

---

3. "변동을 설명한다"는 직관적 의미
- 데이터는 평균에서 들쭉날쭉(=변동) 퍼져 있다.  
- 모델이 잡은 직선(혹은 곡선)이 점들의 **패턴**을 얼마나 잘 따라가는지가 "설명력".  
- 설명력이 높다 = 점들의 흩어진 이유를 모델이 잘 포착했다는 뜻.  

**모델 평가와 홀드아웃 샘플의 문제점**

1. 기존 지표의 한계
- R², F 통계량, p값 등은 **훈련 데이터 기준**으로 계산됨.
- 따라서 **새로운 데이터에서의 성능(일반화 능력)** 은 보장하지 못함.

2. 해결책: 홀드아웃(hold-out)
- 데이터를 **훈련(train) / 테스트(test)** 로 분리.
- 훈련 데이터로 모델을 만들고, 테스트 데이터로 성능을 검증.

3. 문제점
- **테스트 세트가 작으면** 샘플 편차가 커짐 → 성능 지표가 불안정.
- 즉, 모델의 **진짜 성능과 측정된 성능 간 오차(불확실성)** 이 커질 수 있음.

4. 대안
- 교차검증(K-fold CV) : 데이터를 k개로 나누고, 각 부분을 번갈아가며 Test set로 활용하는 것을 의미한다.

**모형 선택 및 단계적 회귀** : 많은 변수를 추가한다고 해서 꼭 더 좋은 모델을 얻는 것인가? (X)

비교
| 기준 | R² | 수정 R² | AIC |
|------|-----|---------|-----|
| 의미 | 설명된 변동 비율 | R² + 자유도 보정 | 적합도 vs 복잡도 균형 |
| 해석 | 값 ↑ → 좋음 | 값 ↑ → 좋음 | 값 ↓ → 좋음 |
| 변수 추가 효과 | 항상 좋아짐 | 유효한 변수만 좋아짐 | 복잡도에 따라 불이익 |
| 한계 | 과적합 착시 | 여전히 훈련 데이터 기반 | 절대적 의미 없음, 비교만 가능 |

---

- **R²**: 단순 설명력 (변수 추가하면 착시 발생)  
- **수정 R²**: 변수 추가에 패널티 반영, 유효 변수만 인정  
- **AIC**: 잔차 크기와 변수 수를 함께 고려, 값이 작을수록 좋은 모델  

AIC를 최소로 하거나, 수정 R 제곱을 최대로 하는 모델을 어떻게 찾을 수 있을까?

1. 부분집합회귀 : 모든 가능한 모델을 검색 -> 계산 비용 많이 소모 + 대용량 데이터와 다변수 문제에 적합하지 않음

2. 단계적 회귀 (대안적 회귀방법)

(1) 전진 선택 (Forward Selection)
- 처음에 변수 없음 → 하나씩 추가  
- 기준: p값, F-test, AIC/BIC 등  

(2) 후진 제거 (Backward Elimination)
- 모든 변수를 포함한 모델에서 시작 → 덜 중요한 변수 제거  
- 기준: p값, F-test 등  

(3) 단계적 선택 (Stepwise Selection)
- 전진 선택 + 후진 제거 혼합  
- 변수를 추가하면서, 기존 변수가 유의하지 않으면 제거  
- **유의미한 모델이 될 때까지 변수 집합을 조정**  
- 한계: 데이터 샘플에 민감, 변수 많을 때 불안정  

---

3. 벌점 회귀 (Penalized Regression)
- 아이디어: 변수를 빼는 대신 **벌점(Penalty)** 으로 중요도 조절
$$\[
\min_\beta \; RSS(\beta) + \lambda \cdot P(\beta)
\] $$
- Ridge (L2): $$\(\lambda \sum \beta_j^2\$$) → 계수 크기 축소, 다중공선성 완화  
- Lasso (L1): $$\(\lambda \sum |\beta_j|\$$) → 일부 계수 0, 변수 선택 효과  

4. 교차검증 (Cross Validation, CV)
- 데이터를 반복 분할하여 훈련/검증 → **새로운 데이터에서의 성능** 추정  
- 과적합/과소적합 여부를 확인 가능  
- 벌점 회귀의 λ 같은 하이퍼파라미터 선택에도 사용

**회귀분석 → (단계적 회귀 또는 벌점 회귀)로 변수 조정 → 교차검증으로 과적합 여부 확인** 

이 과정을 통해 가장 신뢰할 수 있는 모델을 선택할 수 있다.

---

가중회귀 : 방정식 피팅할 때 레코드별로 가중치를 주기 위해 사용 : 각 관측값의 중요도나 신뢰도가 다르다고 판단될 때, 가중치(weight) 를 곱해 잔차 제곱합을 계산

---

**회귀에서의 요인변수** : 개수가 제한된 이산값을 취하는 변수들

용어 정리

1. 가변수 (Dummy Variable)
- **정의**: 회귀나 다른 모델에서 요인 데이터를 사용하기 위해 0과 1의 이진변수로 부호화한 변수.

2. 기준 부호화 (Reference Coding)
- **정의**: 통계학자들이 많이 사용하는 부호화 형태.  
  한 요인을 **기준(reference)** 으로 하고, 다른 요인들이 이 기준에 따라 비교될 수 있도록 표현.  
- **유의어**: 처리 부호화 (Treatment Coding)

3. 원-핫 인코딩 (One-Hot Encoding)
- **정의**: 머신러닝 분야에서 많이 사용되는 부호화 방식.  
  모든 요인 수준이 각각 독립된 변수로 표현됨.  
- **특징**: 머신러닝 알고리즘에는 유용하나, 다중선형회귀에는 적합하지 않음 (다중공선성 문제 발생).

4. 편차 부호화 (Deviation Coding)
- **정의**: 기준 수준과는 반대로, **전체 평균**에 대해 각 수준을 비교하는 부호화 방식.  
- **유의어**: 총합 대비 (Sum Contrast)

---

요인변수는 회귀를 위해 수치형 변수로 변환해야 함.

다수의 수준을 갖는 요인변수의 경우 : 더 적은 수의 수준을 갖는 변수가 되도록 수준들을 통합해야 함 (과적합 & 차원의 저주 방지)

순서를 갖는 요인변수의 경우 : 수치형 변수로 변환하여 사용할 수 있음 (EX : A,B,C 등급)

**회귀방정식 해석** : X,Y 간 관계의 본질을 이해하기 위해 방정식 자체로부터 통찰을 얻는 방법.

1. 예측변수 간 상관 

2. 다중공선성 : 한 예측 변수가 다른 변수들의 선형결합으로 표현됨을 의미

원인 :

- 오류로 인해 한 변수가 여러 번 포함된 경우
- 요인변수로부터 P-1개가 아닌 P개의 가변수가 만들어진 경우
- 두 변수가 서로 거의 완벽하게 상관성이 있는 경우

-> 다중공선성이 사라질 때까지 회귀분석에서 변수를 제거해야 함. (완전 다중공선성이 존재하는 상황에서는 회귀를 통해 제대로 된 답을 도출하기 어려움)

| 구분 | 완전 다중공선성                   | 불완전 다중공선성                        |
| -- | -------------------------- | -------------------------------- |
| 정의 | 한 변수가 다른 변수들의 **정확한 선형결합** | 강한 상관관계는 있으나, 완전 결합은 아님          |
| 결과 | 회귀계수 추정 자체 불가능             | 추정은 가능하지만, 불안정하고 표준오차 큼          |
| 원인 | 가변수 함정(dummy trap), 중복된 변수 | 높은 상관관계 변수 동시 포함                 |
| 해결 | 변수 제거, 기준 범주 삭제            | 변수 선택, 차원 축소(PCA, Ridge/Lasso 등) |

※ 트리, 클러스터링, 최근접 이웃 알고리즘 등 비선형회귀 유형이 아닌 방법에서는 다중공선성이 크게 문제가 되지 않는다. 그럼에도 불구하고 다중공선성은 줄이는 게 좋다.

3. 교란변수 : 회귀방정식에 중요한 변수가 포함되지 못해서 생기는 누락의 문제

4. 상호작용과 주효과 : 두 변수의 결합 효과, 즉 한 변수의 효과가 다른 변수 값에 따라 달라지는 현상 -> 변수의 결과가 서로 의존적일 때 고려할 필요 있음 (공선성은 다른 문제다)

※ 다수의 변수가 존재하는 문제의 경우, 어떤 상호작용을 고려해야 할지 결정하기 어려움

해결 : 

- 사전 지식, 직관을 통한 상호작용 결정
- 단계적 선택을 통한 제거
- 벌점 회귀 등을 통한 상호작용 가려내기
- 랜덤 포레스트, 그레이디언트 부스팅 트리와 같은 트리모델은 자동적으로 최적의 상호작용 항을 걸러냄

**회귀 진단**

용어 정리

1. 표준화잔차 (Standardized Residual)
- 잔차를 표준오차로 나눈 값 : 특잇값 발견 가능

2. 특잇값 (Outlier)
- 나머지 데이터(혹은 예측값)와 멀리 떨어진 레코드(혹은 출력값)

3. 영향값 (Influential Value)
- 있을 때와 없을 때 회귀방정식에 큰 차이를 보이는 값 혹은 레코드

4. 지렛대 (레버리지, Leverage)
- 회귀식에 한 레코드가 미치는 영향력의 정도  
- **유의어**: 햇값 (hat-value)

5. 비정규 잔차 (Non-normal Residual)
- 정규분포를 따르지 않는 잔차는 회귀분석의 요건을 무효로 만들 수 있음  
- 데이터 과학에서는 별로 중요하게 다루지 않음

6. 이분산성 (Heteroskedasticity)
- 어떤 범위 내 출력값의 잔차가 매우 넓은 분산을 보이는 경향  
- (어떤 예측변수를 회귀식에 놓치고 있다는 것을 의미할 수 있음)

7. 편차그림 (Partial Residual Plot)
- 결과변수와 특정 예측변수 사이의 관계를 진단하는 그림  
- **유의어**: 추가변수그림 (Added Variable Plot)


## 여기부터제대로다시











