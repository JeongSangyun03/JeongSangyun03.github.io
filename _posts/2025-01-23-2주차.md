![Image-1 (24)](https://github.com/user-attachments/assets/df52948d-679b-4e3c-bf6b-2086dbbb8eb9)---
layout : post
title : 2. 모델 훈련
categories : ML 기초
---
# 쿠다 ML 기초 2주차

이 장에서는 가장 간단한 모델인 선형 회귀와 비선형 데이터셋에서 훈련시킬 수 있는 조금 더 복잡한 모델인 다항회귀를 살펴보겠다.

***용어 정리***

**비선형 데이터셋 : 독립 변수(입력 특성)와 종속 변수(목표값) 간의 관계가 단순한 직선(선형 함수)으로 설명되지 않는 데이터셋**

**과대 적합 : 과대적합이란 머신러닝 모델이 훈련 데이터를 너무 잘 학습한 나머지, 테스트 데이터나 새로운 데이터에 대해 일반화 성능이 떨어지는 현상을 말함. 즉, 모델이 훈련 데이터의 잡음(noise)이나 세부적인 패턴까지 과도하게 학습하여, 실제로 중요한 일반적인 패턴을 제대로 잡아내지 못하는 상황.**

## 4.1 선형 회귀
선형 모델은 입력 특성의 가중치 합과 편향이라는 상수를 더해 예측을 만든다.
![Image-1 (1)](https://github.com/user-attachments/assets/545ed7b1-1c1c-4a10-9f17-a01072d207bd)
이는 벡터 형태로 다음과 같이 더 간단하게 쓸 수 있다.
![Image-1 (2)](https://github.com/user-attachments/assets/9d89c85a-a1df-4483-a13e-9c319aa92320)
이것이 선형 회귀 모델이다. 이를 훈련시킨다는 것은 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정하는 것이다. 이를 위해 먼저 모델이 훈련 데이터에 얼마나 잘 드어맞는지 측정해야 한다.
성능 측정 지표를 사용하자. 가장 널리 사용되는 성능 측정 지표는 평균 제곱근 오차(RMSE)이다. RMSE를 최소화하는 ∂를 찾아야 한다.
(실제로는 평균 제곱 오차(MSE)를 최소화하는 것이 같은 결과를 내면서 더 간단하다.)
![Image-1 (3)](https://github.com/user-attachments/assets/05193959-cdd3-42c7-afef-d851f0ec6358)
여기서 X는 데이터셋에 있는 모든 샘플의 모든 특성값(레이블 제외)을 포함하는 행렬이다. h는 시스템의 예측 함수이며 가설이라고도 한다.

### 4.1.1 정규 방정식
비용 함수를 최소화하는 ∂값을 찾기 위한 해석적인 방법이 있다. 여기서 이러한 결과를 바로 얻을 수 있는 수학 공식을 정규 방정식이라고 한다.
![Image-1 (4)](https://github.com/user-attachments/assets/bb41bc8c-7c05-49ea-be01-74509c232c25)
이 공식을 테스트하기 위해 선형처럼 보이는 데이터를 생성하겠다.

```python
import numpy as np
np.random.seed(42)
m = 100
X = 2*np.random.rand(m,1)
y = 4 + 3*X + np.random.randn(m,1)
```
이를 시각화하면 다음과 같다.

![image](https://github.com/user-attachments/assets/a2c2018c-02f1-44e0-ae67-a3c59d445690)

이제 정규 방정식을 사용해 ∂를 계산해보겠다.
넘파이 선형대수 모듈 (np.linalg)에 있는 inv() 함수를 사용해 역행렬을 계산하고 dot() 메소드를 사용해 행렬 곱셈을 수행하겠다.

```python
from sklearn.preprocessing import add_dummy_feature
X_b = add_dummy_feature (X) # 각 샘플에 x0 = 1을 추가한다.
theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y # 선형 회귀 모델의 최적 파라미터 벡터
```
이 데이터를 생성하기 위해 사용한 함수는 y = 4+3x1 + 가우스 잡음이다.
계산 결과는 다음과 같다.
array([[4.21509616],
       [2.77011339]])
∂dml 첫번째와 두번째 값인 4와 3을 기대했다. 비슷하지만 잡음 때문에 원래 함수의 파라미터를 정확하게 재현하지 못했다.
이처럼 데이터셋이 작고 잡음이 많을수록 정확한 값을 얻기 힘들다.
이번에는 예측의 ∂를 사용하여 예측해보겠다.
```python
X_new = np.array([[0],[2]]) # 새로운 입력데이터 생성
X_new_b = add_dummy_feature(X_new) #바이어스 추가 (모든 샘플 앞에 상수 1을 추가한다.)
y_predict = X_new_b @ theta_best # 새로운 입력 데이터에 대한 예측 값 계산 (@는 행렬 곱셈)
```
결과는 다음과 같다.

array([[4.21509616],
       [9.75532293]])
       
이를 그래프에 나타내보겠다.
```python
import matplotlib.pyplot as plt
plt.plot(X_new, y_predict, "r-", label = "예측") # 선형 회귀 모델의 예측값을 선형 그래프로 그린다. X_new = 새로운 입력값 (X)이다. y_predict = 새로운 입력값에 대한 모델의 예측 결과이다. 'r-' = 빨간색 실선을 의미한다. (- 선스타일 실선)
plt.plot(X, y, "b.") #실제 데이터 점을 파란색 점으로 그린다. X와y는 기존의 독립변수와 실제 종속 변수. b.는 점 스타일과 파란색을 의미한다.
plt.axis([0,2,0,15]) # 그래프의 축 범위 설정 (x축 범위 : 0~2, y축 범위 : 0~15)
plt.show()
```
이에 대한 결괏값은 아래와 같다.

![download](https://github.com/user-attachments/assets/72f990b9-b4cc-43c7-83c3-5433d92e689e)

위에서는 정규 방정식을 직접 구현하고, 더미 특성을 추가하여 직접 데이터를 변환하며 모든 과정을 수작업으로 처리했다.
장점으로는 수학적 이해와 직접 제어가 가능하다는 점이 있지만, 단점으로는 비효율성과 재사용성 부족이라는 점을 꼽을 수 있다. 그렇다면, 더 간단하게 할 수 있는 방법이 없을까?
사이킷런에서 선형 회귀를 수행하는 것은 비교적 간단하다.
```python
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression () #사이킷런의 API 사용 : 선형 회귀 모델 객체를 생성 
lin_reg.fit(X,y) #fit()를 호출하면 X와 y를 사용하여 모델을 학습
lin_reg.intercept_, lin_reg.coef_ #절편과 가중치를 속성에 저장
lin_reg.predict(X_new)
```
이는 사이킷런의 API를 사용하고, 자동화가 가능하며, 효율적인 내부 구현이 이루어진다. 이는 간결하고 사용하기 쉬우며, 확장성과 효율성이라는 특징을 가지고 있다. 
Linear Regression 클래스는 scipy 기반의 최소제곱법을 사용하는 것을 기반으로 한다. 이는 정규 방정식을 직접 계산하는 바식보다 **안정적이고, 효율적이며, 데이터가 클 때나 특이 행렬 문제가 있을 경우에도 잘 동작한다.**
다음과 같이 이 함수를 직접 호출할 수 있다.
```python
theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
theta_best_svd
```
array([[4.21509616],
       [2.77011339]])

이 함수는 아래의 식을 계산한다.
![image](https://github.com/user-attachments/assets/9966f426-ba8c-4d84-a02f-3ce5b1668f49)
여기서 X+는 X의 유사역행렬이다. np.linalg.inv()를 이용했을 때는 역행렬을 계산했었다. 이번에는 np.linalg.pinv()를 이용해 유사역행렬을 계산해보겠다.

**유사역행렬 : 유사 역행렬은 일반적인 역행렬이 존재하지 않는 경우에도 사용할 수 있는 대안적인 행렬
```pyton
np.linalg.pinv(X_b) @ y
```

결괏값은 동일하게 출력된다. 

array([[4.21509616],
       [2.77011339]])

유사역행렬 자체는 **(SVD) 특잇값 분해라는 표준 행렬 분해 기법**을 사용해 계산된다. 
아래를 참고할 것.

![image](https://github.com/user-attachments/assets/ee64fc0b-507c-42ab-a613-85287a4f316f)

이를 통해 정규방정식이 작동하지 않는 상황에서도 유사역행렬을 구할 수 있다.

### 4.1.2 계산 복잡도
정규 방정식과 SVD 방법 모두 특성 수가 많아지면 매우 느려진다. 역행렬 계산의 복잡도는 일반적으로 O*(n^2.4)에서 O*(n^3)이다.
학습된 선형 회귀 모델은 예측이 매우 빠르다. 예측 계산 복잡도는 샘플 수와 특성 수에 선형적이다. 다시 말해 예측하려는 샘플이 두 배로 늘어나면 걸리는 시간도 거의 두 배 증가한다.

특성의 수가 많거나 훈련 샘플이 너무 많아 메모리에 모두 담을 수 없을 때 적합한 다른 방법으로 선형 회귀 모델을 훈련시켜 보려 한다.

## 4.2 경사하강법
경사 하강법이 무엇인가? **경사하강법은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘이다.** 이 방법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조절해가는 것이다.
파라미터 벡터에 대해 비용 함수의 현재 그레이디언트를 계산하고, 그레이디언트가 감소하는 방향으로 진행한다. 그레이디언트가 0이 되면 최솟값에 도달한 것으로 본다.
구체적으로 보면 파라미터 벡터를 임의의 값으로 시작해서 (**이를 랜덤 초기화라고 한다.**) 한 번에 조금씩 비용 함수가 감소되는 방향으로 진행하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상시킨다.

![image](https://github.com/user-attachments/assets/05f3222d-b86c-426d-8335-027a88d1b2b8)

경사 하강법에서 중요한 파라미터는 각 단계, 즉 스텝의 크기로, **학습률** 하이퍼파라미터로 결정된다. 

**하이퍼 파라미터 : 머신러닝 모델을 학습시키기 전에 사용자가 직접 설정해야 하는 변수들을 말한다.**

학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 하므로 시간이 오래 걸린다.

![image](https://github.com/user-attachments/assets/6f5bc3f9-2007-4fa9-b34e-0921b7b97938)

한편, 학습률이 너무 크면 골짜기를 가로질러 반대편으로 건너뛰게 되어 이전보다 더 높은 곳으로 올라가게 될 수도 있다. 이는 알고리즘을 더 큰 값으로 발산하게 만들어 적절한 해법을 찾지 못하게 된다.

![image](https://github.com/user-attachments/assets/6756ca4a-5635-4c7a-950b-f6b9cb2f3910)

모든 비용 함수가 위와 같이 매끈하지 않다. 특이 지점이 있으면 최솟값으로 수렴하기 매우 어려운 상황이 발생한다. 다음 그림은 경사 하강법의 두 가지 문제점을 보여준다. 

![image](https://github.com/user-attachments/assets/8498a106-b70d-465b-9a0b-4c1132873509)

램덤 초기화 때문에 알고리즘이 왼쪽에서 시작하면 **전역 최솟값**보다 덜좋은 **지역 최솟값**에 수렴한다. 알고리즘이 오른쪽에서 시작하면 평탄한 지역을 지나기 위해 시간이 오래 걸리고 일찍 멈추게 되어 전역 최솟값에 도달하지 못한다.

다행히 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택해 선을 그어도 곡선을 가로지르지 않는 볼록 함수이다. 

![d4249533-bf1d-4dd0-ab81-7c9b39ed79ee](https://github.com/user-attachments/assets/7a4822fa-46e3-4b60-9d5c-9a1e942f86ea)

위처럼 MSE는 지역 최솟값이 없고 하나의 전역 최솟값만 있다. 또한 연속된 함수이고 기울기가 갑자기 변하지 않는다. 이 두 사실로부터 경사 하강법이 학습률이 높고 충분한 시간이 주어지면 전역 최솟값에 가깝게 접근할 수 있다는 것을 보장한다.

***결론적으로, 선형회귀의 MSE 비용함수는 항상 볼록 함수이며, 이는 학습 과정에서 전역 최적해를 보장한다.***

사실 비용 함수는 그릇 모양을 하고 있지만, 특성들의 스케일이 매우 다르면 길쭉한 모양일 수 있다. 다음 그림은 특성 1과 특성 2의 스케일이 같은 훈련 세트(왼쪽), 특성 1이 특성 2보다 더 작은 훈련 세트(오른쪽)에 대한 경사 하강법을 보여준다.

![image](https://github.com/user-attachments/assets/8389eaf8-5f8e-488c-89af-8565dcafc6aa)

***그래프 추가 설명***

**1. 그래프 설명**

이 그래프는 등고선의 형태를 나타낸다. 이는 비용 함수의 값이 일정한 지점을 나타내고, 중심으로 갈수록 비용이 작아지며 최적의 파라미터 벡터를 나타낸다. 특성 스케일링이 중요하다. 왼쪽 그림부터 살펴보자. 01과 02의 스케일이 같다면, 비용 함수의 등고선은 **대칭적인 원 모양**을 가진다. 이 경우 경사 하강법이 매 반복마다 **직접 중심으로 향하는 경로를 따른다.** 결과적으로 빠르고 효율적으로 최적점을 찾을 수 있다. 오른쪽 그림은 스케일이 다른 경우이다. 01과 02의 스케일이 다르다면 비용 함수의 등고선이 길쭉한 타원 모양이 된다. 경사 하강법은 최적점을 향해 나아가지만, **직접적인 경로를 따르지 못하고 지그재그로 움직이며 학습 속도가 느려진다.** 결과적으로 학습 과정이 비효율적이고 시간이 오래 걸린다. 

**2. 왜 스케일이 다른 경우 문제가 될까?**

경사 하강법은 비용 함수의 기울기를 따라 이동하면서 최적점을 찾는다. 특성 스케일이 다를 경우, 비용 함수의 경사가 특정 축에서 매우 가파르거나 완만하게 나타난다. 이로 인해 한 축에서는 작은 변화가 필요하고, 다른 축에서는 큰 변화가 필요한데, 이런 비대칭성이 학습 속도를 저하시킨다.

위 그림은 모델 훈련이 훈련 세트에서 비용 함수를 최소화하는 모델 파라미터의 조합을 찾는 일임을 설명해준다. 이를 모델의 **파라미터 공간**에서 찾는다고 말한다. 모델이 가진 파라미터가 많을수록 이 공간의 차원은 커지고 검색이 더 어려워진다. 다행히 선형 회귀의 경우 비용함수가 볼록 함수이기에 파라미터 벡터는 맨 아래에 있을 것이다.

### 4.2.1 배치 경사 하강법
경사 하강법을 구현하려면 각 모델파라미터에 대해 비용 함수의 그레이디언트를 계산해야 한다.

**용어 정리**

**모델 파라미더 : 모델이 데이터를 분석하고 학습하는 데 사용하는 가중치와 편향들을 의미한다. 이 값들은 훈련 데이터로부터 학습되며, 입력 데이터와 출력 데이터 간의 관계를 정의한다.**

**가중치 : 뉴럴 네트워크의 각 연결에서 신호의 중요도를 조정하는 값**

**편향 : 뉴런의 출력에 추가로 더해지는 값으로, 모델이 더 유연하게 학습하도록 도움**

**그레이디언트 : 비용 함수의 기울기. 즉, 비용 함수가 특정 파라미터 값에서 변화하는 방향과 크기를 나타내는 벡터. 특정 점에서 비용 함수의 그레이디언트는 기울기가 가장 가파르게 증가하는 방향을 가리키며, 그 크기는 변화의 정도를 나타냄**

이는 **편도함수**로 계산한다. 모델 파라미터가 조금 변경될 때 비용 함수가 얼마나 바뀌는지 계산하는 것이다.
다음은 파라미터에 대한 비용함수의 편도함수를 나타낸 식이다.

![Image-1 (7)](https://github.com/user-attachments/assets/b57d6eff-aff7-4671-be6b-d9b7e29f3d69)

편도함수를 각각 계산하는 대신 다음을 사용하여 한꺼번에 계산할 수도 있다.

![Image-1 (8)](https://github.com/user-attachments/assets/7a679916-9f2d-4c4a-ae0b-f006c138e3a3)

그레이디언트 벡터에 대한 식이다. 이는 비용함수의 편도함수를 모두 담고 있다.
이 공식은 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 계산한다. 그래서 이 알고리즘을 배치 경사 하강법이라고 한다. 즉, 매 스텝에서 훈련 데이터 전체를 사용한다. 이런 이유로 매우 큰 훈련 세트에서는 아주 느리다. 그러나 경사하강법은 수십만 개의 특성에서 선형 회귀를 훈련시킬 때 정규 방정식이나 SVD 분해보다 빠르다.

만약, 위로 향하는 그레이디언트 벡터가 구해지면 반대 방향인 아래로 가야한다. 

![Image-1 (9)](https://github.com/user-attachments/assets/adac4710-efbb-4a5e-81f5-a9518d4dae2b)

이렇게 theta에서 그레이디언트 벡터를 빼야 한다. 여기서 학습률인 에타가 사용된다. 내려가는 스텝의 크기를 결정하기 위해 그레이디언트 벡터에 에타를 곱한다.

이를 코드로 간단히 구현해보자.
```python
eta = 0.1 # 학습률
n_epochs = 1000 
m = len(X_b) # 샘플 개수

np.random.seed(42)
theta = np.random.randn(2,1) # 모델 파라미터를 랜덤하게 초기화한다.

for epoch in range (n_epochs) :
  gradients = 2 / m*X_b.T @ (X_b @ theta - y)
  theta = theta - eta * gradients
```
여기서 epoch는 훈련 세트를 한 번 반복하는 것을 의미한다. 계산된 theta는 다음과 같다.

array([[4.21509616],
       [2.77011339]])

이는 정규 방정식으로 찾은 것과 정확히 같다. **모델이 선형 회귀 문제에서 최적의 해(비용 함수 최소화)를 정확히 찾았음을 의미하는 것이다.** 선형 회귀 문제에서는 MSE 비용함수가 볼록 함수이기 때문에 최저점이 하나뿐이므로 정규 방정식으로 구한 값과 동일하다.

학습률 에타를 바꿔보면 어떨까? 앞서 경사 하강법은 비용함수의 기울기(그레이디언트)를 따라 최솟값 방향으로 이동하며, 이동할 거리는 기울기의 크기와 학습률에 의해 결정된다고 언급했다. 

![Image-1 (10)](https://github.com/user-attachments/assets/dbae3719-a07e-4a1a-8358-16df4c22d8cc)

다음은 학습률의 크기에 따른 경사하강법의 첫 20 스텝을 나타낸 것이다. 각 그래프에서 맨 아래에 있는 선은 랜덤한 시작점을 나타내며, 각 에포크는 점점 더 진한 선으로 표시된다. 해석을 해보자.

|그림|해석|
|--|--|
|왼쪽|학습률이 너무 낮다. 알고리즘은 최적점에 도달하겠지만 시간이 오래 걸릴 것이다.|
|가운데|학습률이 적당해 보인다. 반복 몇 번만에 이미 최적점에 수렴했다.|
|오른쪽|학습률이 너무 높다. 알고리즘이 이리저리 널뛰면서 최적점에서 점점 더 멀어져 발산한다.|

적절한 학습률을 찾기 위해 그리드 서치를 사용한다. 

**용어 정리**

**그리드 서치 : 모델 학습에 필요한 하이퍼파라미터를 최적화하기 위해 사용되는 기법. 여러 하이퍼파라미터 조합을 체계적으로 탐색하여 최적의 값을 찾는 방법. 이 과정에서 하이퍼파라미터로 학습률을 포함할 수 있다.**

![image](https://github.com/user-attachments/assets/b9dbc4f6-5c79-409a-ab8e-817a9589dd40)

하지만 그리드 서치에서 수렴하는 데 너무 오래 걸리는 모델이 제외될 수 있도록 반복 횟수를 제한해야 한다.

한복 횟수는 어떻게 지정할까? 반복횟수가 너무 작으면 최적점에 도달하기 전에 알고리즘이 멈춘다. 너무 크면 모델 파라미터가 더는 변하지 않는 동안 시간을 낭비하게 된다. 간단한 해결책은 반복횟수를 아주 크게 지정하고 그레이디언트 벡터가 아주 작아지면, 즉 벡터의 노름이 어떤 값 (허용오차)보다 작아지면 경사 하강법이 거의 최솟값에 도달한 것이므로 알고리즘을 중지하는 것이다.

허용오차의 범위 안에서 최적의 솔루션에 도달하기 위해서는 1/허용오차 의 반복이 필요할 수도 있다.
(ex : 더 정확한 값을 얻기 위해 허용 오차를 1/10으로 줄이면 알고리즘의 반복은 10배 늘어날 것이다.)

### 4.2.2 확률적 경사 하강법
앞서 배치 경사 하강법에 대해 살펴보았다. 이 방법의 가장 큰 문제는 매 스텝에서 전체 훈련 세트를 사용해 그레이디언트를 계산한다는 사실이다. 따ㄹ서 위에서 언급했듯이, 훈련 세트가 커지면 매우 느려지게 된다. 이와는 정반대로 확률적 경사 하강법은 매 스텝에서 한 개의 샘플을 랜덤으로 선택하고 그 하나의 샘플에 대한 그레이디언트를 게산하여 훨씬 빠르게 수행한다. 또한 매 반복에서 하나의 샘플만 메모리에 있으면 되므로 매우 큰 훈련 세트도 훈련할 수 있다. 문제는 랜덤이다. 그래서 이 알고리즘은 배치 경사 하강법보다 훨씬 불안정하다. 비용 함수가 최솟값에 다다를 때까지 부드럽게 감소하지 않고 위아래로 요동치며 평균적으로 감소한다. 시간이 지나면 최솟값에 매우 근접하겠으나 계속 요동쳐 최솟값에 안착하지 못한다. 알고리즘이 멈추면 좋은 파라미터가 구해지겠지만 최적치는 아니다. 

비용함수가 매우 불규칙할 때 알고리즘이 지역 최솟값을 건너뛰도록 도와주므로, 확률적 경사 하강법이 배치 경사 하강법보다 전역 최솟값을 찾을 가능성이 높다. 그럼에도 알고리즘을 전역 최솟값에 다다르지 못하게 한다는 점에서는 좋지 않다. ***이 딜레마를 해결하는 한 가지 방법은 학습률을 점진적으로 감소시키는 것이다. 시작할 때는 학습률을 크게 하여 수렴을 빠르게 하고 지역 최솟값에 빠지지 않도록 한다. 그러다가 점차 작게 줄여서 알고리즘이 전역 최솟값에 도달하게 한다.***

![Image-1 (11)](https://github.com/user-attachments/assets/7dad5c40-3a62-4ad3-8e64-c5b455c94fb5)

매 반복에서 학습률을 결정하는 함수를 **학습 스케줄**이라고 한다.
**학습률이 너무 빨리 줄어들면** 지역 최솟값에 갇히거나 최솟값까지 가는 중간에 멈출 수도 있다. 

**학습률이 너무 천천히 줄어들면** 오랫동안 최솟값 주변을 맴돌거나 훈련을 너무 일찍 중지해서 지역 최솟값에 머무를 수도 있다.

다음 코드는 간단한 학습 스케줄을 사용한 확률적 경사 하강법의 구현이다.
```python
n_epochs = 50
t0, t1 = 5, 50 # 학습 스케줄 하이퍼파라미터

def learning_schedule (t) :
  return t0 / (t + t1)

np.random.seed(42)
theta = np.random.randn(2,1) # 무작위 초기화

for epoch in range (n_epochs) :
  for iteration in range (m) : # m은 샘플 개수
    random_index = np.random.randint(m)
    xi = X_b[random_index:random_index+1]
    yi = y[random_index:random_index+1]
    gradients = 2 * xi.T @ (xi @ theta - yi)
    eta = learning_schedule (epoch * m + iteration)
    theta = theta - eta * gradients
```

theta의 값은 다음과 같다.

array([[4.21076011],
       [2.74856079]])

일반적으로 한 반복에서 m번 되풀이되는데 이때 각 반복을 이전처럼 에포크라고 한다. 배치 경사 하강법 코드가 전체 훈련 세트에 대해 1000번 반복하는 동안 이 코드는 훈련 세트에서 50번만 반복하고도 매우 좋은 값에 도달하였다.

![Image-1 (12)](https://github.com/user-attachments/assets/60634beb-8180-41da-ac3c-f1eaa022528b)

위 그림은 훈련의 첫 20 스텝을 보여준다. 해석해보자.

스텝이 굉장히 불규칙하다. 샘플을 랜덤으로 선택하기에 어떤 샘플은 한 에포크에서 여러 번 선택될 수 있고, 어떤 샘플은 전혀 선택되지 못할 수도 있다. 알고리즘이 에포크마다 모든 샘플을 사용하게 하려면 훈련 세트를 섞은 후 차례대로 하나씩 선택하고 다음 에포크에서 다시 섞는 방법을 사용할 수 있다. 하지만 이 방식은 더 복잡하고 일반적으로 결과가 더 향상되지 않는다. 

사이킷런에서 SGD 방식으로 선형 회귀를 사용하려면 기본값으로 제곱 오차 비용 함수를 최적화하는 SGDRegressor 클래스를 사용한다. 다음 코드는 최대 1000번 에포크 (max_iter) 동안 실행된다. 또는 100번 에포크 (n_iter_no_change) 동안 손실이 10^-5 (tol)보다 작아질 때까지 실행한다. 학습률 0.01 (eta0)로 기본 학습 스케줄을 사용한다. 규제는 전혀 사용하지 않았다. (penalty = none)
```python
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01, n_iter_no_change = 100, random_state=42)
sgd_reg.fit(X, y.ravel()) #fit()이 1D 타깃을 기대하기 때문에 y.ravel()로 쓴다.
sgd_reg.intercept_, sgd_reg.coef_
```

**REMIND**

**fit() : 모델 학습의 역할을 수행한다. 즉, 데이터를 사용해 모델의 가중치(parameter)을 최적화하는 과정이다. 이를 통해 모델이 주어진 입력 데이터 X와 타깃 데이터 y 간의 관계를 학습하게 된다.**

**sgd_reg.intercept_ : 선형 회귀 모델의 절편 파라미터**

**sgd_reg.coef_ : 모델의 기울기 계수(가중치)**

## 4.2.3 미니배치 경사 하강법
지금까지 배치 경사 하강법, 확률적 경사 하강법에 대해 살펴보았다. 마지막으로 살펴볼 경사 하강법 알고리즘은 미니배치 경사 하강법이다. 이는 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레이디언트를 계산한다. 확률적 경사 하강법에 비해 미니배치 경사 하강법의 주요 장점은 행렬 연산에 최적화된 하드웨어, 특히 GPU를 사용해서 성능을 향상시킬 수 있다는 점이다. 특히 미니배치를 어느 정도 크게 하면 알고리즘은 파라미터 공간에서 SGD보다 덜 뷸규칙하게 움직인다. 이는 미니배치 경사 하강법이 SGD보다 최솟값에 더 가까이 도달하게 될 것임을 의미한다. (SGD는 확률적 경사 하강법)

이제, 배치 경사 하강법, 확률적 경사 하강법, 미니배치 경사 하강법을 비교해보자.

다음 그림은 세 가지 경사 하강법 알고리즘이 훈련 과정 동안 파라미터 공간에서 움직인 경로이다.

![Image-1 (13)](https://github.com/user-attachments/assets/5d4e3908-5f60-4c2d-bd94-900135c5f9c1)

모두 최솟값 근처에 도달했지만, 배치 경사 하강버은 실제로 최솟값에서 멈춘 반면 확률적 경사 하강법과 미니배치 경사 하강법은 근처에서 맴돌고 있다. 그러나, 배치 경사 하강법의 한계점을 잊지 말자. 이는 매 스텝에서 많은 시간이 소요된다. 반면, 확률적 경사 하강법과 미니배치 경사 하강법의 경우에도 적절한 학습 스케줄을 사용하면 최솟값에 도달한다는 것 역시 잊지 말자.

마지막으로, 아래의 표를 보며 지금까지 논의한 알고리즘을 비교해보자. (m = 훈련 샘플 수, n = 특성 수)

![Image-1 (14)](https://github.com/user-attachments/assets/de413334-a7bc-4777-a9ed-58c3515fd511)

## 4.3 다항회귀
주어진 데이터가 단순한 직선보다 복잡한 형태라면 어떨까? ***이러한 비선형 데이터를 학습하는 데에도 선형 모델을 사용할 수 있다.*** 이렇게 하는 간단한 방법은 ***각 특성의 거듭제곱을 새로운 특성으로 추가하고, 이 확장된 특성을 포함한 데이터셋에 선형 모델을 훈련시키는 것***이다. 이를 **다항회귀**라고 한다.

2차 방정식의 기본형에 약간 잡음을 추가한 비선형 데이터를 생성해보자.

![Image-1 (15)](https://github.com/user-attachments/assets/5c6b3c40-f641-4994-8827-37341a4f0eeb)

```python
np.random.seed(42) # 시드 설정 (재현성을 위해 고정) => 동일한 난수 생성
m = 100 # 샘플 수
X = 6 * np.random.rand(m,1) - 3 # 입력 데이터 생성 (균등 분포에서 랜덤 샘플 생성) [0,1) 범위에서 mX1 크기의 랜덤값 생성 후 6으로 확장 & 3을 더하여, X는 [-3,3) 범위의 균등 분포로 변환
y = 0.5 * X ** 2 + X + 2 + np.random.randn(m,1) # 출력 데이터 생성 (2차 방정식 + 잡음 추가) # y = 0.5x^2 + x + 2 의 기본형에 잡음 np.random.randn(m,1)을 추가하여 비선형 데이터를 만듦
```
이를 사이킷런의 PolynomialFeatures를 사용해 훈련 데이터를 변환하려고 한다. 훈련 세트에 있는 각 특성을 제곱 (2차 다항)하여 새로운 특성으로 추가하자. (특성 하나가 추가됨)
```python
from sklearn.preprocessing import PolynomialFeatures
poly_features = PolynomialFeatures(degree=2, include_bias=False)
X_poly = poly_features.fit_transform(X)
```

X_poly는 원래 특성 X와 이 특성의 제곱 (새로운 특성 X^2)을 포함한다. 이제 선형 회귀를 적용하여 최적의 파라미터를 도출하겠다.
```python
lin_reg = LinearRegression()
lin_reg.fit (X_poly, y)
lin_reg.intercept_, lin_reg.coef_
```
결과부터 확인하고 비교를 우선 해보자.
결괏값은 

(array([1.78134581]), array([[0.93366893, 0.56456263]]))

실제로 원래 함수가 0.5x^2 + x + 2 + 가우스 잡음이었던 것과 비교한다면, 

예측된 모델은 0.56x^2 + 0.93x + 1.78로 유사하다.

![Image-1 (16)](https://github.com/user-attachments/assets/af00eb3d-9836-4d36-ab73-cef61d955432)


하나씩 분석해보자. 특성은 X를 나타내며, X는 모델의 입력 데이터이다. **특성을 추가하거나 변환할 경우**, X를 기반으로 한 파생 변수들도 특성으로 간주한다. ***예를 들어, X^2은 X에서 파생된 새로운 특성이다.***

![image](https://github.com/user-attachments/assets/1683c5c5-e166-4122-bd3a-0f4f61b2490e)

이런식으로 확장되는 것이다. 

특성을 확장하는 이유는 다음과 같다. 

이 방정식은 비선형 관계를 나타낸다. 기본적으로 선형회귀는 직선으로 데이터를 근사하는데, 그렇다면 이차방정식은 비선형 데이터이므로 제대로 학습하지 못한다. 따라서 PolynomialFeatures를 사용해 파생 특성을 추가하여 비선형 관계를 학습하는 것이다.

![image](https://github.com/user-attachments/assets/8ca7ce5c-0bf9-4c60-a865-1ccc050e1632)


특성이 여러 개일 때, 다항 회귀는 특성 사이의 관계를 찾을 수 있다. PolynomialFeatures가 주어진 차수까지 특성 간의 모든 교차항을 추가하기 때문이다.

예를 들어, 두 개의 특성 a와 b가 있을 때 degree=3으로 PolynomialFeatures를 적용하면 a^2, a^3, b^2, b^3 뿐만 아니라 ab, a^2b, ab^2도 특성으로 추가한다. 아래 설명을 참고할 것.

![Image-1 (17)](https://github.com/user-attachments/assets/4126ef1c-b79a-4e60-a658-331d980b2db2)


## 4.4 학습 곡선

고차 다항 회귀를 적용하면 일반 선형 회귀에서보다 훨씬 더 훈련 데이터에 잘 맞추려고 할 것이다. 예를 들어, 아래 그림과 같이 300차 다항 회귀 모델을 이전 훈련 데이터에 적용하면 비교했을 때 훈련 샘플에 가능한 한 가까이 가려고 구불구불하게 나타난다.

![Image-1 (18)](https://github.com/user-attachments/assets/e426c878-78ab-4a1a-983b-2ef2ead5f497)

비교해보자. 당연히 2차 방정식으로 생성한 데이터이기 때문에 일반화가 가장 잘 되는 모델은 2차 다항 회귀일 것이다. 그러나, 일반적으로는 어떤 함수로 데이터가 생성했는지 알 수 없다. 이 경우 300차 다항 회귀 모델은 과대적합, 1차 회귀 모델은 과소적합하다. 그렇다면, 어떻게 모델이 데이터에 과대적합 또는 과소적합한지 알 수 있을까?

### 1. 교차 검증 사용

앞서 교차 검증 방법에 대해 언급한 바 있다. 두 가지로 비교할 수 있다.

훈련 데이터에서,

|성능|교차 검증 점수|결과|
|---|---|---|
|좋음|나쁨|과대적합|
|나쁨|나쁨|과소적합|

위와 같이 분류할 수 있다.

### 2. 학습 곡선 확인

**학습 곡선 : 모델의 훈련 오차와 검증 오차를 훈련 반복 횟수의 함수로 나타낸 그래프**

1. 훈련하는 동안 훈련 세트와 검증 세트에서 일정한 간격으로 모델을 평가하고 그 결과를 그리면 된다.

2. 모델을 점진적으로 훈련할 수 없는 경우에는 훈련 세트의 크기를 점점 늘려가면서 여러 번 훈련하면 된다.

(예를 들어, 훈련 데이터가 10,000개라면 1,000개, 2,000개, 4,000개, ..., 10,000개와 같이 점진적으로 증가하는 크기의 데이터셋을 만듦. 그 후 각 서브셋으로 독립적으로 모델 훈련을 늘려가면서 함. 각 훈련 단계에서 훈련 세트와 검증 세트에 대한 손실 또는 오차 값을 기록하고, 기록된 값을 훈련 데이터 크기의 함수로 저장함)

2-1. 왜 이런 방법을 사용하는가? 

점진적으로 훈련할 수 없는 모델에서는 매번 처음부터 훈련을 다시 시작해야 한다. 이 방법은 모델의 일관된 학습 패턴을 확인할 수 있는 현실적인 대안이다. 점진적으로 데이터 크기를 늘려감으로써 훈련 데이터가 모델 성능에 미치는 영향을 시각적으로 확인할 수 있다.

다음 결과 해석을 참고하자.

![image](https://github.com/user-attachments/assets/8dade253-0842-4fab-aea1-9d6f4da6365f)

사이킷런에는 이를 위해 교차 검증을 사용하여 모델을 훈련하고 평가하는 learning_curve() 함수가 있다. 

**기본적으로** 이 함수는 훈련 세트의 크기를 증가시키면서 모델을 재훈련한다.

**모델이 점진적 학습을 지원하는 경우** learning_curve()를 호출할 때 exploit_incremental_learning = True로 지정하면 대신 모델을 점진적으로 훈련시킨다.

이 함수는 모델을 평가한 훈련 세트 크기를 반환하고, 각각의 크기와 교차 검증 폴드에서 측정한 훈련 및 검증 점수를 반환한다. 이를 이용하여 선형 회귀 모델의 학습 곡선을 다음 코드로 나타내겠다.

첫 번째는 선형 회귀 모델의 학습 곡선이다. 코드를 보자.

```python
from sklearn.model_selection import learning_curve

train_sizes, train_scores, valid_scores = learning_curve (LinearRegression(), X, y, train_sizes=np.linspace(0.01,1.0,40), cv = 5, scoring = "neg_root_mean_squared_error")
train_errors =  -train_scores.mean(axis = 1)
valid_errors = -valid_scores.mean(axis = 1)

import matplotlib.pyplot as plt
from matplotlib import rc
import platform

# 한글 폰트 설정
if platform.system() == "Windows":
    rc('font', family='Malgun Gothic')  # Windows: 맑은 고딕
elif platform.system() == "Darwin":
    rc('font', family='AppleGothic')  # MacOS: 애플 고딕
else:
    rc('font', family='NanumGothic')  # Linux: 나눔 고딕

# 음수 기호 깨짐 방지
plt.rcParams['axes.unicode_minus'] = False


plt.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train error")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="validation error")
# 레이블 추가
plt.title("learning_curve", fontsize=16)
plt.xlabel("train set size", fontsize=14)
plt.ylabel("RMSE", fontsize=14)

# 그리드 추가
plt.grid(True)

# 범례 추가
plt.legend(fontsize=12)
plt
```

![download](https://github.com/user-attachments/assets/d5ac2f12-58f8-448c-babb-fd27a98d2c83)


그래프의 선은 **훈련 오차와 검증 오차**를 나타낸다.
위 코드에 대한 결괏값이다. 해석해보자.

이 모델은 과소적합하다. 이유를 알기 위해 먼저 훈련 오차를 살펴보겠다.

그래프가 0에서 시작하므로 훈련 세트에 하나 혹은 두 개의 샘플이 있을 때 완벽하게 작동한다. 좀 더 구체적으로 이유를 알아보자면, ***훈련 샘플이 1~2개일 때 모델이 완벽하게 작동하는 이유는*** 모델이 이 데이터를 정확히 암기하거나 패턴을 단순히 맞추는 데 충분하다. 또한, 선형 회귀와 같은 모델은 데이터를 선형적으로 맞추기에, 소수의 데이터 포인트에 대해 손실이 0에 가까운 결과를 도출하기 쉽다. RMSE는 데이터 포인트가 적을수록 계산 결과가 더 작아질 가능성이 있다. 

***그러나, 훈련 세트가 커지면서 오차가 증가하는 이유는** 모델이 더 많은 데이터를 고려해야 하며, 이 과정에서 단순히 데이터 포인트를 암기하는 방식으로는 더 이상 오차를 줄일 수 없다. 또한, 모델이 전체 데이터를 적절히 학습하지 못한 상태라면, 데이터가 많아질수록 훈련 오차가 줄어들지만 여전히 높은 수준에 머물게 된다.

곡선이 어느정도 평편해질 때까지 오차는 계속 상승한다. 이 위치에서는 훈련 세트에 샘플이 추가되어도 평균 오차가 크게 나아지거나 나빠지지 않는다. 

이제 검증 오차를 보겠다. 모델이 적은 수의 훈련 샘플로 훈련될 때는 제대로 일반화될 수 없어서 검증 오차가 초기에 매우 크다. 모델에 훈련 샘플이 추가됨에 따라 학습이 되고 검증 오차가 천천히 감소한다. 하지만 선형 회귀의 직선은 데이터를 제대로 모델링할 수 없으므로 오차가 완만하게 감소하면서 훈련 세트의 그래프와 가까워진다.

결론적으로, 이는 모델이 훈련 데이터의 복잡한 패턴을 학습하지 못하고 있다는 것을 의미한다. 주어진 데이터에 대해 모델의 복잡도가 부족하고ㅓ나, 데이터 전처리가 충분하지 않을 수도 있다.

위의 그래프는 **과소적합 모델의 전형적인 모습**이다. (두 곡선이 수평한 구간을 만들고 꽤 높은 오차에서 매우 가까이 근접)

이제 **10차 다항 회귀 모델의 학습 곡선**을 같은 데이터에서 살펴보겠다.

```python
from sklearn.model_selection import learning_curve
from sklearn.pipeline import make_pipeline

polynomial_regression = make_pipeline(PolynomialFeatures(degree=2, include_bias=False), LinearRegression())

train_sizes, train_scores, valid_scores = learning_curve (polynomial_regression, X,y, train_sizes=np.linspace(0.01,1.0, 40), cv = 5, scoring = "neg_root_mean_squared_error")
train_errors = -train_scores.mean(axis = 1)
valid_errors = -valid_scores.mean(axis = 1)


import matplotlib.pyplot as plt
from matplotlib import rc
import platform

# 한글 폰트 설정
if platform.system() == "Windows":
    rc('font', family='Malgun Gothic')  # Windows: 맑은 고딕
elif platform.system() == "Darwin":
    rc('font', family='AppleGothic')  # MacOS: 애플 고딕
else:
    rc('font', family='NanumGothic')  # Linux: 나눔 고딕

# 음수 기호 깨짐 방지
plt.rcParams['axes.unicode_minus'] = False


plt.plot(train_sizes, train_errors, "r-+", linewidth=2, label="train error")
plt.plot(train_sizes, valid_errors, "b-", linewidth=3, label="validation error")
# 레이블 추가
plt.title("learning_curve", fontsize=16)
plt.xlabel("train set size", fontsize=14)
plt.ylabel("RMSE", fontsize=14)

# 그리드 추가
plt.grid(True)

# 범례 추가
plt.legend(fontsize=12)
plt
```
**책에서는 degree=10으로 설정해놨다. 근데 이러면 너무 크다. 2로 맞추겠다.**

이에 대한 결과는 다음과 같다.

![download](https://github.com/user-attachments/assets/a6e899ab-21f3-4d07-b812-be4273a3fd6c)

이 학습 곡선은 이전과 비슷해 보이지만, 두 가지 매우 중요한 차이점이 있다.

1. 훈련 데이터의 오차가 이전보다 훨씬 낮다.

2. 두 곡선 사이에 공간이 있다. 이 말은 검증 데이터에서보다 훈련 데이터에서 모델이 훨씬 더 나은 성능을 보인다는 뜻으로, 과대적합 모델의 특징이다. 그러나 더 큰 훈련 세트를 사용하면 두 곡선이 점점 가까워진다.

헷갈리면 과대적합의 정의를 생각하자.

![image](https://github.com/user-attachments/assets/61b90a7d-52a6-432c-92ac-5a3ed14ad0d8)

## 4.5 규제가 있는 선형 모델

과대적합을 줄이는 좋은 방법은 자유도를 줄이는 것이다. 즉, 모델을 규제(제한)하는 것이다. **다항 회귀 모델을 규제하는 간단한 방법**은 다항식의 차수를 줄이는 것이다.

**선형 회귀 모델을 규제하는 방법**은 모델의 가중치를 제한하는 것이다.

각기 다른 방법으로 가중치를 제한하는 *릿지 회귀, 라쏘 회귀, 엘라스틱넷 회귀*를 살펴보겠다.

### 4.5.1 릿지 회귀

**릿지 회귀**는 규제가 추가된 선형 회귀 버전이다. 이는 MSE에 규제항이 추가되어 예측 성능을 평가하는데 사용된다. 이는 학습 알고리즘을 데이터에 맞추는 것 뿐만 아니라 모델의 가중치가 가능한 한 작게 유지되도록 한다. 규제항은 훈련 중에만 비용 함수에 추가되며, 모델의 훈련이 끝나면 모델의 성능을 규제가 없는 MSE로 평가한다.

릿지 회귀의 비용함수를 살펴보자. 

![image](https://github.com/user-attachments/assets/5a324df9-7759-4e98-9520-7383b19f19bd)

**RSS : 실제값과 예측값의 오차를 제곱한 값의 sum. 작을수록 모델의 예측이 실제 데이터에 더 잘 맞는다.**

여기서 α는 하이퍼파라미터로, 모델을 얼마나 많이 규제할지 조절한다. α=0이면 릿지 회귀는 선형 회귀와 같아진다. α가 아주 크면 모든 가중치가 0에 가까워지고 결국 데이터의 평균을 지나는 수평선이 된다.

조금 더 자세히 알아보자.

**α가 커지면 가중치가 줄어드는 이유**

α가 커질수록 규제항이 손실함수에서 더 큰 비중을 차지한다. 모델은 RSS를 최소화하려는 동시에 (RSS가 작을수록 오차가 작은 것을 의미한다.)∥w∥를 가능한 한 0에 가깝게 만들려고 한다. α->무한대인 경우 규제항의 비용이 너무 커져, 손실 함수를 최소화하려면 w->0이 되어야 한다.

w->0이면, 회귀 방정식에서 

![image](https://github.com/user-attachments/assets/f6b24924-97f5-4855-a502-787a5d0c85cf)

w1, w2, ..., wn이 0에 가까워지고, 결국 y net ~= w0이 된다.

w0은 데이터의 평균값으로 계산되기 때문에, 모델의 예측값은 모든 입력 x에 대해 데이터의 평균값으로 수렴하게 된다.

직관적으로 이해하면,

릿지 회귀에서 α가 커질수록 모델은 점점 더 단순해지며, 복잡도를 완전히 제거하려고 한다. 결국, 모든 입력 데이터에 대해 동일한 값을 예측하는 극단적인 수평선 모델로 변하게 된다. 이는 과대적합을 완전히 방지하려는 지나친 정규화로 인해 데이터의 구조를 무시한 결과이다.

그림을 통해 이해해보자.

![Image-1 (20)](https://github.com/user-attachments/assets/8b43b4c6-ae73-431a-afea-581fee2e321a)

위 그림은 잡음이 많은 선형 데이터에 몇 가지 다른 α를 사용하여 릿지 모델을 훈련시킨 결과이다. 왼쪽 그래프는 평범한 릿지 모델을 사용하여 선형적인 예측을 만들었다. 오른쪽 그래프는 PolynomialFeatures(degree=10)을 이용하여 먼저 데이터를 확장하고 StandardScaler을 사용해 스케일을 조정한 후 릿지 모델을 적용했다. 이는 결국 릿지 규제를 사용한 다항 회귀가 된다. α가 증가할수록 직선에 가까워진다. 즉, 모델의 분산은 줄지만 편향은 커지게 된다.

선형회귀와 마찬가지로 릿지 회귀를 계산하기 위해 정규방정식이나 경사 하강법을 사용할 수 있다. 장단점은 선형회귀 때와 같다.

다음은 릿지 회귀의 정규방정식 해이다.

![image](https://github.com/user-attachments/assets/7ddfbae0-c13d-40e0-8249-e3e59c018b8c)

다음은 사이킷런에서 정규 방정식을 사용한 릿지 회귀를 적용하는 예이다.

```python
from sklearn.linear_model import Ridge
ridge_reg = Ridge (alpha = 0.1, solver = "cholesky") #alpha : 정규화 강도 조정의 하이퍼파라미터
ridge_reg.fit(X,y)
ridge_reg.predict([[1.5]]) # x = 1.5 (새로운 데이터 포인트의 독립 변수)의 예측값

```

![image](https://github.com/user-attachments/assets/db1ee013-c250-4268-9b6b-525b08b3961d)

학습된 릿지 회귀 모델은 학습된 가중치와 절편을 사용해 위의 수식을 계산한다.

다음은 확률의 경사 하강법을 이용할 때이다.

```python
sgd_reg = SGDRegressor (penalty = 'l2', alpha = 0.1 / m, tol = None, max_iter = 1000, eta0 = 0.01, random_state = 42) #penalty : 사용할 규제 지정 (l1,l2)
sgd_reg.fit (X,y.ravel())
sgd_reg.predict([[1.5]]))
```
이렇게 되면 위 정규방정식과 굉장히 유사한 결과가 나온다.

### 4.5.2 라쏘 회귀

라쏘 회귀는 선형 회귀의 또 다른 규제 버전이다. 릿지 회귀처럼 비용 함수에 규제항을 더하지만, l1 노름을 사용한다. 여기서는 2α를 곱한다. 이는 α값이 훈련 세트의 크기와 무관하도록 하기 위함이다. 노름이 달라지면 곱셈 계수가 다를 수 있다.

![image](https://github.com/user-attachments/assets/79113f91-e3bf-45cf-9647-49161f3d14a9)

위는 라쏘 회귀의 비용함수이다.

![Image-1 (21)](https://github.com/user-attachments/assets/ea17795a-9f73-49a5-b6bf-192fb9c30c34)

라쏘 회귀의 중요한 특징은 덜 중요한 특성의 가중치를 제거하려고 한다는 점이다. 예를 들어 오른쪽 그래프에서 α=0.01인 점선은 3차 방정식처럼 보인다. 차수가 높은 다항 특성의 가중치가 모두 0이 되었다는 의미이다. 

고차항은 데이터에 더 적은 영향을 미치거나, 잡음을 과대 적합할 가능성이 높기 때문에, α>0일 때 우선적으로 제거된다.

예를 들어보자.

![image](https://github.com/user-attachments/assets/9c5eb140-f398-4438-b3a9-43965a87b4ad)

이런식으로 동작하는 것이다. 그래프를 하나씩 분석해보면 다음과 같다.

![image](https://github.com/user-attachments/assets/8f44fe6b-921a-4d28-a772-494d0eb72004)

이처럼 라쏘 회귀는 자동으로 특성 선택을 수행하고 **희소 모델(0이 아닌 특성의 가중치가 적다는 것)** 을 만든다.

![Image-1 (22)](https://github.com/user-attachments/assets/8def8b4b-0c2b-4fac-977a-6a4aec2d1670)

그래프로 이해해보자. 두 축은 모델 파라미터 두 개를 나타내고, 배경의 등고선은 각기 다른 손실 함수를 나타낸다. 왼쪽 위 그래프의 등고선은 l1 손실을 나타내며 축에 가까워지면서 선형적으로 줄어든다. 오른쪽 위 그래프는 라쏘 손실 함수를 나타낸다. 이는 하얀 작은 원이 경사하강법이 초기화된 모델 파라미터를 최적화하는 모습을 보여준다.

두 회귀는 빨간 사각형인 전역 최적점에 도달한다. α가 증가하면 전역 최적점이 노란 점선을 따라 왼쪽으로 이동하며, α가 감소하면 전역 최적점이 오른쪽으로 이동한다. 이에 대해 조금 더 구체적으로 알아보자.

**α가 증가하면 (규제 강도 증가)**

라쏘 : α가 커지면 가중치가 더 강하게 0으로 수렴하려는 경향이 생긴다. 비용함수가 제약조건 (마름모 모양의 l1 페널티 영역)에 더 영향을 받고, 최적점이 좌표축 근처로 이동한다. 이로 인해 모델은 더 단순화되고, 일부 변수는 완전 제거된다.

릿지 : α가 커지면 규제항 비중이 커져 가중치가 작아지려는 경향이 강해진다. 비용함수의 최적점이 원형의 l2 제약조건 경계선에 더 가까워지며 전체적으로 가중치 크기가 줄어든다.


라쏘의 비용함수는 wㅑ = 0에서 미분 불가능하다. 하지만 쟈 = 0일때 서브그레이디언트 벡터 g를 사용하면 경사 하강법을 적용하는 데 문제가 없다. 

![Image-1 (23)](https://github.com/user-attachments/assets/9a63cb94-688e-4c4b-9769-fa3af21a8c63)

위 식은 경사 하강법을 위해 라쏘 비용 함수에 사용할 수 있는 서브그레이디언트 벡터 공식이다. 

다음은 Lasso 클래스를 사용한 간단한 사이킷런 코드이다.

```python
from sklearn.linear_model import Lasso
lasso_reg = Lasso(alpha = 0.1)
lasso_reg.fit (X,y)
lasso_reg.predict ([[1.5]])
```

### 4.5.3 엘라스틱넷

엘라스틱넷 회귀는 릿지 회귀와 라쏘 회귀를 절충한 모델이다. 규제항은 릿지와 라쏘의 규제항을 단순히 더한 것이며, 혼합 정도는 혼합 비율 r을 사용해 조절한다. r=0이면 엘라스틱넷은 릿지 회귀와 같고, r=1이면 라쏘 화귀와 같다.

![image](https://github.com/user-attachments/assets/dd4ee26a-4c58-445d-a8c3-58ac57bec2cc)

위 식의 로우가 r (조합 비율)과 같다.

![image](https://github.com/user-attachments/assets/cdb1ef16-0990-463e-b797-d70405757e69)

보통의 엘라스틱넷 회귀, 릿지 회귀, 라쏘 회귀, 일반적인 선형 회귀 (규제가 없는 모델)를 언제 사용해야 할까? 규제가 약간 있는 것이 대부분에 좋으므로 일반적으로 평범한 선형회귀는 피할 것. 릿지가 기본이 되지만 몇 가지 특성만 유용하다고 생각되면 라쏘나 엘라스틱넷이 낫다. 이 세 가지 회귀는 불필요한 특성의 가중치를 0으로 만들어준다. 특성 수가 훈련 샘플 수보다 많거나 특성 몇 개가 강하게 연관되어 있을 때는 보통 라쏘가 문제를 일으키므로 엘라스틱넷이 좋다.

사이킷런의 ElasticNet을 사용한 예시를 살펴보자.
```python
from sklearn.linear_model import ElasticNet
elastic_net = ElasticNet (alpha = 0.1, l1_ratio=0.5)
elastic_net.fit (X,y)
elastic_net.predict ([[1.5]])
```


### 4.5.4 조기종료

경사 하강법 같은 반복적인 학습 알고리즘을 규제하는 간단한 방법이 존재한다. 과대적합을 방지하는 색다른 방법이 존재하는 것이다. 이는 검증 오차가 최솟값에 도달하면 바로 훈련을 중지시키는 것이다. 이를 **조기종료**라고 한다.

![Image-1 (24)](https://github.com/user-attachments/assets/82db7891-22cd-44dd-9991-464094e76380)

이는 앞에서 사용한 2차 방정식 데이터셋에 배치 경사 하강법으로 훈련시킨 복잡한 모델을 보여준다. 에포크 (반복 횟수)가 진행됨에 따라 알고리즘이 점차 학습되어 훈련 세트에 대한 예측 오차 (RMSE)와 검증 세트에 대한 예측 오차가 줄어든다. 그러나 잠시 후 감소하던 검증 오차가 멈추었다가 다시 상승한다. 이는 모델이 훈련 데이터에 과대적합되기 시작했다는 것을 의미한다. (잡음까지 학습하거나, 데이터의 지나친 특화로 인해 일반적인 패턴 학습이 어려워지는 것) 

조기 종료는 검증 오차가 최소에 도달하는 즉시 훈련을 멈춘다.

```python
from copy import deepcopy
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

X_train, y_train, X_valid, y_valid = X[:40], y[:40], X[40:], y[40:]

preprocessing = make_pipeline(PolynomialFeatures(degree=10, include_bias=False), StandardScaler())
X_train_prep = preprocessing.fit_transform(X_train)
X_valid_prep = preprocessing.transform(X_valid)
sgd_reg = SGDRegressor(penalty=None, eta0 = 0.002, random_state=42)
n_epochs =500
best_valid_rmse = float('inf')

for epoch in range (n_epochs) :
  sgd_reg.partial_fit(X_train_prep, y_train)
  y_valid_predict = sgd_reg.predict(X_valid_prep)
  val_error = mean_squared_error (y_valid, y_valid_predict)
  if val_error < best_valid_rmse :
    best_valid_rmse = val_error
    best_model = deepcopy(sgd_reg)
```

이 코드는 먼저 다항 특성을 추가하고, 훈련 세트와 검증 세트 모두에 대해 모든 입력 특성의 스케일을 조정한다. 그 후 규제가 없고 학습률이 작은 SGDRegressor 모델을 생성한다. 훈련 반복에서는 partial_fit()을 사용하여 점진적인 학습을 수행한다. 각 에포크에서 검증 세트의 RMSE를 측정해 지금까지 확인된 가장 낮은 RMSE보다 작으면 best_model 변수에 모델 복사본을 저장한다. 이렇게 하여 학습 후 최상의 모델로 되돌린다. 

## 4.6 로지스틱 회귀

일부 회귀 알고리즘은 분류에서도 사용할 수 있다. **로지스틱 회귀**는 샘플이 특정 클래스에 속할 확률을 추정하는 데 널리 사용된다. (ex : 이 이메일이 스팸일 확률은?)
추정 확률이 주어진 임곗값 (일반적으로 50%)보다 크면 모델은 그 샘플이 해당 클래스에 속한다고 예측한다. (레이블 1인 양성 클래스). 그 반대라고도 예측한다. (레이블 0인 음성 클래스) 이를 이진 분류기라고 한다.

### 4.6.1 확률 추정
그렇다면, 로지스틱 회귀는 어떻게 작동하는 걸까? 선형 회귀 모델과 같이 로지스틱 회귀 모델은 입력 특성의 가중치 합을 계산하고, 편향을 더한다. 대신 선형회귀처럼 바로 결과를 출력하지 않고 결괏값의 로지스틱을 출력한다.

**로지스틱 : 0~1 사이의 값을 출력하는 시그모이드 함수**

이는 아래 그림과 같다.

![Image-1 (25)](https://github.com/user-attachments/assets/984bf7d3-5d30-4ac2-a1c6-bbfea8684a4d)

![image](https://github.com/user-attachments/assets/01291e0c-f772-43be-9327-4b5c457ac1a5)

여기서 -k(x-x0)를 편의상 t라고 치환하자.

t<0이면 0.5보다 작고, t>=0이면 0.5보다 크다. 이를 바탕으로 예측한다.

![Image-1 (26)](https://github.com/user-attachments/assets/b5b12937-3a9a-4c81-ad1f-e78c4bcf4680)

꽃잎의 너비를 기반으로 Iris-Versicolor 종을 감지하는 분류기를 만들어보자.

먼저, 데이터를 로드하고 살펴본 후, 이를 분할하고 훈련 세트에서 로지스틱 회귀 모델을 훈련시킨다. 그 다음 꽃잎의 너비가 0~3cm인 꽃에 대해 모델의 추정 확률을 계산해보겠다.
```python
from sklearn.datasets import load_iris
iris = load_iris (as_frame = True)
print (list(iris))
print (iris.data.head(3))
print (iris.target.head(3))
print (iris.target_names)
```
위 코드의 결과는 다음과 같다.

![image](https://github.com/user-attachments/assets/8fd6e04b-9b13-47c6-a0bf-7b10888b295e)

```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

X = iris.data[['petal width (cm)']].values
y = iris.target_names[iris.target] == 'virginica'
X_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 42)

log_reg = LogisticRegression (random_state = 42)
log_reg.fit (X_train, y_train)

X_new = np.linspace(0,3,100).reshape (-1,1)
y_proba = log_reg.predict_proba(X_new)
decision_boundary = X_new[y_proba[:,1] >= 0.5][0,0]

import matplotlib.pyplot as plt
from matplotlib import rc
import platform 

plt.plot(X_new, y_proba[:,0], "b--", linewidth=2, label="Non-Virginica")
plt.plot(X_new, y_proba[:,1], "g-", linewidth=2, label="Virginica")  
plt.plot([decision_boundary,decision_boundary],[0,1],'k:', linewidth =2, label = "decision boundary")
plt.show()
```

![Image-1 (27)](https://github.com/user-attachments/assets/6945e140-f9e5-4d27-bccb-d8b67c16a511)

Iris-Virginica의 꽃잎 너비는 1.4~2.5cm에 분포하고, 다른 붓꽃은 일반적으로 너비가 더 작아 0.1~1.8cm에 분포한다. 그래서 중첩되는 부분이 있다. 2cm이상인 꽃은 분류기가 Iris-Virginica로 강하게 확신하며, 1cm 미만이면 아니라고 강하게 확신한다. 이 두 극단 사이에는 분류가 확실하진 않으나, 가장 가능성 높은 클래스를 반환하여 예측한다. 양쪽 확률이 50%가 되는 1.6cm 근방에서 결정 경계가 만들어진다. 

![Image-1 (28)](https://github.com/user-attachments/assets/ac6ec43e-e407-4271-9a15-5c4b6267e19e)

위 그림은 같은 데이터셋을 보여주지만 꽃잎 너비와 꽃잎 길이라는 두 개의 특성으로 보여준다. 훈련이 끝나면 로지스틱 회귀 분류기가 이 특성들을 기반으로 새로운 꽃이 Iris-Virginica인지 확률을 추정할 수 있다. 점선이 모델이 50% 확률 추정 지점의 결정 경계이다. 이는 선형이다. 15%부터 90%까지 나란한 직선들은 모델이 특정 확률을 출력하는 포인트를 나타내고, 모델은 맨 오른쪽 직선을 넘는 꽃들을 90% 확률로 Iris-Virginica로 판단할 것이다.

### 4.6.4 소프트맥스 회귀





