---
layout : post
title : 2. 모델 훈련
categories : ML 기초
---
# 쿠다 ML 기초 2주차

이 장에서는 가장 간단한 모델인 선형 회귀와 비선형 데이터셋에서 훈련시킬 수 있는 조금 더 복잡한 모델인 다항회귀를 살펴보겠다.

***용어 정리***

**비선형 데이터셋 : 독립 변수(입력 특성)와 종속 변수(목표값) 간의 관계가 단순한 직선(선형 함수)으로 설명되지 않는 데이터셋**

**과대 적합 : 과대적합이란 머신러닝 모델이 훈련 데이터를 너무 잘 학습한 나머지, 테스트 데이터나 새로운 데이터에 대해 일반화 성능이 떨어지는 현상을 말함. 즉, 모델이 훈련 데이터의 잡음(noise)이나 세부적인 패턴까지 과도하게 학습하여, 실제로 중요한 일반적인 패턴을 제대로 잡아내지 못하는 상황.**

## 4.1 선형 회귀
선형 모델은 입력 특성의 가중치 합과 편향이라는 상수를 더해 예측을 만든다.
![Image-1 (1)](https://github.com/user-attachments/assets/545ed7b1-1c1c-4a10-9f17-a01072d207bd)
이는 벡터 형태로 다음과 같이 더 간단하게 쓸 수 있다.
![Image-1 (2)](https://github.com/user-attachments/assets/9d89c85a-a1df-4483-a13e-9c319aa92320)
이것이 선형 회귀 모델이다. 이를 훈련시킨다는 것은 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정하는 것이다. 이를 위해 먼저 모델이 훈련 데이터에 얼마나 잘 드어맞는지 측정해야 한다.
성능 측정 지표를 사용하자. 가장 널리 사용되는 성능 측정 지표는 평균 제곱근 오차(RMSE)이다. RMSE를 최소화하는 ∂를 찾아야 한다.
(실제로는 평균 제곱 오차(MSE)를 최소화하는 것이 같은 결과를 내면서 더 간단하다.)
![Image-1 (3)](https://github.com/user-attachments/assets/05193959-cdd3-42c7-afef-d851f0ec6358)
여기서 X는 데이터셋에 있는 모든 샘플의 모든 특성값(레이블 제외)을 포함하는 행렬이다. h는 시스템의 예측 함수이며 가설이라고도 한다.

### 4.1.1 정규 방정식
비용 함수를 최소화하는 ∂값을 찾기 위한 해석적인 방법이 있다. 여기서 이러한 결과를 바로 얻을 수 있는 수학 공식을 정규 방정식이라고 한다.
![Image-1 (4)](https://github.com/user-attachments/assets/bb41bc8c-7c05-49ea-be01-74509c232c25)
이 공식을 테스트하기 위해 선형처럼 보이는 데이터를 생성하겠다.

```python
import numpy as np
np.random.seed(42)
m = 100
X = 2*np.random.rand(m,1)
y = 4 + 3*X + np.random.randn(m,1)
```
이를 시각화하면 다음과 같다.

![image](https://github.com/user-attachments/assets/a2c2018c-02f1-44e0-ae67-a3c59d445690)

이제 정규 방정식을 사용해 ∂를 계산해보겠다.
넘파이 선형대수 모듈 (np.linalg)에 있는 inv() 함수를 사용해 역행렬을 계산하고 dot() 메소드를 사용해 행렬 곱셈을 수행하겠다.

```python
from sklearn.preprocessing import add_dummy_feature
X_b = add_dummy_feature (X) # 각 샘플에 x0 = 1을 추가한다.
theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y # 선형 회귀 모델의 최적 파라미터 벡터
```
이 데이터를 생성하기 위해 사용한 함수는 y = 4+3x1 + 가우스 잡음이다.
계산 결과는 다음과 같다.
array([[4.21509616],
       [2.77011339]])
∂dml 첫번째와 두번째 값인 4와 3을 기대했다. 비슷하지만 잡음 때문에 원래 함수의 파라미터를 정확하게 재현하지 못했다.
이처럼 데이터셋이 작고 잡음이 많을수록 정확한 값을 얻기 힘들다.
이번에는 예측의 ∂를 사용하여 예측해보겠다.
```python
X_new = np.array([[0],[2]]) # 새로운 입력데이터 생성
X_new_b = add_dummy_feature(X_new) #바이어스 추가 (모든 샘플 앞에 상수 1을 추가한다.)
y_predict = X_new_b @ theta_best # 새로운 입력 데이터에 대한 예측 값 계산 (@는 행렬 곱셈)
```
결과는 다음과 같다.

array([[4.21509616],
       [9.75532293]])
       
이를 그래프에 나타내보겠다.
```python
import matplotlib.pyplot as plt
plt.plot(X_new, y_predict, "r-", label = "예측") # 선형 회귀 모델의 예측값을 선형 그래프로 그린다. X_new = 새로운 입력값 (X)이다. y_predict = 새로운 입력값에 대한 모델의 예측 결과이다. 'r-' = 빨간색 실선을 의미한다. (- 선스타일 실선)
plt.plot(X, y, "b.") #실제 데이터 점을 파란색 점으로 그린다. X와y는 기존의 독립변수와 실제 종속 변수. b.는 점 스타일과 파란색을 의미한다.
plt.axis([0,2,0,15]) # 그래프의 축 범위 설정 (x축 범위 : 0~2, y축 범위 : 0~15)
plt.show()
```
이에 대한 결괏값은 아래와 같다.

![download](https://github.com/user-attachments/assets/72f990b9-b4cc-43c7-83c3-5433d92e689e)

위에서는 정규 방정식을 직접 구현하고, 더미 특성을 추가하여 직접 데이터를 변환하며 모든 과정을 수작업으로 처리했다.
장점으로는 수학적 이해와 직접 제어가 가능하다는 점이 있지만, 단점으로는 비효율성과 재사용성 부족이라는 점을 꼽을 수 있다. 그렇다면, 더 간단하게 할 수 있는 방법이 없을까?
사이킷런에서 선형 회귀를 수행하는 것은 비교적 간단하다.
```python
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression () #사이킷런의 API 사용 : 선형 회귀 모델 객체를 생성 
lin_reg.fit(X,y) #fit()를 호출하면 X와 y를 사용하여 모델을 학습
lin_reg.intercept_, lin_reg.coef_ #절편과 가중치를 속성에 저장
lin_reg.predict(X_new)
```
이는 사이킷런의 API를 사용하고, 자동화가 가능하며, 효율적인 내부 구현이 이루어진다. 이는 간결하고 사용하기 쉬우며, 확장성과 효율성이라는 특징을 가지고 있다. 
Linear Regression 클래스는 scipy 기반의 최소제곱법을 사용하는 것을 기반으로 한다. 이는 정규 방정식을 직접 계산하는 바식보다 **안정적이고, 효율적이며, 데이터가 클 때나 특이 행렬 문제가 있을 경우에도 잘 동작한다.**
다음과 같이 이 함수를 직접 호출할 수 있다.
```python
theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
theta_best_svd
```
array([[4.21509616],
       [2.77011339]])

이 함수는 아래의 식을 계산한다.
![image](https://github.com/user-attachments/assets/9966f426-ba8c-4d84-a02f-3ce5b1668f49)
여기서 X+는 X의 유사역행렬이다. np.linalg.inv()를 이용했을 때는 역행렬을 계산했었다. 이번에는 np.linalg.pinv()를 이용해 유사역행렬을 계산해보겠다.

**유사역행렬 : 유사 역행렬은 일반적인 역행렬이 존재하지 않는 경우에도 사용할 수 있는 대안적인 행렬
```pyton
np.linalg.pinv(X_b) @ y
```

결괏값은 동일하게 출력된다. 

array([[4.21509616],
       [2.77011339]])

유사역행렬 자체는 **(SVD) 특잇값 분해라는 표준 행렬 분해 기법**을 사용해 계산된다. 
아래를 참고할 것.

![image](https://github.com/user-attachments/assets/ee64fc0b-507c-42ab-a613-85287a4f316f)

이를 통해 정규방정식이 작동하지 않는 상황에서도 유사역행렬을 구할 수 있다.

### 4.1.2 계산 복잡도
정규 방정식과 SVD 방법 모두 특성 수가 많아지면 매우 느려진다. 역행렬 계산의 복잡도는 일반적으로 O*(n^2.4)에서 O*(n^3)이다.
학습된 선형 회귀 모델은 예측이 매우 빠르다. 예측 계산 복잡도는 샘플 수와 특성 수에 선형적이다. 다시 말해 예측하려는 샘플이 두 배로 늘어나면 걸리는 시간도 거의 두 배 증가한다.

특성의 수가 많거나 훈련 샘플이 너무 많아 메모리에 모두 담을 수 없을 때 적합한 다른 방법으로 선형 회귀 모델을 훈련시켜 보려 한다.

## 4.2 경사하강법
경사 하강법이 무엇인가? **경사하강법은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘이다.** 이 방법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조절해가는 것이다.
파라미터 벡터에 대해 비용 함수의 현재 그레이디언트를 계산하고, 그레이디언트가 감소하는 방향으로 진행한다. 그레이디언트가 0이 되면 최솟값에 도달한 것으로 본다.
구체적으로 보면 파라미터 벡터를 임의의 값으로 시작해서 (**이를 랜덤 초기화라고 한다.**) 한 번에 조금씩 비용 함수가 감소되는 방향으로 진행하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상시킨다.

![image](https://github.com/user-attachments/assets/05f3222d-b86c-426d-8335-027a88d1b2b8)

경사 하강법에서 중요한 파라미터는 각 단계, 즉 스텝의 크기로, **학습률** 하이퍼파라미터로 결정된다. 

**하이퍼 파라미터 : 머신러닝 모델을 학습시키기 전에 사용자가 직접 설정해야 하는 변수들을 말한다.**

학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 하므로 시간이 오래 걸린다.

![image](https://github.com/user-attachments/assets/6f5bc3f9-2007-4fa9-b34e-0921b7b97938)

한편, 학습률이 너무 크면 골짜기를 가로질러 반대편으로 건너뛰게 되어 이전보다 더 높은 곳으로 올라가게 될 수도 있다. 이는 알고리즘을 더 큰 값으로 발산하게 만들어 적절한 해법을 찾지 못하게 된다.

![image](https://github.com/user-attachments/assets/6756ca4a-5635-4c7a-950b-f6b9cb2f3910)

모든 비용 함수가 위와 같이 매끈하지 않다. 특이 지점이 있으면 최솟값으로 수렴하기 매우 어려운 상황이 발생한다. 다음 그림은 경사 하강법의 두 가지 문제점을 보여준다. 

![image](https://github.com/user-attachments/assets/8498a106-b70d-465b-9a0b-4c1132873509)

램덤 초기화 때문에 알고리즘이 왼쪽에서 시작하면 **전역 최솟값**보다 덜좋은 **지역 최솟값**에 수렴한다. 알고리즘이 오른쪽에서 시작하면 평탄한 지역을 지나기 위해 시간이 오래 걸리고 일찍 멈추게 되어 전역 최솟값에 도달하지 못한다.

다행히 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택해 선을 그어도 곡선을 가로지르지 않는 볼록 함수이다. 

![d4249533-bf1d-4dd0-ab81-7c9b39ed79ee](https://github.com/user-attachments/assets/7a4822fa-46e3-4b60-9d5c-9a1e942f86ea)

위처럼 MSE는 지역 최솟값이 없고 하나의 전역 최솟값만 있다. 또한 연속된 함수이고 기울기가 갑자기 변하지 않는다. 이 두 사실로부터 경사 하강법이 학습률이 높고 충분한 시간이 주어지면 전역 최솟값에 가깝게 접근할 수 있다는 것을 보장한다.

***결론적으로, 선형회귀의 MSE 비용함수는 항상 볼록 함수이며, 이는 학습 과정에서 전역 최적해를 보장한다.***

사실 비용 함수는 그릇 모양을 하고 있지만, 특성들의 스케일이 매우 다르면 길쭉한 모양일 수 있다. 다음 그림은 특성 1과 특성 2의 스케일이 같은 훈련 세트(왼쪽), 특성 1이 특성 2보다 더 작은 훈련 세트(오른쪽)에 대한 경사 하강법을 보여준다.

![image](https://github.com/user-attachments/assets/8389eaf8-5f8e-488c-89af-8565dcafc6aa)

***그래프 추가 설명***

**1. 그래프 설명**

이 그래프는 등고선의 형태를 나타낸다. 이는 비용 함수의 값이 일정한 지점을 나타내고, 중심으로 갈수록 비용이 작아지며 최적의 파라미터 벡터를 나타낸다. 특성 스케일링이 중요하다. 왼쪽 그림부터 살펴보자. 01과 02의 스케일이 같다면, 비용 함수의 등고선은 **대칭적인 원 모양**을 가진다. 이 경우 경사 하강법이 매 반복마다 **직접 중심으로 향하는 경로를 따른다.** 결과적으로 빠르고 효율적으로 최적점을 찾을 수 있다. 오른쪽 그림은 스케일이 다른 경우이다. 01과 02의 스케일이 다르다면 비용 함수의 등고선이 길쭉한 타원 모양이 된다. 경사 하강법은 최적점을 향해 나아가지만, **직접적인 경로를 따르지 못하고 지그재그로 움직이며 학습 속도가 느려진다.** 결과적으로 학습 과정이 비효율적이고 시간이 오래 걸린다. 

**2. 왜 스케일이 다른 경우 문제가 될까?**

경사 하강법은 비용 함수의 기울기를 따라 이동하면서 최적점을 찾는다. 특성 스케일이 다를 경우, 비용 함수의 경사가 특정 축에서 매우 가파르거나 완만하게 나타난다. 이로 인해 한 축에서는 작은 변화가 필요하고, 다른 축에서는 큰 변화가 필요한데, 이런 비대칭성이 학습 속도를 저하시킨다.

위 그림은 모델 훈련이 훈련 세트에서 비용 함수를 최소화하는 모델 파라미터의 조합을 찾는 일임을 설명해준다. 이를 모델의 **파라미터 공간**에서 찾는다고 말한다. 모델이 가진 파라미터가 많을수록 이 공간의 차원은 커지고 검색이 더 어려워진다. 다행히 선형 회귀의 경우 비용함수가 볼록 함수이기에 파라미터 벡터는 맨 아래에 있을 것이다.

### 4.2.1 배치 경사 하강법
경사 하강법을 구현하려면 각 모델파라미터에 대해 비용 함수의 그레이디언트를 계산해야 한다.

**용어 정리**

**모델 파라미더 : 모델이 데이터를 분석하고 학습하는 데 사용하는 가중치와 편향들을 의미한다. 이 값들은 훈련 데이터로부터 학습되며, 입력 데이터와 출력 데이터 간의 관계를 정의한다.**

**가중치 : 뉴럴 네트워크의 각 연결에서 신호의 중요도를 조정하는 값**

**편향 : 뉴런의 출력에 추가로 더해지는 값으로, 모델이 더 유연하게 학습하도록 도움**

**그레이디언트 : 비용 함수의 기울기. 즉, 비용 함수가 특정 파라미터 값에서 변화하는 방향과 크기를 나타내는 벡터. 특정 점에서 비용 함수의 그레이디언트는 기울기가 가장 가파르게 증가하는 방향을 가리키며, 그 크기는 변화의 정도를 나타냄**

이는 **편도함수**로 계산한다. 모델 파라미터가 조금 변경될 때 비용 함수가 얼마나 바뀌는지 계산하는 것이다.
다음은 파라미터에 대한 비용함수의 편도함수를 나타낸 식이다.

![Image-1 (7)](https://github.com/user-attachments/assets/b57d6eff-aff7-4671-be6b-d9b7e29f3d69)

편도함수를 각각 계산하는 대신 다음을 사용하여 한꺼번에 계산할 수도 있다.

![Image-1 (8)](https://github.com/user-attachments/assets/7a679916-9f2d-4c4a-ae0b-f006c138e3a3)

그레이디언트 벡터에 대한 식이다. 이는 비용함수의 편도함수를 모두 담고 있다.
이 공식은 매 경사 하강법 스텝에서 전체 훈련 세트 X에 대해 계산한다. 그래서 이 알고리즘을 배치 경사 하강법이라고 한다. 즉, 매 스텝에서 훈련 데이터 전체를 사용한다. 이런 이유로 매우 큰 훈련 세트에서는 아주 느리다. 그러나 경사하강법은 수십만 개의 특성에서 선형 회귀를 훈련시킬 때 정규 방정식이나 SVD 분해보다 빠르다.

만약, 위로 향하는 그레이디언트 벡터가 구해지면 반대 방향인 아래로 가야한다. 

![Image-1 (9)](https://github.com/user-attachments/assets/adac4710-efbb-4a5e-81f5-a9518d4dae2b)

이렇게 theta에서 그레이디언트 벡터를 빼야 한다. 여기서 학습률인 에타가 사용된다. 내려가는 스텝의 크기를 결정하기 위해 그레이디언트 벡터에 에타를 곱한다.

이를 코드로 간단히 구현해보자.
```python
eta = 0.1 # 학습률
n_epochs = 1000 
m = len(X_b) # 샘플 개수

np.random.seed(42)
theta = np.random.randn(2,1) # 모델 파라미터를 랜덤하게 초기화한다.

for epoch in range (n_epochs) :
  gradients = 2 / m*X_b.T @ (X_b @ theta - y)
  theta = theta - eta * gradients
```
여기서 epoch는 훈련 세트를 한 번 반복하는 것을 의미한다. 계산된 theta는 다음과 같다.

array([[4.21509616],
       [2.77011339]])

이는 정규 방정식으로 찾은 것과 정확히 같다. **모델이 선형 회귀 문제에서 최적의 해(비용 함수 최소화)를 정확히 찾았음을 의미하는 것이다.** 선형 회귀 문제에서는 MSE 비용함수가 볼록 함수이기 때문에 최저점이 하나뿐이므로 정규 방정식으로 구한 값과 동일하다.

학습률 에타를 바꿔보면 어떨까? 앞서 경사 하강법은 비용함수의 기울기(그레이디언트)를 따라 최솟값 방향으로 이동하며, 이동할 거리는 기울기의 크기와 학습률에 의해 결정된다고 언급했다. 

![Image-1 (10)](https://github.com/user-attachments/assets/dbae3719-a07e-4a1a-8358-16df4c22d8cc)

다음은 학습률의 크기에 따른 경사하강법의 첫 20 스텝을 나타낸 것이다. 각 그래프에서 맨 아래에 있는 선은 랜덤한 시작점을 나타내며, 각 에포크는 점점 더 진한 선으로 표시된다. 해석을 해보자.

|그림|해석|
|--|--|
|왼쪽|학습률이 너무 낮다. 알고리즘은 최적점에 도달하겠지만 시간이 오래 걸릴 것이다.|
|가운데|학습률이 적당해 보인다. 반복 몇 번만에 이미 최적점에 수렴했다.|
|오른쪽|학습률이 너무 높다. 알고리즘이 이리저리 널뛰면서 최적점에서 점점 더 멀어져 발산한다.|

적절한 학습률을 찾기 위해 그리드 서치를 사용한다. 

**용어 정리**

**그리드 서치 : 모델 학습에 필요한 하이퍼파라미터를 최적화하기 위해 사용되는 기법. 여러 하이퍼파라미터 조합을 체계적으로 탐색하여 최적의 값을 찾는 방법. 이 과정에서 하이퍼파라미터로 학습률을 포함할 수 있다.**

![image](https://github.com/user-attachments/assets/b9dbc4f6-5c79-409a-ab8e-817a9589dd40)

하지만 그리드 서치에서 수렴하는 데 너무 오래 걸리는 모델이 제외될 수 있도록 반복 횟수를 제한해야 한다.

한복 횟수는 어떻게 지정할까? 반복횟수가 너무 작으면 최적점에 도달하기 전에 알고리즘이 멈춘다. 너무 크면 모델 파라미터가 더는 변하지 않는 동안 시간을 낭비하게 된다. 간단한 해결책은 반복횟수를 아주 크게 지정하고 그레이디언트 벡터가 아주 작아지면, 즉 벡터의 노름이 어떤 값 (허용오차)보다 작아지면 경사 하강법이 거의 최솟값에 도달한 것이므로 알고리즘을 중지하는 것이다.

허용오차의 범위 안에서 최적의 솔루션에 도달하기 위해서는 1/허용오차 의 반복이 필요할 수도 있다.
(ex : 더 정확한 값을 얻기 위해 허용 오차를 1/10으로 줄이면 알고리즘의 반복은 10배 늘어날 것이다.)

### 4.2.2 확률적 경사 하강법
앞서 배치 경사 하강법에 대해 살펴보았다. 이 방법의 가장 큰 문제는 매 스텝에서 전체 훈련 세트를 사용해 그레이디언트를 계산한다는 사실이다. 따ㄹ서 위에서 언급했듯이, 훈련 세트가 커지면 매우 느려지게 된다. 이와는 정반대로 확률적 경사 하강법은 매 스텝에서 한 개의 샘플을 랜덤으로 선택하고 그 하나의 샘플에 대한 그레이디언트를 게산하여 훨씬 빠르게 수행한다. 또한 매 반복에서 하나의 샘플만 메모리에 있으면 되므로 매우 큰 훈련 세트도 훈련할 수 있다. 문제는 랜덤이다. 그래서 이 알고리즘은 배치 경사 하강법보다 훨씬 불안정하다. 비용 함수가 최솟값에 다다를 때까지 부드럽게 감소하지 않고 위아래로 요동치며 평균적으로 감소한다. 시간이 지나면 최솟값에 매우 근접하겠으나 계속 요동쳐 최솟값에 안착하지 못한다. 알고리즘이 멈추면 좋은 파라미터가 구해지겠지만 최적치는 아니다. 

비용함수가 매우 불규칙할 때 알고리즘이 지역 최솟값을 건너뛰도록 도와주므로, 확률적 경사 하강법이 배치 경사 하강법보다 전역 최솟값을 찾을 가능성이 높다. 그럼에도 알고리즘을 전역 최솟값에 다다르지 못하게 한다는 점에서는 좋지 않다. ***이 딜레마를 해결하는 한 가지 방법은 학습률을 점진적으로 감소시키는 것이다. 시작할 때는 학습률을 크게 하여 수렴을 빠르게 하고 지역 최솟값에 빠지지 않도록 한다. 그러다가 점차 작게 줄여서 알고리즘이 전역 최솟값에 도달하게 한다.***

![Image-1 (11)](https://github.com/user-attachments/assets/7dad5c40-3a62-4ad3-8e64-c5b455c94fb5)

매 반복에서 학습률을 결정하는 함수를 **학습 스케줄**이라고 한다.
**학습률이 너무 빨리 줄어들면** 지역 최솟값에 갇히거나 최솟값까지 가는 중간에 멈출 수도 있다. 

**학습률이 너무 천천히 줄어들면** 오랫동안 최솟값 주변을 맴돌거나 훈련을 너무 일찍 중지해서 지역 최솟값에 머무를 수도 있다.

다음 코드는 간단한 학습 스케줄을 사용한 확률적 경사 하강법의 구현이다.
```python
n_epochs = 50
t0, t1 = 5, 50 # 학습 스케줄 하이퍼파라미터

def learning_schedule (t) :
  return t0 / (t + t1)

np.random.seed(42)
theta = np.random.randn(2,1) # 무작위 초기화

for epoch in range (n_epochs) :
  for iteration in range (m) : # m은 샘플 개수
    random_index = np.random.randint(m)
    xi = X_b[random_index:random_index+1]
    yi = y[random_index:random_index+1]
    gradients = 2 * xi.T @ (xi @ theta - yi)
    eta = learning_schedule (epoch * m + iteration)
    theta = theta - eta * gradients
```

theta의 값은 다음과 같다.

array([[4.21076011],
       [2.74856079]])

일반적으로 한 반복에서 m번 되풀이되는데 이때 각 반복을 이전처럼 에포크라고 한다. 배치 경사 하강법 코드가 전체 훈련 세트에 대해 1000번 반복하는 동안 이 코드는 훈련 세트에서 50번만 반복하고도 매우 좋은 값에 도달하였다.

![Image-1 (12)](https://github.com/user-attachments/assets/60634beb-8180-41da-ac3c-f1eaa022528b)

위 그림은 훈련의 첫 20 스텝을 보여준다. 해석해보자.

스텝이 굉장히 불규칙하다. 샘플을 랜덤으로 선택하기에 어떤 샘플은 한 에포크에서 여러 번 선택될 수 있고, 어떤 샘플은 전혀 선택되지 못할 수도 있다. 알고리즘이 에포크마다 모든 샘플을 사용하게 하려면 훈련 세트를 섞은 후 차례대로 하나씩 선택하고 다음 에포크에서 다시 섞는 방법을 사용할 수 있다. 하지만 이 방식은 더 복잡하고 일반적으로 결과가 더 향상되지 않는다. 

사이킷런에서 SGD 방식으로 선형 회귀를 사용하려면 기본값으로 제곱 오차 비용 함수를 최적화하는 SGDRegressor 클래스를 사용한다. 다음 코드는 최대 1000번 에포크 (max_iter) 동안 실행된다. 또는 100번 에포크 (n_iter_no_change) 동안 손실이 10^-5 (tol)보다 작아질 때까지 실행한다. 학습률 0.01 (eta0)로 기본 학습 스케줄을 사용한다. 규제는 전혀 사용하지 않았다. (penalty = none)
```python
from sklearn.linear_model import SGDRegressor
sgd_reg = SGDRegressor(max_iter=1000, tol=1e-5, penalty=None, eta0=0.01, n_iter_no_change = 100, random_state=42)
sgd_reg.fit(X, y.ravel()) #fit()이 1D 타깃을 기대하기 때문에 y.ravel()로 쓴다.
sgd_reg.intercept_, sgd_reg.coef_
```

**REMIND**

**fit() : 모델 학습의 역할을 수행한다. 즉, 데이터를 사용해 모델의 가중치(parameter)을 최적화하는 과정이다. 이를 통해 모델이 주어진 입력 데이터 X와 타깃 데이터 y 간의 관계를 학습하게 된다.**

**sgd_reg.intercept_ : 선형 회귀 모델의 절편 파라미터**

**sgd_reg.coef_ : 모델의 기울기 계수(가중치)**

## 4.2.3 미니배치 경사 하강법
지금까지 배치 경사 하강법, 확률적 경사 하강법에 대해 살펴보았다. 마지막으로 살펴볼 경사 하강법 알고리즘은 미니배치 경사 하강법이다. 이는 미니배치라 부르는 임의의 작은 샘플 세트에 대해 그레이디언트를 계산한다. 확률적 경사 하강법에 비해 미니배치 경사 하강법의 주요 장점은 행렬 연산에 최적화된 하드웨어, 특히 GPU를 사용해서 성능을 향상시킬 수 있다는 점이다. 특히 미니배치를 어느 정도 크게 하면 알고리즘은 파라미터 공간에서 SGD보다 덜 뷸규칙하게 움직인다. 이는 미니배치 경사 하강법이 SGD보다 최솟값에 더 가까이 도달하게 될 것임을 의미한다. (SGD는 확률적 경사 하강법)

이제, 배치 경사 하강법, 확률적 경사 하강법, 미니배치 경사 하강법을 비교해보자.

다음 그림은 세 가지 경사 하강법 알고리즘이 훈련 과정 동안 파라미터 공간에서 움직인 경로이다.

![Image-1 (13)](https://github.com/user-attachments/assets/5d4e3908-5f60-4c2d-bd94-900135c5f9c1)

모두 최솟값 근처에 도달했지만, 배치 경사 하강버은 실제로 최솟값에서 멈춘 반면 확률적 경사 하강법과 미니배치 경사 하강법은 근처에서 맴돌고 있다. 그러나, 배치 경사 하강법의 한계점을 잊지 말자. 이는 매 스텝에서 많은 시간이 소요된다. 반면, 확률적 경사 하강법과 미니배치 경사 하강법의 경우에도 적절한 학습 스케줄을 사용하면 최솟값에 도달한다는 것 역시 잊지 말자.

마지막으로, 아래의 표를 보며 지금까지 논의한 알고리즘을 비교해보자. (m = 훈련 샘플 수, n = 특성 수)

![Image-1 (14)](https://github.com/user-attachments/assets/de413334-a7bc-4777-a9ed-58c3515fd511)




