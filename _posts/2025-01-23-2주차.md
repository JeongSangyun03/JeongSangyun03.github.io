---
layout : post
title : 2. 모델 훈련
categories : ML 기초
---
# 쿠다 ML 기초 2주차

이 장에서는 가장 간단한 모델인 선형 회귀와 비선형 데이터셋에서 훈련시킬 수 있는 조금 더 복잡한 모델인 다항회귀를 살펴보겠다.

***용어 정리***

**비선형 데이터셋 : 독립 변수(입력 특성)와 종속 변수(목표값) 간의 관계가 단순한 직선(선형 함수)으로 설명되지 않는 데이터셋**

**과대 적합 : 과대적합이란 머신러닝 모델이 훈련 데이터를 너무 잘 학습한 나머지, 테스트 데이터나 새로운 데이터에 대해 일반화 성능이 떨어지는 현상을 말함. 즉, 모델이 훈련 데이터의 잡음(noise)이나 세부적인 패턴까지 과도하게 학습하여, 실제로 중요한 일반적인 패턴을 제대로 잡아내지 못하는 상황.**

## 4.1 선형 회귀
선형 모델은 입력 특성의 가중치 합과 편향이라는 상수를 더해 예측을 만든다.
![Image-1 (1)](https://github.com/user-attachments/assets/545ed7b1-1c1c-4a10-9f17-a01072d207bd)
이는 벡터 형태로 다음과 같이 더 간단하게 쓸 수 있다.
![Image-1 (2)](https://github.com/user-attachments/assets/9d89c85a-a1df-4483-a13e-9c319aa92320)
이것이 선형 회귀 모델이다. 이를 훈련시킨다는 것은 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정하는 것이다. 이를 위해 먼저 모델이 훈련 데이터에 얼마나 잘 드어맞는지 측정해야 한다.
성능 측정 지표를 사용하자. 가장 널리 사용되는 성능 측정 지표는 평균 제곱근 오차(RMSE)이다. RMSE를 최소화하는 ∂를 찾아야 한다.
(실제로는 평균 제곱 오차(MSE)를 최소화하는 것이 같은 결과를 내면서 더 간단하다.)
![Image-1 (3)](https://github.com/user-attachments/assets/05193959-cdd3-42c7-afef-d851f0ec6358)
여기서 X는 데이터셋에 있는 모든 샘플의 모든 특성값(레이블 제외)을 포함하는 행렬이다. h는 시스템의 예측 함수이며 가설이라고도 한다.

### 4.1.1 정규 방정식
비용 함수를 최소화하는 ∂값을 찾기 위한 해석적인 방법이 있다. 여기서 이러한 결과를 바로 얻을 수 있는 수학 공식을 정규 방정식이라고 한다.
![Image-1 (4)](https://github.com/user-attachments/assets/bb41bc8c-7c05-49ea-be01-74509c232c25)
이 공식을 테스트하기 위해 선형처럼 보이는 데이터를 생성하겠다.

```python
import numpy as np
np.random.seed(42)
m = 100
X = 2*np.random.rand(m,1)
y = 4 + 3*X + np.random.randn(m,1)
```
이를 시각화하면 다음과 같다.
![image](https://github.com/user-attachments/assets/a2c2018c-02f1-44e0-ae67-a3c59d445690)
이제 정규 방정식을 사용해 ∂를 계산해보겠다.
넘파이 선형대수 모듈 (np.linalg)에 있는 inv() 함수를 사용해 역행렬을 계산하고 dot() 메소드를 사용해 행렬 곱셈을 수행하겠다.

```python
from sklearn.preprocessing import add_dummy_feature
X_b = add_dummy_feature (X) # 각 샘플에 x0 = 1을 추가한다.
theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y # 선형 회귀 모델의 최적 파라미터 벡터
```
이 데이터를 생성하기 위해 사용한 함수는 y = 4+3x1 + 가우스 잡음이다.
계산 결과는 다음과 같다.
array([[4.21509616],
       [2.77011339]])
∂dml 첫번째와 두번째 값인 4와 3을 기대했다. 비슷하지만 잡음 때문에 원래 함수의 파라미터를 정확하게 재현하지 못했다.
이처럼 데이터셋이 작고 잡음이 많을수록 정확한 값을 얻기 힘들다.
이번에는 예측의 ∂를 사용하여 예측해보겠다.
```python
X_new = np.array([[0],[2]]) # 새로운 입력데이터 생성
X_new_b = add_dummy_feature(X_new) #바이어스 추가 (모든 샘플 앞에 상수 1을 추가한다.)
y_predict = X_new_b @ theta_best # 새로운 입력 데이터에 대한 예측 값 계산 (@는 행렬 곱셈)
```
결과는 다음과 같다.
array([[4.21509616],
       [9.75532293]])
이를 그래프에 나타내보겠다.
```python
import matplotlib.pyplot as plt
plt.plot(X_new, y_predict, "r-", label = "예측") # 선형 회귀 모델의 예측값을 선형 그래프로 그린다. X_new = 새로운 입력값 (X)이다. y_predict = 새로운 입력값에 대한 모델의 예측 결과이다. 'r-' = 빨간색 실선을 의미한다. (- 선스타일 실선)
plt.plot(X, y, "b.") #실제 데이터 점을 파란색 점으로 그린다. X와y는 기존의 독립변수와 실제 종속 변수. b.는 점 스타일과 파란색을 의미한다.
plt.axis([0,2,0,15]) # 그래프의 축 범위 설정 (x축 범위 : 0~2, y축 범위 : 0~15)
plt.show()
```
이에 대한 결괏값은 아래와 같다.

![download](https://github.com/user-attachments/assets/72f990b9-b4cc-43c7-83c3-5433d92e689e)

위에서는 정규 방정식을 직접 구현하고, 더미 특성을 추가하여 직접 데이터를 변환하며 모든 과정을 수작업으로 처리했다.
장점으로는 수학적 이해와 직접 제어가 가능하다는 점이 있지만, 단점으로는 비효율성과 재사용성 부족이라는 점을 꼽을 수 있다. 그렇다면, 더 간단하게 할 수 있는 방법이 없을까?
사이킷런에서 선형 회귀를 수행하는 것은 비교적 간단하다.
```python
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression () #사이킷런의 API 사용 : 선형 회귀 모델 객체를 생성 
lin_reg.fit(X,y) #fit()를 호출하면 X와 y를 사용하여 모델을 학습
lin_reg.intercept_, lin_reg.coef_ #절편과 가중치를 속성에 저장
lin_reg.predict(X_new)
```
이는 사이킷런의 API를 사용하고, 자동화가 가능하며, 효율적인 내부 구현이 이루어진다. 이는 간결하고 사용하기 쉬우며, 확장성과 효율성이라는 특징을 가지고 있다. 
Linear Regression 클래스는 scipy 기반의 최소제곱법을 사용하는 것을 기반으로 한다. 이는 정규 방정식을 직접 계산하는 바식보다 **안정적이고, 효율적이며, 데이터가 클 때나 특이 행렬 문제가 있을 경우에도 잘 동작한다.**
다음과 같이 이 함수를 직접 호출할 수 있다.
```python
theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
theta_best_svd
```
array([[4.21509616],
       [2.77011339]])

이 함수는 아래의 식을 계산한다.
![image](https://github.com/user-attachments/assets/9966f426-ba8c-4d84-a02f-3ce5b1668f49)
여기서 X+는 X의 유사역행렬이다. np.linalg.inv()를 이용했을 때는 역행렬을 계산했었다. 이번에는 np.linalg.pinv()를 이용해 유사역행렬을 계산해보겠다.
**유사역행렬 : 유사 역행렬은 일반적인 역행렬이 존재하지 않는 경우에도 사용할 수 있는 대안적인 행렬,
```pyton
np.linalg.pinv(X_b) @ y
```

결괏값은 동일하게 출력된다. 
array([[4.21509616],
       [2.77011339]])

유사역행렬 자체는 (SVD) 특잇값 분해라는 표준 행렬 분해 기법을 사용해 계산된다. 
아래를 참고할 것.
![image](https://github.com/user-attachments/assets/ee64fc0b-507c-42ab-a613-85287a4f316f)

이를 통해 정규방정식이 작동하지 않는 상황에서도 유사역행렬을 구할 수 있다.

### 4.1.2 계산 복잡도
정규 방정식과 SVD 방법 모두 특성 수가 많아지면 매우 느려진다. 역행렬 계산의 복잡도는 일반적으로 O*(n^2.4)에서 O*(n^3)이다.
학습된 선형 회귀 모델은 예측이 매우 빠르다. 예측 계산 복잡도는 샘플 수와 특성 수에 선형적이다. 다시 말해 예측하려는 샘플이 두 배로 늘어나면 걸리는 시간도 거의 두 배 증가한다.

특성의 수가 많거나 훈련 샘플이 너무 많아 메모리에 모두 담을 수 없을 때 적합한 다른 방법으로 선형 회귀 모델을 훈련시켜 보려 한다.

## 4.2 경사하강법
경사 하강법이 무엇인가? **경사하강법은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘이다.** 이 방법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조절해가는 것이다.
파라미터 벡터에 대해 비용 함수의 현재 그레이디언트를 계산하고, 그레이디언트가 감소하는 방향으로 진행한다. 그레이디언트가 0이 되면 최솟값에 도달한 것으로 본다.
구체적으로 보면 파라미터 벡터를 임의의 값으로 시작해서 (**이를 랜덤 초기화라고 한다.**) 한 번에 조금씩 비용 함수가 감소되는 방향으로 진행하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상시킨다.

