---
layout : post
title : 2. 모델 훈련
categories : ML 기초
---
# 쿠다 ML 기초 2주차

이 장에서는 가장 간단한 모델인 선형 회귀와 비선형 데이터셋에서 훈련시킬 수 있는 조금 더 복잡한 모델인 다항회귀를 살펴보겠다.

***용어 정리***

**비선형 데이터셋 : 독립 변수(입력 특성)와 종속 변수(목표값) 간의 관계가 단순한 직선(선형 함수)으로 설명되지 않는 데이터셋**

**과대 적합 : 과대적합이란 머신러닝 모델이 훈련 데이터를 너무 잘 학습한 나머지, 테스트 데이터나 새로운 데이터에 대해 일반화 성능이 떨어지는 현상을 말함. 즉, 모델이 훈련 데이터의 잡음(noise)이나 세부적인 패턴까지 과도하게 학습하여, 실제로 중요한 일반적인 패턴을 제대로 잡아내지 못하는 상황.**

## 4.1 선형 회귀
선형 모델은 입력 특성의 가중치 합과 편향이라는 상수를 더해 예측을 만든다.
![Image-1 (1)](https://github.com/user-attachments/assets/545ed7b1-1c1c-4a10-9f17-a01072d207bd)
이는 벡터 형태로 다음과 같이 더 간단하게 쓸 수 있다.
![Image-1 (2)](https://github.com/user-attachments/assets/9d89c85a-a1df-4483-a13e-9c319aa92320)
이것이 선형 회귀 모델이다. 이를 훈련시킨다는 것은 모델이 훈련 세트에 가장 잘 맞도록 모델 파라미터를 설정하는 것이다. 이를 위해 먼저 모델이 훈련 데이터에 얼마나 잘 드어맞는지 측정해야 한다.
성능 측정 지표를 사용하자. 가장 널리 사용되는 성능 측정 지표는 평균 제곱근 오차(RMSE)이다. RMSE를 최소화하는 ∂를 찾아야 한다.
(실제로는 평균 제곱 오차(MSE)를 최소화하는 것이 같은 결과를 내면서 더 간단하다.)
![Image-1 (3)](https://github.com/user-attachments/assets/05193959-cdd3-42c7-afef-d851f0ec6358)
여기서 X는 데이터셋에 있는 모든 샘플의 모든 특성값(레이블 제외)을 포함하는 행렬이다. h는 시스템의 예측 함수이며 가설이라고도 한다.

### 4.1.1 정규 방정식
비용 함수를 최소화하는 ∂값을 찾기 위한 해석적인 방법이 있다. 여기서 이러한 결과를 바로 얻을 수 있는 수학 공식을 정규 방정식이라고 한다.
![Image-1 (4)](https://github.com/user-attachments/assets/bb41bc8c-7c05-49ea-be01-74509c232c25)
이 공식을 테스트하기 위해 선형처럼 보이는 데이터를 생성하겠다.

```python
import numpy as np
np.random.seed(42)
m = 100
X = 2*np.random.rand(m,1)
y = 4 + 3*X + np.random.randn(m,1)
```
이를 시각화하면 다음과 같다.

![image](https://github.com/user-attachments/assets/a2c2018c-02f1-44e0-ae67-a3c59d445690)

이제 정규 방정식을 사용해 ∂를 계산해보겠다.
넘파이 선형대수 모듈 (np.linalg)에 있는 inv() 함수를 사용해 역행렬을 계산하고 dot() 메소드를 사용해 행렬 곱셈을 수행하겠다.

```python
from sklearn.preprocessing import add_dummy_feature
X_b = add_dummy_feature (X) # 각 샘플에 x0 = 1을 추가한다.
theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y # 선형 회귀 모델의 최적 파라미터 벡터
```
이 데이터를 생성하기 위해 사용한 함수는 y = 4+3x1 + 가우스 잡음이다.
계산 결과는 다음과 같다.
array([[4.21509616],
       [2.77011339]])
∂dml 첫번째와 두번째 값인 4와 3을 기대했다. 비슷하지만 잡음 때문에 원래 함수의 파라미터를 정확하게 재현하지 못했다.
이처럼 데이터셋이 작고 잡음이 많을수록 정확한 값을 얻기 힘들다.
이번에는 예측의 ∂를 사용하여 예측해보겠다.
```python
X_new = np.array([[0],[2]]) # 새로운 입력데이터 생성
X_new_b = add_dummy_feature(X_new) #바이어스 추가 (모든 샘플 앞에 상수 1을 추가한다.)
y_predict = X_new_b @ theta_best # 새로운 입력 데이터에 대한 예측 값 계산 (@는 행렬 곱셈)
```
결과는 다음과 같다.

array([[4.21509616],
       [9.75532293]])
       
이를 그래프에 나타내보겠다.
```python
import matplotlib.pyplot as plt
plt.plot(X_new, y_predict, "r-", label = "예측") # 선형 회귀 모델의 예측값을 선형 그래프로 그린다. X_new = 새로운 입력값 (X)이다. y_predict = 새로운 입력값에 대한 모델의 예측 결과이다. 'r-' = 빨간색 실선을 의미한다. (- 선스타일 실선)
plt.plot(X, y, "b.") #실제 데이터 점을 파란색 점으로 그린다. X와y는 기존의 독립변수와 실제 종속 변수. b.는 점 스타일과 파란색을 의미한다.
plt.axis([0,2,0,15]) # 그래프의 축 범위 설정 (x축 범위 : 0~2, y축 범위 : 0~15)
plt.show()
```
이에 대한 결괏값은 아래와 같다.

![download](https://github.com/user-attachments/assets/72f990b9-b4cc-43c7-83c3-5433d92e689e)

위에서는 정규 방정식을 직접 구현하고, 더미 특성을 추가하여 직접 데이터를 변환하며 모든 과정을 수작업으로 처리했다.
장점으로는 수학적 이해와 직접 제어가 가능하다는 점이 있지만, 단점으로는 비효율성과 재사용성 부족이라는 점을 꼽을 수 있다. 그렇다면, 더 간단하게 할 수 있는 방법이 없을까?
사이킷런에서 선형 회귀를 수행하는 것은 비교적 간단하다.
```python
from sklearn.linear_model import LinearRegression
lin_reg = LinearRegression () #사이킷런의 API 사용 : 선형 회귀 모델 객체를 생성 
lin_reg.fit(X,y) #fit()를 호출하면 X와 y를 사용하여 모델을 학습
lin_reg.intercept_, lin_reg.coef_ #절편과 가중치를 속성에 저장
lin_reg.predict(X_new)
```
이는 사이킷런의 API를 사용하고, 자동화가 가능하며, 효율적인 내부 구현이 이루어진다. 이는 간결하고 사용하기 쉬우며, 확장성과 효율성이라는 특징을 가지고 있다. 
Linear Regression 클래스는 scipy 기반의 최소제곱법을 사용하는 것을 기반으로 한다. 이는 정규 방정식을 직접 계산하는 바식보다 **안정적이고, 효율적이며, 데이터가 클 때나 특이 행렬 문제가 있을 경우에도 잘 동작한다.**
다음과 같이 이 함수를 직접 호출할 수 있다.
```python
theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)
theta_best_svd
```
array([[4.21509616],
       [2.77011339]])

이 함수는 아래의 식을 계산한다.
![image](https://github.com/user-attachments/assets/9966f426-ba8c-4d84-a02f-3ce5b1668f49)
여기서 X+는 X의 유사역행렬이다. np.linalg.inv()를 이용했을 때는 역행렬을 계산했었다. 이번에는 np.linalg.pinv()를 이용해 유사역행렬을 계산해보겠다.

**유사역행렬 : 유사 역행렬은 일반적인 역행렬이 존재하지 않는 경우에도 사용할 수 있는 대안적인 행렬
```pyton
np.linalg.pinv(X_b) @ y
```

결괏값은 동일하게 출력된다. 

array([[4.21509616],
       [2.77011339]])

유사역행렬 자체는 (SVD) 특잇값 분해라는 표준 행렬 분해 기법을 사용해 계산된다. 
아래를 참고할 것.
![image](https://github.com/user-attachments/assets/ee64fc0b-507c-42ab-a613-85287a4f316f)

이를 통해 정규방정식이 작동하지 않는 상황에서도 유사역행렬을 구할 수 있다.

### 4.1.2 계산 복잡도
정규 방정식과 SVD 방법 모두 특성 수가 많아지면 매우 느려진다. 역행렬 계산의 복잡도는 일반적으로 O*(n^2.4)에서 O*(n^3)이다.
학습된 선형 회귀 모델은 예측이 매우 빠르다. 예측 계산 복잡도는 샘플 수와 특성 수에 선형적이다. 다시 말해 예측하려는 샘플이 두 배로 늘어나면 걸리는 시간도 거의 두 배 증가한다.

특성의 수가 많거나 훈련 샘플이 너무 많아 메모리에 모두 담을 수 없을 때 적합한 다른 방법으로 선형 회귀 모델을 훈련시켜 보려 한다.

## 4.2 경사하강법
경사 하강법이 무엇인가? **경사하강법은 여러 종류의 문제에서 최적의 해법을 찾을 수 있는 일반적인 최적화 알고리즘이다.** 이 방법의 기본 아이디어는 비용 함수를 최소화하기 위해 반복해서 파라미터를 조절해가는 것이다.
파라미터 벡터에 대해 비용 함수의 현재 그레이디언트를 계산하고, 그레이디언트가 감소하는 방향으로 진행한다. 그레이디언트가 0이 되면 최솟값에 도달한 것으로 본다.
구체적으로 보면 파라미터 벡터를 임의의 값으로 시작해서 (**이를 랜덤 초기화라고 한다.**) 한 번에 조금씩 비용 함수가 감소되는 방향으로 진행하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상시킨다.

![image](https://github.com/user-attachments/assets/05f3222d-b86c-426d-8335-027a88d1b2b8)

경사 하강법에서 중요한 파라미터는 각 단계, 즉 스텝의 크기로, **학습률** 하이퍼파라미터로 결정된다. 

**하이퍼 파라미터 : 머신러닝 모델을 학습시키기 전에 사용자가 직접 설정해야 하는 변수들을 말한다.**

학습률이 너무 작으면 알고리즘이 수렴하기 위해 반복을 많이 진행해야 하므로 시간이 오래 걸린다.

![image](https://github.com/user-attachments/assets/6f5bc3f9-2007-4fa9-b34e-0921b7b97938)

한편, 학습률이 너무 크면 골짜기를 가로질러 반대편으로 건너뛰게 되어 이전보다 더 높은 곳으로 올라가게 될 수도 있다. 이는 알고리즘을 더 큰 값으로 발산하게 만들어 적절한 해법을 찾지 못하게 된다.

![image](https://github.com/user-attachments/assets/6756ca4a-5635-4c7a-950b-f6b9cb2f3910)

모든 비용 함수가 위와 같이 매끈하지 않다. 특이 지점이 있으면 최솟값으로 수렴하기 매우 어려운 상황이 발생한다. 다음 그림은 경사 하강법의 두 가지 문제점을 보여준다. 

![image](https://github.com/user-attachments/assets/8498a106-b70d-465b-9a0b-4c1132873509)

램덤 초기화 때문에 알고리즘이 왼쪽에서 시작하면 **전역 최솟값**보다 덜좋은 **지역 최솟값**에 수렴한다. 알고리즘이 오른쪽에서 시작하면 평탄한 지역을 지나기 위해 시간이 오래 걸리고 일찍 멈추게 되어 전역 최솟값에 도달하지 못한다.

다행히 선형 회귀를 위한 MSE 비용 함수는 곡선에서 어떤 두 점을 선택해 선을 그어도 곡선을 가로지르지 않는 볼록 함수이다. 

![d4249533-bf1d-4dd0-ab81-7c9b39ed79ee](https://github.com/user-attachments/assets/7a4822fa-46e3-4b60-9d5c-9a1e942f86ea)

위처럼 MSE는 지역 최솟값이 없고 하나의 전역 최솟값만 있다. 또한 연속된 함수이고 기울기가 갑자기 변하지 않는다. 이 두 사실로부터 경사 하강법이 학습률이 높고 충분한 시간이 주어지면 전역 최솟값에 가깝게 접근할 수 있다는 것을 보장한다.

***결론적으로, 선형회귀의 MSE 비용함수는 항상 볼록 함수이며, 이는 학습 과정에서 전역 최적해를 보장한다.***

사실 비용 함수는 그릇 모양을 하고 있지만, 특성들의 스케일이 매우 다르면 길쭉한 모양일 수 있다. 다음 그림은 특성 1과 특성 2의 스케일이 같은 훈련 세트(왼쪽), 특성 1이 특성 2보다 더 작은 훈련 세트(오른쪽)에 대한 경사 하강법을 보여준다.

![image](https://github.com/user-attachments/assets/8389eaf8-5f8e-488c-89af-8565dcafc6aa)

***그래프 추가 설명***

**1. 그래프 설명**

이 그래프는 등고선의 형태를 나타낸다. 이는 비용 함수의 값이 일정한 지점을 나타내고, 중심으로 갈수록 비용이 작아지며 최적의 파라미터 벡터를 나타낸다. 특성 스케일링이 중요하다. 왼쪽 그림부터 살펴보자. 01과 02의 스케일이 같다면, 비용 함수의 등고선은 **대칭적인 원 모양**을 가진다. 이 경우 경사 하강법이 매 반복마다 **직접 중심으로 향하는 경로를 따른다.** 결과적으로 빠르고 효율적으로 최적점을 찾을 수 있다. 오른쪽 그림은 스케일이 다른 경우이다. 01과 02의 스케일이 다르다면 비용 함수의 등고선이 길쭉한 타원 모양이 된다. 경사 하강법은 최적점을 향해 나아가지만, **직접적인 경로를 따르지 못하고 지그재그로 움직이며 학습 속도가 느려진다.** 결과적으로 학습 과정이 비효율적이고 시간이 오래 걸린다. 

**2. 왜 스케일이 다른 경우 문제가 될까?**

경사 하강법은 비용 함수의 기울기를 따라 이동하면서 최적점을 찾는다. 특성 스케일이 다를 경우, 비용 함수의 경사가 특정 축에서 매우 가파르거나 완만하게 나타난다. 이로 인해 한 축에서는 작은 변화가 필요하고, 다른 축에서는 큰 변화가 필요한데, 이런 비대칭성이 학습 속도를 저하시킨다.

위 그림은 모델 훈련이 훈련 세트에서 비용 함수를 최소화하는 모델 파라미터의 조합을 찾는 일임을 설명해준다. 이를 모델의 **파라미터 공간**에서 찾는다고 말한다. 모델이 가진 파라미터가 많을수록 이 공간의 차원은 커지고 검색이 더 어려워진다. 다행히 선형 회귀의 경우 비용함수가 볼록 함수이기에 파라미터 벡터는 맨 아래에 있을 것이다.






