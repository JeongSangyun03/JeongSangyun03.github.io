---
layout : post
title : 쿠다 ML 기초 5주차
categories : ML 기초
---
#쿠다 ML 기초 5주차
이번 장에서는 차원 축소와 비지도 학습에 대하여 알아보고자 한다.
머신러닝 문제는 훈련 샘플 각각이 수천~수백만 개의 특성을 가지고 있다.
☞ 많은 특성 : 훈련 속도 저하 + 좋은 솔루션 탐색 어려움 : **차원의 저주**
☞ 해결 방법 : 특성 수를 줄여서 가능한 범위로 변경 가능 : **차원 축소**
차원 축소의 장점 : 훈련 속도 향상 (일반적) + 성능 향상의 가능성 (일반적인 것은 아님) + 데이터 시각화 유용성

## 8.1 차원의 저주
3차원 이상의 고차원 공간을 직관적으로 상상할 수 있는가? 어렵다.

![image](https://github.com/user-attachments/assets/49acdbd9-ae6e-4ebe-8cf7-7240f723366d)

고차원 공간에서는 많은 것이 상당히 다르게 작동한다. 
→ 저차원과 달리 고차원 초입방체에 있는 대다수의 점은 경계와 매우 가까이 있다.

→ 고차원은 많은 공간을 가지고 있어, 고차원 데이터셋이 매우 희박할 위험이 있다. (=새로운 샘플도 훈련 샘플과 멀리 떨어져 있을 가능성이 높다.) 
※ 예측을 위해 외삽을 해야 하기에 저차원일 때보다 예측이 더 불안정하며, 훈련 세트의 차원이 클수록 과대적합 위험이 커진다.

## 8.2 차원 축소를 위한 접근법
차원을 감소시키는 두 가지 주요한 접근법인 투영과 매니폴드 학습을 살펴보자.
### 8.2.1 투영
차원 축소에 중점을 맞춰둔 상태로 이해해보자. 대부분의 실전 문제에서는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져 있지 않다. 그러나 많은 특성은 MNIST 데이터셋의 숫자 그림처럼, 모든 훈련 샘플이 고차원 공간 안의 저차원 **부분 공간**에 놓여 있다. 

![image](https://github.com/user-attachments/assets/80021984-ee10-41e3-8243-527aa8321602)

위 그림과 같이 작은 공으로 표현된 3차원 데이터셋이 있다. 
☞ 모든 훈련 샘플이 거의 평면 형태로 존재 (고차원 공간에 있는 저차원 부분 공간)
☞ 모든 훈련 샘플을 이 부분 공간에 수직으로 투영 시 아래와 같은 2D 데이터셋 얻음

![image](https://github.com/user-attachments/assets/52304ce6-aa45-4fef-b796-4e42ce335d44)

차원 축소에 있어 투영이 언제나 최선의 방법은 아니다. 많은 경우 아래의 그림에 표현된 **스위스 롤** 데이터셋처럼 부분 공간이 뒤틀리거나 휘어 있기도 하다.

![image](https://github.com/user-attachments/assets/dddbda38-db7d-44b4-9c67-13ce418f5fce)

이를 그냥 평면에 투영시키면 왼쪽처럼 겹쳐서 나오지만, 우리가 원하는 것은 펼쳐진 오른쪽 2D 데이터셋이다.

![image](https://github.com/user-attachments/assets/5ea8e35b-1131-42fb-bda7-24ff5bee8efe)

### 8.2.2 매니폴드 학습

매니폴드란? : 어떤 공간이 d차원 매니폴드라는 것은, 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부 (d<n)
많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 **매니폴드**를 모델링하는 식으로 작동 : 매니폴드 학습

자유도 : **데이터를 만들 때 가질 수 있느 ㄴ가능한 변형의 수**
(ex : 28*28 픽셀의 흑백 이미지라면, 총 784개의 픽셀 조정 가능 & 각각의 픽셀은 0~255까지의 값을 가질 수 있어 **이론적으로 매우 많은 이미지 생성 가능**, 그러나 관심 있는 것은 숫자 이미지. 즉 특정한 패턴을 가진 이미지이므로 변형의 자유도가 낮아진다.)
데이터셋이 저차원 매니폴드로 압축될 수 있다는 의미 : 숫자들은 784차원 공간에 저차원 구조(매니폴드) 위에 놓여 있다.
즉, MNIST 데이터는 **784차원 공간 전체를 차지하는 게 아닌, 훨씬 작은 차원의 매니폴드에 놓여 있다.**
이를 활용하여 훨씬 적은 차원의 공간에서 효과적으로 표현할 수 있다.

매니폴드 가정이 저차원 매니폴드 공간에 표현되면 항상 더 간단해질까? 가정이 유효할까? 항상 그렇지 않다.

![image](https://github.com/user-attachments/assets/9fe57211-5982-4afb-b1de-7e49d50160e1)

1행의 그림은 결정 경계가 2D 공간에서 단순한 직선으로 표현되나, 2행의 그림은 3D 공간에서 단순한 수직 평면의 결정 경계가 펼쳐진 매니폴드에서는 더 복잡해졌다. 
즉, **전적으로 데이터셋에 달려있다.**

## 8.3 주성분 분석
가장 인기 있는 차원 축소 알고리즘인 **주성분 분석**에 대하여 알아보자. 먼저, 데이터에 가장 가까운 초평면을 정의한 후, 데이터를 이 평면에 투영시킨다.
### 8.3.1 분산 보존
먼저 올바른 초평면을 선택해야 한다. 예를 들어보자.

![image](https://github.com/user-attachments/assets/f010b978-6ccb-42d1-96cd-dc7cd305dd2d)

☞ 왼쪽 : 간단한 2D 데이터셋이 세 개의 축과 함께 표현됨
☞ 오른쪽 : 각 축이 투영된 결과

분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되므로 합리적이다. 
(=원본 데이터셋과 투영된 것 사이의 **평균 제곱 거리 최소화**)
### 8.3.2 주성분
PCA는 훈련 세트에서 분산이 최대인 축을 찾는다. (**데이터의 정보 최대한 유지**) 또한, 첫 번째 축에 직교하고 남은 분산을 최대한 보존하는 두 번째 축을 찾는다. (위 그림에서는 선택의 여지가 없어 점선이 됨)
※ 고차원 데이터셋은 PCA가 이전의 두 축에 직교하는 세 번째 축을 찾으며 데이터셋에 있는 차원의 수만큼 4,5,...n번째 축을 찾는다.

i번째 주성분 : 데이터의 i번째 축 (PC)
그림을 살펴보자.
첫 번째 PC : 벡터 C1 축
두 번째 PC : 벡터 C2 축
다시 돌아와서,

![image](https://github.com/user-attachments/assets/24592241-bd08-4c46-b231-c0324ce253af)

이 그림에서는 처음 두 개의 PC가 투영 평면에 있으며, 세 번째 PC가 이 평면에 수직인 축이다.
투영된 후, 

![image](https://github.com/user-attachments/assets/2be0d18b-dcd5-4f4c-b3c5-58d6f3cab4d7)

첫 번째 PC : Z1
두 번째 PC : Z2

훈련 세트의 주성분은 **특잇값 분해(SVD)** 라는 표준 행렬 분해 기술로 찾는다. 

아래의 파이썬 코드는 넘파이의 svd() 함수를 사용해 아래 그림의 3D 훈련 세트의 모든 주성분을 구한 후 처음 두 개의 PC를 정의하는 두 개의 단위 벡터를 추출한다.
```python
import numpy as np

# 3D 데이터 생성 (2D 평면에 가깝게)
np.random.seed(42)
n_samples = 100

# x1, x2는 [-1, 1] 범위에서 균등 분포를 따르는 점들
x1 = np.random.uniform(-1, 1, n_samples)
x2 = np.random.uniform(-1, 1, n_samples)

# x3는 특정 평면 (예: x3 ≈ 0) 주변에 노이즈를 추가하여 생성
x3 = 0.05 * np.random.randn(n_samples)  # 작은 노이즈 추가

# 데이터셋을 (n_samples, 3) 형태의 배열로 변환
X = np.vstack((x1, x2, x3)).T

# 중앙 정렬 (평균을 0으로 맞춤)
X_centered = X - X.mean(axis=0)

# SVD를 이용한 주성분 분석 (PCA)
U, s, Vt = np.linalg.svd(X_centered)

# 첫 번째와 두 번째 주성분 벡터 추출
c1 = Vt[0]  # 첫 번째 PC
c2 = Vt[1]  # 두 번째 PC
```

### 8.3.3 d차원으로 투영하기
주성분을 모두 추출했다면, 처음 d개의 주성분으로 정의한 초평면에 투영하여 데이터셋의 차원을 d차원으로 축소시킬 수 있다. 
예를 들어,

![image](https://github.com/user-attachments/assets/10ba14ba-6aab-4a0f-a644-ebf3ab3a96a0)

3D 데이터셋은 데이터셋의 분산이 가장 큰 두 개의 주성분으로 구성된 2D 평면에 투영되었다. 이 2D 투영은 원본 3D 데이터셋과 매우 비슷해보인다.

초평면에 훈련 세트를 투영하고 d차원으로 축소된 데이터셋 Xd-proj를 얻기 위해서는 아래 식과 같이 행렬 X와 V의 첫 d열로 구성된 행렬 Wd를 행렬 곱셈하면 된다.

![image](https://github.com/user-attachments/assets/681b906a-12fa-41fd-b8b4-d826fed702a4)

아래의 파이썬 코드는 첫 두 개의 주성분으로 정의된 평면에 훈련 세트를 투영한다.
```python
W2 = Vt[:2].T
X2D = X_centered @ W2
```

PCA 변환이 되었다. 지금까지 분산을 가능한 한 최대로 유지하면서 어떻게 데이터셋의 차원을 특정 차원을 ㅗ축소하는지 보았다.

### 8.3.4 사이킷런 이용하기
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X2D = pca.fit_transform(X)
```
사이킷런의 PCA 모델은 자동으로 데이터를 중앙에 맞춰준다. 이는 PCA 모델을 사용해 데이터셋의 차원을 2로 줄이는 코드이다.
PCA 변환기를 데이터셋에 학습시키고 나면 components_ 속성에 Wd의 전치가 담겨 있다. 이 배열의 행은 처음 d개의 주성분에 해당한다.

### 8.3.5 설명된 분산의 비율
explined_variance_ratio_ 변수에 저장된 주성분의 **설명된 분산의 비율**도 유용한 정보다. 이는 각 주성분의 축을 따라 있는 데이터셋의 분산 비율을 나타낸다. 
```python
pca.explained_variance_ratio_
```
### 8.3.6 적절한 차원 수 선택
축소할 차원 수를 임의로 정하기보다는 충분한 분산이 될 때까지 더해야 할 차원 수를 선택하는 것이 좋다.
PCA 차원 축소 목표 : 데이터의 중요한 정보를 최대한 유지하면서 불필요한 차원 제거 (**최소한의 차원으로 최대한의 분산 유지**)

(주성분 벡터 개수 = 최종적으로 선택한 차원의 수)

MNIST 데이터셋을 로드하고 분할한 후, 차원을 줄이지 않고 PCA를 수행해보자. 그 후 훈련 집합의 분산 95%를 보존하는 데 필요한 최소 차원 수를 계산하자.

```python
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', as_frame=False)
X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]
X_test, y_test = mnist.data[60_000:],mnist.target[60_000:]

pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1
```
그 후, n_components = d로 설정하여 PCA를 다시 실행한다. 보존하려는 분산의 비율을 n_components에서 0~1 사이로 설정하는 게 좋다.

```python
pca = PCA (n_components = 0.95)
X_reduced = pca.fit_transform (X_train)
```
실제 주성분 개수는 훈련 중 결정되며, n_components_ 속성에 저장된다.
```python
pca.n_components_
```
다른 방법은 설명된 분산을 차원 수에 대한 함수로 그리는 것이다. 

![image](https://github.com/user-attachments/assets/46ff1141-4293-4ef9-a9a9-65173d9b2c1b)

이 그래프에서는 설명된 분산의 빠른 성장이 멈추는 변곡점이 있다. 여기서는 차원을 약 100으로 축소해도 설명된 분산 손해를 보지 않을 것이다.

지도 학습 작업 (분류)의 전처리 단계로 차원 축소를 사용하는 경우, 다른 하이퍼파라미터와 마찬가지로 차원 수를 튜닝할 수 있다. 

아래의 코드에서는 두 단계로 구성된 파이프라인을 생성한다. 먼저 PCA를 사용하여 차원을 줄인 후, 랜덤 포레스트를 사용하여 분류를 수행한다. 그 후 RandomizedSearchCV를 사용해 PCA와 랜덤 포레스트 분류기에 잘 맞는 하이퍼파라미터 조합을 찾는다. 

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.pipeline import make_pipeline

clf = make_pipeline (PCA(random_state=42), RandomForestClassifier(random_state=42)) 
param_distrib = {
    "pca__n_components": np.arange(10,80),
    "randomforestclassifier__n_estimators": np.arange(10,100)
}
rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3, random_state=42)
rnd_search.fit(X_train[:1000], y_train[:1000])
```
앞에서 찾은 최상의 하이퍼파라미터를 살펴보자.
```python 
print (rnd_search.best_params_)
```




