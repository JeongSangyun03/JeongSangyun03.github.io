---
layout : post
title : 쿠다 ML 기초 5주차
categories : ML 기초
---
#쿠다 ML 기초 5주차
이번 장에서는 차원 축소와 비지도 학습에 대하여 알아보고자 한다.
머신러닝 문제는 훈련 샘플 각각이 수천~수백만 개의 특성을 가지고 있다.
☞ 많은 특성 : 훈련 속도 저하 + 좋은 솔루션 탐색 어려움 : **차원의 저주**
☞ 해결 방법 : 특성 수를 줄여서 가능한 범위로 변경 가능 : **차원 축소**
차원 축소의 장점 : 훈련 속도 향상 (일반적) + 성능 향상의 가능성 (일반적인 것은 아님) + 데이터 시각화 유용성

## 8.1 차원의 저주
3차원 이상의 고차원 공간을 직관적으로 상상할 수 있는가? 어렵다.

![image](https://github.com/user-attachments/assets/49acdbd9-ae6e-4ebe-8cf7-7240f723366d)

고차원 공간에서는 많은 것이 상당히 다르게 작동한다. 
→ 저차원과 달리 고차원 초입방체에 있는 대다수의 점은 경계와 매우 가까이 있다.

→ 고차원은 많은 공간을 가지고 있어, 고차원 데이터셋이 매우 희박할 위험이 있다. (=새로운 샘플도 훈련 샘플과 멀리 떨어져 있을 가능성이 높다.) 
※ 예측을 위해 외삽을 해야 하기에 저차원일 때보다 예측이 더 불안정하며, 훈련 세트의 차원이 클수록 과대적합 위험이 커진다.

## 8.2 차원 축소를 위한 접근법
차원을 감소시키는 두 가지 주요한 접근법인 투영과 매니폴드 학습을 살펴보자.
### 8.2.1 투영
차원 축소에 중점을 맞춰둔 상태로 이해해보자. 대부분의 실전 문제에서는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져 있지 않다. 그러나 많은 특성은 MNIST 데이터셋의 숫자 그림처럼, 모든 훈련 샘플이 고차원 공간 안의 저차원 **부분 공간**에 놓여 있다. 

![image](https://github.com/user-attachments/assets/80021984-ee10-41e3-8243-527aa8321602)

위 그림과 같이 작은 공으로 표현된 3차원 데이터셋이 있다. 
☞ 모든 훈련 샘플이 거의 평면 형태로 존재 (고차원 공간에 있는 저차원 부분 공간)
☞ 모든 훈련 샘플을 이 부분 공간에 수직으로 투영 시 아래와 같은 2D 데이터셋 얻음

![image](https://github.com/user-attachments/assets/52304ce6-aa45-4fef-b796-4e42ce335d44)

차원 축소에 있어 투영이 언제나 최선의 방법은 아니다. 많은 경우 아래의 그림에 표현된 **스위스 롤** 데이터셋처럼 부분 공간이 뒤틀리거나 휘어 있기도 하다.

![image](https://github.com/user-attachments/assets/dddbda38-db7d-44b4-9c67-13ce418f5fce)

이를 그냥 평면에 투영시키면 왼쪽처럼 겹쳐서 나오지만, 우리가 원하는 것은 펼쳐진 오른쪽 2D 데이터셋이다.

![image](https://github.com/user-attachments/assets/5ea8e35b-1131-42fb-bda7-24ff5bee8efe)

### 8.2.2 매니폴드 학습




