---
layout : posts
title : "쿠다 ML 기초 4주차"
categories : "쿠다 ML 기초"
---
# 16. RNN과 어텐션을 사용한 자연어 처리

## 16.1 Char-RNN으로 셰익스피어 문체 생성하기
Char-RNN을 훈련하여 문장에서 다음 글자를 예측하는 방법 : 한 번에 한 글자씩 새로운 텍스트를 생성

### 16.1.1 데이터셋 생성하기
셰익스피어 문체 생성 방법
1. 안드레이 카르파시의 Char-RNN 프로젝트 셰익스피어 작품 다운로드
2. 셰익스피어 텍스트를 문자 단위로 벡터화 (split="character" 사용하여 문자 단위로 쪼갠 후 숫자로 바꿈)
3. RNN 학습을 위해 입력 시퀀스와 타깃 시퀀스로 나누는 데이터셋을 생성 (타깃 시퀀스로 다음 글자 예측하도록 구성)
4. 훈련 세트, 검증 세트, 테스트 세트 생성

### 16.1.2 Char-RNN 모델 만들고 훈련하기
언어 모델링 → 데이터셋 多 : 단순한 RNN 이상의 것 필요 (단순 RNN은 오래된 정보 기억 ↓) 

예제에서는 128개의 유닛으로 구성된 하나의 GRU층을 가진 모델을 구축하고 훈련 (정보를 잘 기억하며 효율적인 학습 가능)

```python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),  # ❶
    tf.keras.layers.GRU(128, return_sequences=True),
    tf.keras.layers.Dense(n_tokens, activation="softmax")          # ❷
])

model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer="nadam",
    metrics=["accuracy"]
)

model_ckpt = tf.keras.callbacks.ModelCheckpoint(
    "my_shakespeare_model", monitor="val_accuracy", save_best_only=True  # ❸
)

history = model.fit(
    train_set,
    validation_data=valid_set,
    epochs=10,
    callbacks=[model_ckpt]
)
```
이미 전처리 된 (벡터화된) 숫자 시퀀스만 입력으로 받기에, TextVectorization을 포함한 최종 모델은 원본 텍스트 → 문자 단위 벡터화 → 문맥 이해 → 다음 글자 예측까지 한 번에 처리

### 16.1.3 가짜 셰익스피어 텍스트 생성하기
