---
layout : posts
title : "쿠다 ML 기초 4주차"
categories : "쿠다 ML 기초"
---
# 16. RNN과 어텐션을 사용한 자연어 처리

## 16.1 Char-RNN으로 셰익스피어 문체 생성하기
Char-RNN을 훈련하여 문장에서 다음 글자를 예측하는 방법 : 한 번에 한 글자씩 새로운 텍스트를 생성

### 16.1.1 데이터셋 생성하기
셰익스피어 문체 생성 방법
1. 안드레이 카르파시의 Char-RNN 프로젝트 셰익스피어 작품 다운로드
2. 셰익스피어 텍스트를 문자 단위로 벡터화 (split="character" 사용하여 문자 단위로 쪼갠 후 숫자로 바꿈)
3. RNN 학습을 위해 입력 시퀀스와 타깃 시퀀스로 나누는 데이터셋을 생성 (타깃 시퀀스로 다음 글자 예측하도록 구성)
4. 훈련 세트, 검증 세트, 테스트 세트 생성

### 16.1.2 Char-RNN 모델 만들고 훈련하기
언어 모델링 → 데이터셋 多 : 단순한 RNN 이상의 것 필요 (단순 RNN은 오래된 정보 기억 ↓) 

예제에서는 128개의 유닛으로 구성된 하나의 GRU층을 가진 모델을 구축하고 훈련 (정보를 잘 기억하며 효율적인 학습 가능)

```python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),  # ❶
    tf.keras.layers.GRU(128, return_sequences=True),
    tf.keras.layers.Dense(n_tokens, activation="softmax")          # ❷
])

model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer="nadam",
    metrics=["accuracy"]
)

model_ckpt = tf.keras.callbacks.ModelCheckpoint(
    "my_shakespeare_model", monitor="val_accuracy", save_best_only=True  # ❸
)

history = model.fit(
    train_set,
    validation_data=valid_set,
    epochs=10,
    callbacks=[model_ckpt]
)
```
이미 전처리 된 (벡터화된) 숫자 시퀀스만 입력으로 받기에, TextVectorization을 포함한 최종 모델은 원본 텍스트 → 문자 단위 벡터화 → 문맥 이해 → 다음 글자 예측까지 한 번에 처리

### 16.1.3 가짜 셰익스피어 텍스트 생성하기
**텍스트 생성 방식**

Char-RNN 모델로 다음 글자를 하나씩 예측하여 텍스트를 생성

단순히 확률이 가장 높은 글자만 고르면 반복이 많아짐 → greedy decoding

해결 : tf.random.categorical()로 확률 기반 랜덤 샘플링 수행

온도(Temperature) : 확률 분포를 조절하는 값 (샘플 다양성 조절)

|온도 ↓|정확도 ↑|창의성 ↓|
|---|---|---|
|온도 ↑|창의성 ↑|정확도 ↓|

***온도에 따라 생성되는 문장 스타일이 크게 달라짐***

**한계점 및 개선 방향**

단순 RNN 구조는 긴 문장 처리에 약함

beam search, nucleus sampling 등으로 개선 가능

더 깊거나 넓은 GRU, LSTM 사용 시 성능 향상 가능 : 매우 긴 시퀀스 처리 불가 → **상태가 있는 RNN 사용**

### 16.1.4 상태가 있는 RNN
**상태가 있다? : 뭘 기억하고 있다.**

상태가 있는 RNN : 시퀀스를 짧게 자르며 처리한 훈련 배치의 마지막 상태를 다음 훈련 배치의 초기 상태로 사용 → 긴 문맥을 기억하며 학습할 수 있음 (복습은 짧게 하지만, 이야기의 맥은 이어지고 있다.)

1. 순차적이고 겹치지 않는 입력 시퀀스 생성
2. 배치 크기를 줄이는 등 조정하여 하나의 시퀀스가 연속적으로 이어지게 만듦
3. 에포크마다 상태 초기화 (새로운 문장을 처음부터 학습하기 위해)
4. 모델 훈련

## 16.2 감성 분석
NLP의 가장 일반적인 애플리케이션 : 텍스트 분류 (특히 감성 분석)

IMDb 데이터셋을 사용한 이진 타깃 (긍정 : 1, 부정 : 0) 사례 제시

1. 감성 분류 문제는 쉽지 않다 : 어떤 부정적 리뷰는 첫 문장이 긍정적이라 헷갈림 → 단어 수준 정교한 처리 필요
2. 그래서 사용하는 게 tf.keras.layers.TextVectorization 층 : 텍스트를 단어 단위로 토큰화 (공백 사용) → 공백 방식은 완벽하지 않음
3. 바이트 페어 인코딩 (BPE) : Subword 단위 토큰화 (가장 빈번하게 등장하는 인접 쌍 반복 병합 → 하나의 토큰으로 묶고 유추)

***※ 모델을 만들고 훈련 시 유의할 점***

서로 다른 길이의 리뷰를 패딩 토큰으로 패딩하여 길이를 맞추면 리뷰 내용 잊어버릴 수 있음 (모델은 이 0(padding)을 의미 있는 단어로 착각하면 안됨!!)

▶ 모델에 동일한 길이의 문장으로 구성된 배치 주입

▶ 마스킹 사용한 RNN의 패딩 토큰 무시 (마스킹된 위치는 손실 계산에서 제외)

### 16.2.1 마스킹
1. 케라스에서 Embedding 층 구축 시 mask_zero = True 매개변수 추가 → 모든 층에서 패딩 토큰 무시
2. 모델이 들어가는 다음 층이 call() 함수에서 mask 인자 받도록 구현 → 이 마스크는 자동으로 다음 층에 전달됨 (GRU/LSTM 등은 마스크 자동 처리 가능)
3. 마지막 순환 층은 마스크를 사용하여 마스킹된 스텝 무시 & 마스크 전파 중단

훈련 시, 출력 형태에 따라서 출력에 마스크 사용 여부가 달라짐
|출력 형태|마스크 필요 여부|이유|
|---|---|---|
|하나의 값 (감성 분석)|X|출력이 1개 (입력 시퀀스를 압축해서 하나의 벡터로 표현)라 PAD 영향 없음|
|시퀀스 (번역, 요약)|O|PAD 있는 위치는 손실 제외해야 됨|

간단한 모델 : Masking 층과 자동 마스크 전파 사용이 효과적

복잡한 모델 (Conv1D와 RNN의 결합) : Conv1D가 마스크를 무시하여 끊김 → 직접 계산해서 수동으로 전달해야 함!

### 16.2.2 사전 훈련된 임베딩과 언어 모델 재사용하기
**단어 임베딩과 사전 훈련된 임베딩**

영화 리뷰 데이터셋을 통해 긍정/부정 단어 임베딩을 학습할 수 있으나, 다른 텍스트 도메인에서는 의미가 달라질 수 있음

사전 훈련된 임베딩 (Word2Vec, GloVe, FastText 등) : **다양한 일반 텍스트로부터 미리 학습된 단어 임베딩**

**문맥 임베딩과 ELMo, ULMFiT**

사전 훈련된 단어 임베딩의 한계 : 단어마다 **하나의 의미**만 가지는 한계 존재

(ex : right : 오른쪽 & 옳다의 의미)

극복 방법 : **문맥 기반 임베딩** (단어의 임베딩이 문맥에 따라 매번 다르게 생성)

ELMo : 심층 양방향 언어 모델의 내부 상태에서 학습된 문맥 반영된 단어 임베딩

ULMFiT : **사전 학습된 언어 모델을 NLP 태스크에 미세 조정**하여 소량의 데이터로 높은 성능 달성 

모델이 문장 전체를 보고 의미를 파악함

→ 정확도 향상 & 실전에서 단어 임베딩 대신 사전 훈련 **모델** 사용 일반적

## 16.3 신경망 기계 번역을 위한 인코더-디코더 네트워크
**영어 문장을 스페인어로 번역하는 NMT(신경망 기계 번역) 모델**

<훈련 과정>

![image](https://github.com/user-attachments/assets/654334d6-445a-4a6b-a7bd-fd9e07edb557)

1. 입력 문장 인코딩

▶ 각 단어 숫자로 변환 후 Embedding Layer 거쳐 벡터로 변환

▶ 변환된 벡터는 RNN 계열 인코더에 들어감 → 문장의 의미 요약 후 컨텍스트 벡터 출력 → 디코더의 시작점으로 이동

2. 디코더 입력 준비 (티처 포싱)

▶ 정답 문장을 한 스텝 밀어서 넣음 : **Teacher Forcing** (뭘 예측하든 답 줄테니까 다음 단어 예측해라)

3. 디코터 출력 예측
 
▶ 각 시점마다 예측함 → Dense + Softmax 층 이용하여 스페인어 단어 사전 전체에 대한 확률 분포 계산

▶ 가장 높은 확률의 단어 출력

4. 손실 계산 및 역전파

▶ 각 시점마다 예측된 단어 & 정답 단어 비교 → 손실함수 (sparse_categorical_crossentropy)로 오차 계산 후 가중치 업데이트

<추론 과정>

![image](https://github.com/user-attachments/assets/045546f9-6085-4dc8-ad9c-aef47a9577fd)

디코더에 주입할 타깃 문장 없으므로, 이전 스텝에서 디코더가 출력한 단어 주입

**출력 층 최적화**

출력하는 어휘의 개수가 많은 경우 모든 단어에 대한 예측 출력 시 상당히 느려질 가능성 ↑

1. 샘플링 소프트맥스

전체 단어가 아닌 일부 단어만 뽑아서 softmax 확률 계산

계산량을 크게 줄이고 훈련 속도 향상

tf.nn.sampled_softmax_loss() 함수로 사용 가능 (훈련에서만 가능)

2. 직교 행렬 기법

출력층 가중치 = 임베딩 가중치로 공유 (가중치 tying)

출력층의 행렬을 직교 행렬로 근사하면
→ 따로 가중치를 학습할 필요 없음

모델 파라미터 수 줄이고,
→ 훈련 데이터가 적을 때도 정확도 향상

### 16.3.1 양방향 RNN 
RNN이 미래를 보면 안되는 경우 : 시계열 예측, 기계 번역 디코더 (아직 미래는 다가오지 않았다.)

RNN이 미래를 봐도 되는 경우 : 감정 분석, 문장 분류 (입력 시퀀스 전체를 알고 있기 때문)

**양방향 RNN** : 동일한 입력에 대해 두 개의 순환 층을 실행하여, 왼쪽에서 오른쪽으로 단어를 읽고, 오른쪽에서 왼쪽으로 단어를 읽어 타임 스텝마다 두 출력을 연결

![image](https://github.com/user-attachments/assets/d4dc0d21-5690-40fa-918a-1c49afa67547)

ex : You are the right person for this job. (텍스트 분석)

(인코더 내)

정방향 LSTM : 왼쪽에서 오른쪽으로 장기(c) + 단기(h) 상태 반환

역방향 LSTM : 오른쪽에서 왼쪽으로 장기(c) + 단기(h) 상태 반환

※ 인코더에서 출력된 4개의 LSTM은 디코더로 들어갈 수 없다. (디코더의 LSTM은 두 개의 상태 (장기, 단기)를 기대하기 때문이다.)

인코더의 출력값 (두 개의 장기/단기 상태)을 연결하여 디코더로 보내야 한다.

### 16.3.2 빔 서치
모델이 실수했을 때 고칠 수 있는 방법 : **빔 서치**

![image](https://github.com/user-attachments/assets/dc0b0f26-7e64-47c7-88d6-9d075dedf525)

k개의 가능성 있는 문장의 리스트 유지 & 디코더 단계마다 이 문장의 단어를 하나씩 생성하여 가능성 있는 k개의 문장을 생성 (k : 빔 너비)

(I like soccer 번역)

1. 최상위 세 개 단어와 추정 확률 출력
2. 각각 완성된 문장의 추정 확률 * 각 단어의 추정된 조건부 확률 계산
3. 동일 과정 반복 수행

## 16.4 어텐션 메커니즘
**일반 번역 RNN VS 어텐션 메커니즘**
|일반 번역 RNN|어텐션 메커니즘|
|---|---|
|순차적으로 각 타임스텝마다 전달|입력의 각 단어 전부 읽고 마지막 state 하나만 전달|
|긴 문장에 취약 (정보 손실)|각 단어마다 디코더가 초점을 맞추도록 높은 어텐션 점수 부여|

![image](https://github.com/user-attachments/assets/62728ebe-ca80-4f5f-b2e5-1c7330281bc0)

**어텐션? : 입력의 일부에 모델의 주의를 집중시키는 방법**

**디코더는 어떻게 초점을 맞추는 것인가?** : **정렬 모델**에 의한 가중치 합에서 주의 집중할 단어 결정

**바흐다나우 어텐션** : 디코더가 인코더의 모든 히든 스테이트를 보고, 지금 생성할 단어와 가장 관련 있는 위치에 "집중"해서 번역하는 방식

**바흐다나우 어텐션의 작동 과정**
```r
[Encoder hidden states h₁ ~ hₙ]   +   [Decoder state sₜ]
                ↓                       ↓
     어텐션 스코어 eₜ₁ ~ eₜₙ 계산 (tanh + 가중합)
                ↓
       소프트맥스 → 어텐션 가중치 αₜ₁ ~ αₜₙ
                ↓
   컨텍스트 벡터 cₜ = Σ αₜᵢ hᵢ (입력에 집중!)
                ↓
     cₜ + sₜ → 디코더 출력 단어 생성
```

**루옹 어텐션**

※ "이전" VS "현재" 디코더 상태란?

![image](https://github.com/user-attachments/assets/48bb0d93-c866-4cbe-b96d-c022ee5b68ea)

간단하게 말해서, 

**이전 상태 St-1 : 내가 방금까지 뭘 말했더라?**

**현재 상태 st : 지금 내가 무슨 말을 하려고 하지?**

루옹 어텐션은 지금 내가 무슨 말을 하려고 할지가 먼저 계산된 후, 어텐션을 통해 어떤 입력에 집중할지 정한다.

바흐다나우 어텐션은 먼저 어텐션으로 컨텍스트 ct를 만든 뒤 내가 무슨 말을 하려고 하는 지에 대한 계산에 적용한다.

```css
입력 문장: "I"     "am"     "student"
              ↓        ↓          ↓
          h₁       h₂         h₃  ← 인코더 출력

디코더 현재 상태: sₜ (예: "étudiant" 만들 차례)
          ↓
     score 계산: dot(sₜ, h₁), dot(sₜ, h₂), dot(sₜ, h₃)
          ↓
     소프트맥스 → 어텐션 가중치 α₁, α₂, α₃
          ↓
     컨텍스트 벡터 cₜ = α₁h₁ + α₂h₂ + α₃h₃
          ↓
     cₜ + sₜ → "étudiant" 생성
```
어텐션 메커니즘은 명확한 **키**를 사용하는 대신, 디코더가 현재 번역하려는 단어에 해당하는 **query(쿼리)** 벡터 생성 → 각 단어 벡터 (=key)들과의 유사도 계산 → softmax로 변환 후 확률화 & 가장 관련성 있는 단어에 높은 가중치 부여 = 컨텍스트 구성

***= 유사한 의미를 가진 벡터끼리의 유사도를 기반으로 확률적으로 '스마트 조회' 하는 방식***

Query : 디코더의 현재 상태 (=지금 번역하고 싶은 것)

Key : 인코더가 만든 각 단어의 표현

Value : 실제 사용할 단어의 의미 벡터 

▶ 쿼리와 키 사이의 유사도 계산 → 가중치로 변환 → value에 곱하여 합침 → 최종 컨텍스트 벡터 완성 → 단어 출력

### 16.4.1 트랜스포머 구조 : 어텐션만 있으면 된다
트랜스포머 구조 : 순환층, 합성곱 층 전혀 사용하지 않고 (임베딩 층, 밀집층, 정규화 층, 다른 구성 요소) 어텐션 메커니즘을 사용한 모델

![image](https://github.com/user-attachments/assets/528e50d2-df8b-4176-9dff-777a58d4059e)

**트랜스포머의 학습 vs 추론**

학습 시: 정답 문장(=타겟 문장)을 통째로 알고 있으므로, 디코더에 정답의 모든 단어를 한 번에 줘도 됨.

추론 시: 정답이 뭔지 모르기 때문에, 지금까지 생성한 단어들만 가지고 다음 단어를 예측해야 함.

인코더의 역할 : 여러 개의 층을 통해 각 단어의 표현이 문맥 안에서 단어 의미 완벽하게 포착할 때까지 입력 점진적으로 변화시키는 역할

디코더의 역할 : 번역된 문장의 각 단어 표현을 다음 번역 단어의 표현으로 점진적으로 변환

디코더 통과 후 각 단어 표현 → 소프트맥스 활성화 함수 가진 마지막 Dense 층 거침 : 올바른 다음 단어에 대해 높은 확률 출력


```css
[입력 문장] → [인코더 블록들] → [컨텍스트 벡터] → [디코더 블록들] → [출력 문장]
```

**▶멀티 헤드 어텐션** 
스케일드 점곱 어텐션 수
```mathematica
Attention(Q, K, V) = softmax(QKᵀ / √d) V
```
멀티 헤드 어텐션 = 스케일드 점곱 어텐션 층의 묶음

![image](https://github.com/user-attachments/assets/25c2d5f2-7f80-449f-aad4-ce1d710cb445)



인코더의 **멀티 헤드 어텐션** : 같은 문장에 있는 다른 모든 단어에 주의를 기울여 각 단어 표현 업데이트

디코더의 **▶마스크드 멀티 헤드 어텐션** : 위와 동일한 작업을 수행하나, 한 단어 처리 시 그 뒤 단어에는 관심 기울이지 않음 (인과적 층)

디코더의 **멀티 헤드 어텐션** : 디코더가 영어 문장의 단어에 주의를 기울이는 곳 (=**크로스 어텐션**)

**▶위치 인코딩** : 각 단어의 위치를 나타내는 밀집 벡터 (트랜스포머 구조의 모든 층이 단어 위치를 무시하기 때문 → 입력 시퀀스 혹은 출력 시퀀스 mix 문제) 

단어 자체의 의미(임베딩) + 그 단어의 위치 → 전체 문장의 구조와 의미 파악 가능

## 16.5 언어 모델 분야와 최근 혁신







