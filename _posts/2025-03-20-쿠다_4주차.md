---
layout : posts
title : "쿠다 ML 기초 4주차"
categories : "쿠다 ML 기초"
---
# 16. RNN과 어텐션을 사용한 자연어 처리

## 16.1 Char-RNN으로 셰익스피어 문체 생성하기
Char-RNN을 훈련하여 문장에서 다음 글자를 예측하는 방법 : 한 번에 한 글자씩 새로운 텍스트를 생성

### 16.1.1 데이터셋 생성하기
셰익스피어 문체 생성 방법
1. 안드레이 카르파시의 Char-RNN 프로젝트 셰익스피어 작품 다운로드
2. 셰익스피어 텍스트를 문자 단위로 벡터화 (split="character" 사용하여 문자 단위로 쪼갠 후 숫자로 바꿈)
3. RNN 학습을 위해 입력 시퀀스와 타깃 시퀀스로 나누는 데이터셋을 생성 (타깃 시퀀스로 다음 글자 예측하도록 구성)
4. 훈련 세트, 검증 세트, 테스트 세트 생성

### 16.1.2 Char-RNN 모델 만들고 훈련하기
언어 모델링 → 데이터셋 多 : 단순한 RNN 이상의 것 필요 (단순 RNN은 오래된 정보 기억 ↓) 

예제에서는 128개의 유닛으로 구성된 하나의 GRU층을 가진 모델을 구축하고 훈련 (정보를 잘 기억하며 효율적인 학습 가능)

```python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),  # ❶
    tf.keras.layers.GRU(128, return_sequences=True),
    tf.keras.layers.Dense(n_tokens, activation="softmax")          # ❷
])

model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer="nadam",
    metrics=["accuracy"]
)

model_ckpt = tf.keras.callbacks.ModelCheckpoint(
    "my_shakespeare_model", monitor="val_accuracy", save_best_only=True  # ❸
)

history = model.fit(
    train_set,
    validation_data=valid_set,
    epochs=10,
    callbacks=[model_ckpt]
)
```
이미 전처리 된 (벡터화된) 숫자 시퀀스만 입력으로 받기에, TextVectorization을 포함한 최종 모델은 원본 텍스트 → 문자 단위 벡터화 → 문맥 이해 → 다음 글자 예측까지 한 번에 처리

### 16.1.3 가짜 셰익스피어 텍스트 생성하기
**텍스트 생성 방식**

Char-RNN 모델로 다음 글자를 하나씩 예측하여 텍스트를 생성

단순히 확률이 가장 높은 글자만 고르면 반복이 많아짐 → greedy decoding

해결 : tf.random.categorical()로 확률 기반 랜덤 샘플링 수행

온도(Temperature) : 확률 분포를 조절하는 값 (샘플 다양성 조절)

|온도 ↓|정확도 ↑|창의성 ↓|
|---|---|---|
|온도 ↑|창의성 ↑|정확도 ↓|

***온도에 따라 생성되는 문장 스타일이 크게 달라짐***

**한계점 및 개선 방향**

단순 RNN 구조는 긴 문장 처리에 약함

beam search, nucleus sampling 등으로 개선 가능

더 깊거나 넓은 GRU, LSTM 사용 시 성능 향상 가능 : 매우 긴 시퀀스 처리 불가 → **상태가 있는 RNN 사용**

### 16.1.4 상태가 있는 RNN
**상태가 있다? : 뭘 기억하고 있다.**

상태가 있는 RNN : 시퀀스를 짧게 자르며 처리한 훈련 배치의 마지막 상태를 다음 훈련 배치의 초기 상태로 사용 → 긴 문맥을 기억하며 학습할 수 있음 (복습은 짧게 하지만, 이야기의 맥은 이어지고 있다.)

1. 순차적이고 겹치지 않는 입력 시퀀스 생성
2. 배치 크기를 줄이는 등 조정하여 하나의 시퀀스가 연속적으로 이어지게 만듦
3. 에포크마다 상태 초기화 (새로운 문장을 처음부터 학습하기 위해)
4. 모델 훈련

## 16.2 감성 분석
NLP의 가장 일반적인 애플리케이션 : 텍스트 분류 (특히 감성 분석)

IMDb 데이터셋을 사용한 이진 타깃 (긍정 : 1, 부정 : 0) 사례 제시

1. 감성 분류 문제는 쉽지 않다 : 어떤 부정적 리뷰는 첫 문장이 긍정적이라 헷갈림 → 단어 수준 정교한 처리 필요
2. 그래서 사용하는 게 tf.keras.layers.TextVectorization 층 : 텍스트를 단어 단위로 토큰화 (공백 사용) → 공백 방식은 완벽하지 않음
3. 바이트 페어 인코딩 (BPE) : Subword 단위 토큰화 (가장 빈번하게 등장하는 인접 쌍 반복 병합 → 하나의 토큰으로 묶고 유추)

***※ 모델을 만들고 훈련 시 유의할 점***

서로 다른 길이의 리뷰를 패딩 토큰으로 패딩하여 길이를 맞추면 리뷰 내용 잊어버릴 수 있음 (모델은 이 0(padding)을 의미 있는 단어로 착각하면 안됨!!)

▶ 모델에 동일한 길이의 문장으로 구성된 배치 주입
▶ 마스킹 사용한 RNN의 패딩 토큰 무시 (마스킹된 위치는 손실 계산에서 제외)

### 16.2.1 마스킹
1. 케라스에서 Embedding 층 구축 시 mask_zero = True 매개변수 추가 → 모든 층에서 패딩 토큰 무시
2. 모델이 들어가는 다음 층이 call() 함수에서 mask 인자 받도록 구현 → 이 마스크는 자동으로 다음 층에 전달됨 (GRU/LSTM 등은 마스크 자동 처리 가능)
3. 마지막 순환 층은 마스크를 사용하여 마스킹된 스텝 무시 & 마스크 전파 중단

훈련 시, 출력 형태에 따라서 출력에 마스크 사용 여부가 달라짐
|출력 형태|마스크 필요 여부|이유|
|---|---|---|
|하나의 값 (감성 분석)|X|출력이 1개 (입력 시퀀스를 압축해서 하나의 벡터로 표현)라 PAD 영향 없음|
|시퀀스 (번역, 요약)|O|PAD 있는 위치는 손실 제외해야 됨|

간단한 모델 : Masking 층과 자동 마스크 전파 사용이 효과적

복잡한 모델 (Conv1D와 RNN의 결합) : Conv1D가 마스크를 무시하여 끊김 → 직접 계산해서 수동으로 전달해야 함!

### 16.2.2 사전 훈련된 임베딩과 언어 모델 재사용하기
**단어 임베딩과 사전 훈련된 임베딩**

영화 리뷰 데이터셋을 통해 긍정/부정 단어 임베딩을 학습할 수 있으나, 다른 텍스트 도메인에서는 의미가 달라질 수 있음

사전 훈련된 임베딩 (Word2Vec, GloVe, FastText 등) : **다양한 일반 텍스트로부터 미리 학습된 단어 임베딩**

**문맥 임베딩과 ELMo, ULMFiT**

사전 훈련된 단어 임베딩의 한계 : 단어마다 **하나의 의미**만 가지는 한계 존재

(ex : right : 오른쪽 & 옳다의 의미)

극복 방법 : **문맥 기반 임베딩** (단어의 임베딩이 문맥에 따라 매번 다르게 생성)

ELMo : 심층 양방향 언어 모델의 내부 상태에서 학습된 문맥 반영된 단어 임베딩

ULMFiT : **사전 학습된 언어 모델을 NLP 태스크에 미세 조정**하여 소량의 데이터로 높은 성능 달성 

모델이 문장 전체를 보고 의미를 파악함

→ 정확도 향상 & 실전에서 단어 임베딩 대신 사전 훈련 **모델** 사용 일반적








