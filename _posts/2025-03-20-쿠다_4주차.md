---
layout : posts
title : "쿠다 ML 기초 4주차"
categories : "쿠다 ML 기초"
---
# 16. RNN과 어텐션을 사용한 자연어 처리

## 16.1 Char-RNN으로 셰익스피어 문체 생성하기
Char-RNN을 훈련하여 문장에서 다음 글자를 예측하는 방법 : 한 번에 한 글자씩 새로운 텍스트를 생성

### 16.1.1 데이터셋 생성하기
셰익스피어 문체 생성 방법
1. 안드레이 카르파시의 Char-RNN 프로젝트 셰익스피어 작품 다운로드
2. 셰익스피어 텍스트를 문자 단위로 벡터화 (split="character" 사용하여 문자 단위로 쪼갠 후 숫자로 바꿈)
3. RNN 학습을 위해 입력 시퀀스와 타깃 시퀀스로 나누는 데이터셋을 생성 (타깃 시퀀스로 다음 글자 예측하도록 구성)
4. 훈련 세트, 검증 세트, 테스트 세트 생성

### 16.1.2 Char-RNN 모델 만들고 훈련하기
언어 모델링 → 데이터셋 多 : 단순한 RNN 이상의 것 필요 (단순 RNN은 오래된 정보 기억 ↓) 

예제에서는 128개의 유닛으로 구성된 하나의 GRU층을 가진 모델을 구축하고 훈련 (정보를 잘 기억하며 효율적인 학습 가능)

```python
model = tf.keras.Sequential([
    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),  # ❶
    tf.keras.layers.GRU(128, return_sequences=True),
    tf.keras.layers.Dense(n_tokens, activation="softmax")          # ❷
])

model.compile(
    loss="sparse_categorical_crossentropy",
    optimizer="nadam",
    metrics=["accuracy"]
)

model_ckpt = tf.keras.callbacks.ModelCheckpoint(
    "my_shakespeare_model", monitor="val_accuracy", save_best_only=True  # ❸
)

history = model.fit(
    train_set,
    validation_data=valid_set,
    epochs=10,
    callbacks=[model_ckpt]
)
```
이미 전처리 된 (벡터화된) 숫자 시퀀스만 입력으로 받기에, TextVectorization을 포함한 최종 모델은 원본 텍스트 → 문자 단위 벡터화 → 문맥 이해 → 다음 글자 예측까지 한 번에 처리

### 16.1.3 가짜 셰익스피어 텍스트 생성하기
텍스트 생성 방식

Char-RNN 모델로 다음 글자를 하나씩 예측하여 텍스트를 생성
단순히 확률이 가장 높은 글자만 고르면 반복이 많아짐 → greedy decoding
tf.random.categorical()로 확률 기반 랜덤 샘플링 수행

온도(Temperature) : 확률 분포를 조절하는 값 (샘플 다양성 조절)
온도 ↓ : 정확도 ↑, 창의성 ↓
온도 ↑ : 창의성 ↑, 정확도 ↓

***온도에 따라 생성되는 문장 스타일이 크게 달라짐***

한계점 및 개선 방향

단순 RNN 구조는 긴 문장 처리에 약함

beam search, nucleus sampling 등으로 개선 가능

더 깊거나 넓은 GRU, LSTM 사용 시 성능 향상 가능




