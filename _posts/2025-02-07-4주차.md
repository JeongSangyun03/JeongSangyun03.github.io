---
layout : post
title : 4주차
categories : ML 기초
---
# 쿠다 ML 기초 4주차
이번 주차에서는 앙상블 학습과 랜덤 포레스트에 대하여 공부한다.

일련의 예측기 (분류나 회귀 모델)로부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있다. 이 일련의 예측기를 **앙상블**이라고 하며, 이러한 예측과정을 **앙상블 학습**이라고 한다.

앙상블 방법 중 하나의 예시 : 훈련 세트로부터 랜덤으로 각기 다른 서브셋을 만들어 일련의 결정 트리 분류기를 훈련 시킴. 개별 트리 예측을 모아 가장 많은 선택을 받은 클래스를 앙상블의 예측으로 삼음. 결정트리의 앙상블을 **랜덤 포레스트**라고 함

실제로 앙상블 방법을 사용하여 여러 괜찮은 예측기를 연결하여 더 좋은 예측기를 만든다.

## 7.1 투표 기반 분류기

![image](https://github.com/user-attachments/assets/29063c51-27c9-4562-a54b-8c4732e7eb11)

위와 같이 정확도가 80%인 분류기 여러 개를 훈련시켰다고 가정하자. 더 좋은 분류기를 만들 수 없을까?

이에 대한 간단한 방법은 각 분류기의 예측을 집계하는 것이다. 가장 많은 표를 얻은 클래스가 앙상블의 예측이 된다. 이러한 분류기를 **직접 투표 분류기**라고 한다.

![image](https://github.com/user-attachments/assets/83357205-cb3c-4ee5-87c1-8cf21d406ee5)

각 분류기가 **약한 학습기**일지라도 앙상블에 있는 약한 학습기가 충분하게 많고 다양하다면 앙상블은 **강한 학습기**가 될 수 있다. ***큰수의 법칙***을 생각한다면 연결지어 이해하기 쉬울 것이다.

앙상블 방법은 예측기가 가능한 한 서로 독립적이고 오차에 상관관계가 없을 때 최고의 성능을 발휘한다. 즉, 각 분류기가 서로 다른 방식으로 틀려야 보완적으로 최적의 앙상블 효과를 볼 수 있게 되는 것이다.

사이킷런은 이름/예측기 쌍의 리스트를 제공하기만 하면 일반 분류기처럼 쉽게 사용할 수 있는 **VotingClassifier** 클래스를 제공한다. moons 데이터셋을 사용하여 3가지 다양한 분류기로 구성된 투표 기반 분류기를 생성하고 훈련하겠다.

```python
from sklearn.datasets import make_moons
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

X,y = make_moons(n_samples=500, noise=0.30, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) 

voting_clf = VotingClassifier (
    estimators=[('lr', LogisticRegression(random_state = 42)), ('rf', RandomForestClassifier(random_state = 42)), ('svc', SVC(random_state = 42))]
)
voting_clf.fit(X_train, y_train)  
```
이에 대한 결괏값은 다음과 같다.

![image](https://github.com/user-attachments/assets/1066ea9f-6596-49e5-b9bd-0ffa8728a3bb)

VotingClassifier을 훈련할 때 이 클래스는 모든 추정기를 복제하여 복제된 추정기를 훈련한다. 원본 추정기는 estimators 속성을 통해 참조할 수 있으며 훈련된 복제본은 estimators_속성에 저장된다. 테스트 세트에서 훈련된 각 분류기의 정확도를 살펴보자.

```python
for name, clf in voting_clf.named_estimators_.items() :
  print (name, '=', clf.score(X_test, y_test))
```
이에 대한 결괏값은 아래와 같다.

![image](https://github.com/user-attachments/assets/0dc19dda-5465-4ab8-864e-8c9fc424c688)

투표 기반의 predict() 메소드를 호출하여 직접 투표를 수행할 수 있다. 아래의 코드를 통해 세 분류기 중 두 개가클래스 1을 예측하여 전체적으로 첫번째 샘플에 대해 클래스 1을 예측하는 작업을 살펴보자.

```python
print (voting_clf.predict(X_test[:1]))
print ([clf.predict(X_test[:1]) for clf in voting_clf.estimators_])
```
![image](https://github.com/user-attachments/assets/1ad486e9-98ca-4ae1-adfc-dc7076738a40)

다음은 테스트 세트에서 투표 기반 분류기의 성능을 살펴보겠다.

```python
voting_clf.score (X_test, y_test)
```
값은 0.912로 예상대로 나온다. 투표 기반 분류기가 다른 개별 분류기보다 성능이 더 높다.

모든 분류기가 클래스의 확률을 예측할 수 있으면, 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있다. 이를 **간접 투표**라고 한다. 이는 확률이 높은 투표에 비중을 두기에 직접 투표 방식보다 성능이 높다. 투표 기반 분류기의 voting 매개변수를 soft로 바꾸고 모든 분류기가 클래스의 확률을 추정할 수 있도록 하면 된다.SVC는 probability = True로 지정하면 된다. 살펴보자.

```python
voting_clf.voting = 'soft'
voting_clf.named_estimators['svc'].probability = True 
voting_clf.fit(X_train, y_train)
print (voting_clf.score(X_test, y_test))
```
간접 투표 방식을 사용하여 92%의 정확도를 달성하게 되었다. (결괏값)

## 7.2 배깅과 페이스팅
각기 다른 훈련 알고리즘을 사용할 수도 있지만, 같은 알고리즘을 사용하고 훈련세트의 서브셋을 랜덤으로 구성하여 분류기를 각기 다르게 학습시키는 방법도 있다. 훈련 세트에서 중복을 허용하여 샘플링하는 방식을 **배깅**이라고 하며, 중복을 허용하지 않고 샘플링하는 방식을 **페이스팅**이라고 한다.

이 두 방법은 같은 훈련 샘플을 여러 개의 예측기에 걸쳐 사용할 수 있다. 하지만 배깅만이 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링할 수 있다. 이 샘플링과 훈련 과정을 아래에 나타내었다.

![image](https://github.com/user-attachments/assets/098543c8-a838-42d2-b07a-b9a96845e471)

모든 예측기가 훈련을 마치면 앙상블은 모든 예측기의 예측을 모아서 새로운 샘플에 대한 예측을 만든다. 집계 함수는 일반적으로 분류일 때는 **통계적 최빈값** (직접 투표 분류기처럼 가장 많은 예측 결과)을, 회귀에 대해서는 평균을 계산한다. 개별 예측기는 원본 훈련 세트로 훈련시킨 것보다 훨씬 편향되어 있으나, 집계함수를 통과하면 편향과 분산이 모두 감소한다.

위 그림에서 예측이나 학습을 병렬로 수행할 수 있는 확장성을 볼 수 있다. 이러한 점 덕분에 배깅과 페이스팅의 인기가 높다.

### 7.2.1 사이킷런의 배깅과 페이스팅
사이킷런은 배깅과 페이스팅을 위해 BaggingClassifier (회귀의 경우 BaggingRegressor)를 제공한다. 아래의 코드는 결정 트리 분류기 500개의 앙상블을 훈련시키는 코드이다. 각 분류기는 훈련 세트에서 중복을 허용하여 랜덤으로 선택된 100개의 샘플로 훈련된다. (배깅의 경우이고, 페이스팅의 경우에는 bootstrap=False로)
```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_clf = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators=500,
    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42
) # n_jobs : 사이킷런이 훈련과 예측에 사용할 CPU 코어 수 지정 (-1이면 가용한 모든 코어)
bag_clf.fit(X_train, y_train)
```

![image](https://github.com/user-attachments/assets/98063a79-ef76-43f4-9e4f-9db14e016a91)

그림을 살펴보자. 단일 결정 트리보다 앙상블이 훨씬 일반화가 잘되어있다. 앙상블은 비슷한 편향에서 더 작은 분산을 만들어낸다. 

배깅은 각 예측기가 학습하는 서브셋에 다양성을 추가하므로 배깅이 페이스팅보다 편향이 조금 더 높지만, 다양성을 추가함으로써 예측기들의 상관관계를 줄이므로 앙상블의 분산도 줄어든다. 시간과 여유가 있다면 교차검증으로 배깅과 페이스팅을 모두 평가해서 나은쪽을 선택하자. (전반적으로는 배깅이 더 나은 모델을 만듦)

### 7.2.2 OOB 평가
OOB 평가는 배깅에서 훈련에 사용되지 않은 데이터(OOB 샘플)을 이용해 모델의 성능을 평가하는 방법이다. 평균적으로 37%의 샘플이 남는데, 이를 가지고 예측을 수행한다. 그리하여 모든 모델에 대해 OOB 샘플 예측 결과를 평균 내어 최종 OOB 평가를 수행, 이를 종합하여 모델 성능을 평가한다.

사이킷런에서 BaggingClassifier을 만들 때 oob_score = True로 지정하면 훈련이 끝난 후 자동으로 OOB 평가를 수행한다. 코드를 살펴보자.

```python
bag_clf = BaggingClassifier (DecisionTreeClassifier(), n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)
bag_clf.fit(X_train, y_train)
print (bag_clf.oob_score_)
```
값은 0.896이 나온다. OOB 평가 결과를 보면 이 BaggingClassifier이 테스트 세트에서 약 89.6%의 정확도를 얻을 것으로 보인다. 확인해보자.

```python
from sklearn.metrics import accuracy_score
y_pred = bag_clf.predict(X_test)
print (accuracy_score(y_test, y_pred)) 
```
값은 0.912. 테스트 세트에서 91.2%의 정확도를 얻었다. OOB 평가는 2% 정도 낮아 조금 비관적이었다.

OOB 샘플에 대한 결정 함수의 값도 oob_decision_function_ 변수에서 확인할 수 있다. 결정함수는 각 훈련 샘플의 클래스 확률을 반환한다. 아래의 결과를 보면 OOB 평가는 첫 번째 훈련 샘플이 양성 클래스에 속할 확률을 67%, 음성 클래스를 32%로 추정하고 있다.

```python
bag_clf.oob_decision_function_[:3]
```
![image](https://github.com/user-attachments/assets/02fc0e96-9a69-4140-9ff8-352000d3d34f)

## 7.3 랜덤 패치와 랜덤 서브스페이스
BaggingClassifier는 특성 샘플링도 지원한다. 샘플링은 max_features, bootstrap_features 두 매개변수로 조절된다. 이는 샘플이 아닌 특성에 대한 샘플링에 사용된다. 따라서 각 예측기는 랜덤으로 선택한 입력 특성의 일부분으로 훈련된다.

훈련 특성과 샘플을 모두 샘플링하는 것을 랜덤 패치 방식이라고 한다. 훈련 샘플을 모두 사용하고 (bootstrap = False, max_samples = 1.0으로 설정) 특성을 샘플링하는 것을 랜덤 서브스페이스 방식이라고 한다. 

특성 샘플링은 더 다양한 예측기를 만들며 편향을 늘리는 대신 분산을 낮춘다.

## 7.4 랜덤 포레스트
랜덤 포레스트는 일반적으로 배깅 방법 (또는 페이스팅)을 적용한 결정 트리의 앙상블이다. 일반적으로 max_samples를 훈련 세트의 크기로 지정한다. 결정트리에 최적화되어 사용하기 편리한 RandomForestClassifier을 사용할 수 있다. (회귀 문제를 위한 클래스는 RandomForestRegressor) 다음은 최대 16개의 리프 노드를 갖는 500개의 트리로 이뤄진 랜덤 포레스트 분류기를 가능한 모든 CPU 코어에서 훈련시키는 코드이다.

```python
from sklearn.ensemble import RandomForestClassifier
rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1, random_state=42)
rnd_clf.fit(X_train, y_train)
y_pred_rf = rnd_clf.predict(X_test)
```
RandomForestClassifier은 예외가 있지만 트리의 성장을 조절하기 위한 DecisionTreeClassifier의 매개변수와 앙상블 자체를 제어하는 데 필요한 BaggingClassifier의 매개변수를 모두 가지고 있다.

이 알고리즘은 트리의 노드를 분할할 때 전체 특성 중에서 최선의 특성을 찾는 대신 랜덤으로 선택한 특성 후보 중 최적의 특성을 찾는 식으로 무작위성을 더 주입한다. 

기본적으로 root n개의 특성을 선택한다. (n은 전체 특성 개수) 이는 결국 트리를 더욱 다양하게 만들고 편향을 손해보는 대신 분산을 낮추어 전체적으로 더 훌륭한 모델을 만들어낸다. 

```python
bag_clf = BaggingClassifier (DecisionTreeClassifier(max_features='sqrt',max_leaf_nodes=16),n_estimators=500, n_jobs=-1, random_state=42)
bag_clf.fit(X_train, y_train)
```

### 7.4.1 엑스트라 트리
트리를 더욱 랜덤하게 만들기 위해 최적의 임곗값을 찾는 대신 후보 특성을 사용해 랜덤으로 분할하고 그 중에서 최상의 분할을 선택한다. (DecisionTreeClassifier 만들 때 splitter = 'random'으로)

이와 같이 극단적으로 랜덤한 트리의 랜덤 포레스트를 **익스트림 랜덤 트리**라고 한다. 여기서도 편향이 늘어나는 대신 분산이 줄어든다. 모든 노드에서 특성마다 가장 최적의 임곗값을 찾는 과정 때문에 일반적인 랜덤 포레스트보다 엑스트라 트리의 훈련속도가 훨씬 빠르다. (최적의 임곗값을 찾는 과정 생략)

추가 설명

랜덤 포레스트 :

각 노드에서 최적의 임곗값을 찾기 위해 해당 특성의 모든 가능한 값 탐색 & 정보 이득 (지니 불순도, 엔트로피 감소) 최대화하는 분할 선택

엑스트라 트리 :

각 노드에서 최적의 임곗값 찾지 않고, **후보 특성별로 랜덤한 임곗값 선택 후, 그중에서 최상의 분할 적용**

엑스트라 트리는 각 특성에서 **임곗값을 랜덤으로 선택** : 데이터에 대한 최적의 분할이 아닐 수도 있다. (**정보적인 분할 덜 일어날 수도 있을 가능성**)

![image](https://github.com/user-attachments/assets/6cbe5ef3-5f04-4196-a706-d9d5fd7ec7bc)

사이킷런의 ExtraTreesClassifier을 사용하여 boostrap = False 사용하여 엑스트라 트리를 만든다. 

### 7.4.2 특성 중요도
랜덤 포레스트의 또 다른 장점 : 특성의 상대적 중요도를 측정하기 쉽다.

어떻게? 

사이킷런에서 어떤 특성을 사용한 노드가 평균적으로 **불순도를 얼마나 감소시키는지 확인**하여 특성의 중요도 측정한다.

(더 정확하게는 가중치 평균)

가중치 평균 : 각 노드가 데이터셋의 불순도를 얼마나 감소시키는지를 가중치 (=해당 노드를 통과하는 샘플 수)를 고려하여 평균을 내는 방식

![image](https://github.com/user-attachments/assets/8164eeac-b45a-4998-a360-b9230619b638)

사이킷런은 훈련이 끝난 뒤 특성마다 자동으로 점수를 계산하고, 중요도 전체 합이 1이 되도록 결괏값을 정규화한다.
(**feature_importances에 저장**)

다음 코드에서 각 특성의 중요도를 출력하는 것을 살펴보자. 
```python
from sklearn.datasets import load_iris
iris = load_iris (as_frame = True)
rnd_clf = RandomForestClassifier (n_estimators = 500, random_state = 42)
rnd_clf.fit(iris.data, iris.target)
for score, name in zip (rnd_clf.feature_importances_, iris.data.columns) :
  print (round(score, 2), name)
```

![image](https://github.com/user-attachments/assets/d0c0af6b-dda5-434f-b92b-4cfc5d71f72c)

가장 중요한 특성은 꽃잎의 길이 44%와 너비 42%이다. 꽃받침의 길이와 너비는 각각 11%와 2%로 비교적 덜 중요한 것으로 나타난다.

이와 유사하게 MNIST 데이터셋에 랜덤 포레스트 분류기를 훈련시키고 각 픽셀의 중요도를 그래프로 나타내면 다음과 같은 이미지를 얻게 된다.

![image](https://github.com/user-attachments/assets/84d2dabe-37fb-4a30-9216-6f6a0a4f32d9)

## 7.5 부스팅
**부스팅** : 약한 학습기 여러 개 연결하여 강한 학습기를 만드는 앙상블 방법으로, 앞의 모델을 보완해 나가면서 일련의 예측기를 학습시키는 것이다. 

가장 인기 있는 부스팅 방법 두 개 : **AdaBoost, 그레이디언트 부스팅**

### 7.5.1 AdaBoost
이전 예측기를 보완하는 새로운 예측기를 만드는 방법은, 이전 모델이 과소적합했던 훈련 샘플의 가중치를 더 높이는 것이다. 

예를 들어, 분류기를 만들 때 먼저 알고리즘이 기반이 되는 첫 번째 분류기를 훈련 세트에서 훈련시키고 예측을 만든다. 그 후, 알고리즘이 잘못 분류된 훈련 샘플의 가중치를 상대적으로 높인다 두 번째 분류기는 업데이트된 가중치를 사용해 훈련세트에서 훈련하고 다시 예측을 만든다. 그리고 다시 가중치를 업데이트한다.

![image](https://github.com/user-attachments/assets/6b46d244-6325-47d7-adc8-7aa9507eda1e)

moons 데이터셋에 훈련시킨 다섯 개의 연속된 예측기의 결정 경계를 살펴보겠다. 

첫 번째 분류기가 많은 샘플을 잘못 분류해서 이 샘플들의 가중치 상승 => 두 번째 분류기가 이 샘플들을 더 정확하게 예측

![image](https://github.com/user-attachments/assets/d9ab722d-292a-4858-8810-a3352553a128)

이러한 연속된 학습 기법은 경사 하강법과 비슷한 면이 있다. (비용함수를 최소화하기 위해 한 예측기의 모델 파라미터 조정해가는 방식처럼, AdaBoost는 앙상블에 예측기를 추가하여 성능 향상)

모든 예측기가 훈련을 마치면, 이 앙상블은 배깅이나 페이스팅과 비슷한 방식으로 예측을 만든다. (가중치 적용 안하고 다수결 투표 또느 평균으로) 

하지만 가중치가 적용된 훈련 세트의 전반적 정확도에 따라 예측기마다 다른 가중치가 적용된다.

AdaBoost 알고리즘을 더 자세하게 살펴보자.

각 샘플 가중치 w(i)는 초기에 1/m으로 초기화된다. 첫 번째 예측기가 학습되고 가중치가 적용된 오류율 r1이 훈련 세트에 대해 계산된다. 

![image](https://github.com/user-attachments/assets/1d8b836b-cf68-464e-b6da-639371f2b846)

예측기의 가중치 alpha는 아래의 식을 사용하여 계산된다. 상수는 학습률 하이퍼파라미터이며, 기본값은 1이다. 예측기가 정확할수록 가중치는 높아지고, 랜덤 추측이라면 가중치는 0에 가까워지며, 그보다 나쁘면 가중치는 음수가 된다.

![image](https://github.com/user-attachments/assets/086698d1-fca4-40e0-8e43-d037e608600d)

이제 샘플의 가중치를 아래의 식을 사용하여 업데이트하겠다. 즉, 잘못 분류된 샘플의 가중치를 높일 것이다.

![image](https://github.com/user-attachments/assets/82e7612a-8aa3-4019-8954-ca51c152d420)

그리고 모든 샘플의 가중치를 정규화한다.

새 예측기가 업데이트된 가중치를 사용해 훈련되고 전체 과정이 반복된다. (또 다른 예측기를 훈련)
지정된 예측기 수에 도달하거나 완벽한 예측기가 만들어지면 알고리즘은 중지된다.

![image](https://github.com/user-attachments/assets/981de63d-4009-4c6c-9ac1-dc9efbcebb77)

예측 시 AdaBoost는 단순히 모든 예측기의 예측을 계산하고, 예측기 가중치 alpha를 더해 예측 결과를 만든다. 가중치 합이 가장 큰 클래스가 예측 결과가 된다.

다음 코드는 사이킷런의 AdaBoostClassifier를 사용하여 200개의 아주 얕은 결정 트리를 기반으로 하는 AdaBoost 분류기를 훈련시킨다.

여기서 사용하는 결정 트리는 max_depth=1이다. 이 트리가 AdaBoostClassifier의 기본 추정기이다.

```python
from sklearn.ensemble import AdaBoostClassifier

ada_clf = AdaBoostClassifier (DecisionTreeClassifier (max_depth = 1), n_estimators = 30, learning_rate = 0.5, random_state = 42)
ada_clf.fit(X_train, y_train)
```

### 7.5.2 그레이디언트 부스팅
그레이디언트 부스팅은 앙상블에 이전까지의 오차를 보정하도록 예측기를 순차적으로 추가한다는 점에서 AdaBoost와 유사해보인다. 하지만 AdaBoost처럼 반복마다 샘플의 가중치를 수정하는 대신 이전 예측기가 만든 **잔여 오차**에 새로운 예측기를 학습시킨다. 

뭔 개소리일까?

일반적인 부스팅 개념 : **이전까지의 예측이 틀린 부분을 보완하는 방식으로** 새 모델을 추가하는 게 핵심이다.

그레이디언트 부스팅 : 이전 모델이 틀린 부분을 직접 잔여 오차로 계산해서, 이 잔여 오차를 예측하도록 새로운 모델을 학습시킴. 즉, **새 모델이 해야 할 일은 이전 모델의 실수를 줄이는 것**

반복 과정을 통해 예측값이 실제값에 가까워지도록!

결정 트리를 기반 예측기로 사용하는 간단한 회귀 문제를 풀어보자. (**그레이디언트 트리 부스팅**)

먼저, 2차 방정식으로 잡음이 섞인 데이터셋을 생성하고 DecisionTreeRegressor를 학습시켜보자.
```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor
np.random.seed(42)
X = np.random.rand(100,1) - 0.5
y = 3*X[:,0]**2 + 0.05 * np.random.randn(100) # y = 3x^2 + 가우스 잡음

tree_reg1 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg1.fit(X, y)
```
그 다음, 첫 번째 예측기에서 생긴 잔여 오차에 두 번째 DecisionTreeRegressor을 훈련시키자.

```python
y2 = y - tree_reg1.predict(X)
tree_reg2 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg2.fit(X, y2)
```
그 다음, 두 번째 예측기가 만든 잔여 오차에 세 번째 회귀 모델을 훈련시키자.
```python
y3 = y2 - tree_reg2.predict(X)
tree_reg3 = DecisionTreeRegressor(max_depth=2, random_state=42)
tree_reg3.fit(X, y3)
```
이제, 세 개의 트리를 포함하는 앙상블 모델이 생겼다. 새 샘플에 대한 예측을 만들려면 모든 트리의 예측을 더하면 된다.
```python
X_new = np.array([[-0.4],[0.],[0.5]])
sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))
```
결과는 다음과 같다.

array([0.49484029, 0.04021166, 0.75026781]) 

![image](https://github.com/user-attachments/assets/2b2fdde9-5b79-47f1-b46a-338cb3a747e9)

그림의 왼쪽 열은 이 세 트리의 예측이고, 오른쪽 열은 앙상블의 예측이다. 분석해보자.

첫 번째 행 : 앙상블에 트리 한 개만 존재하여 첫 번째 트리의 예측과 완전히 같음

두 번째 행 : 새로운 트리가 첫 번째 트리의 잔여 오차에 대해 학습됨 (오른쪽의 앙상블 예측 = 두 개의 트리 예측 합)

세 번째 행 : 또 다른 트리가 두 번째 트리의 잔여 오차에 훈련됨 (앙상블에 트리가 추가될 수록 예측력 향상)

사이킷런의 GradientBoostingRegressor를 사용하여 GBRT 앙상블을 간단하게 훈련시킬 수 있다.
트리 수 (n_estimators)와 같이 앙상블의 훈련을 제어하는 매개변수, RandomForestRegressor와 비슷하게 결정 트리의 성장 제어하는 매개변수 (max_depth, min_samples_leaf)를 포함
앙상블을 만드는 코드를 살펴보자.

```python
from sklearn.ensemble import GradientBoostingClassifier
gbrt = GradientBoostingClassifier(max_depth=2, n_estimators=3, learning_rate=1.0, random_state=42)
gbrt.fit(X_train, y_train)
```
learning_rate 매개변수가 각 트리의 기여도를 조절한다. 낮게 설정하면 앙상블을 훈련 세트에 학습시키기 위해 많은 트리가 필요하지만 예측성능은 좋아진다. 이를 **축소**라고 부른다. (규제 방법)

![image](https://github.com/user-attachments/assets/5805ef0a-67ee-45e7-a38f-85240d3ae819)

여기서 추정기 개수 = 트리 개수

트리를 더 많이 추가하면 GBRT가 훈련 세트에 과대적합될 것이다.

최적의 트리 개수를 찾으려면 평소처럼 GridSearchCV 또는 RandomizedSearchCV를 사용하여 교차검증을 수행하거나, n_iter_no_change 하이퍼파라미터를 정숫값 (ex : 10)으로 설정하여 훈련 중에 마지막 10개의 트리가 도움이 되지 않는 경우 GradientBoostingRegressor로 하여금 트리 추가를 자동으로 중지한다. (**단순 조기 종료 방법**)
```python
from sklearn.ensemble import GradientBoostingRegressor
gbrt_test = GradientBoostingRegressor (max_depth = 2, learning_rate = 0.05, n_estimators = 500,n_iter_no_change= 10, random_state = 42)
gbrt_test.fit(X_train, y_train)
```
n_iter_no_change를 너무 낮게 설정하면 훈련이 일찍 중단되어 모델이 과소적합될 수 있고, 높게 설정하면 과대적합된다. 학습률을 상당히 작게 설정하고 추정기 개수를 많이 설정했지만 학습된 앙상블의 실제 추정기 개수는 조기 종료 덕에 훨씬 적다.

GradientBoostingRegressor은 랜덤으로 선택된 훈련 샘플로 학습할 수 있는 subsample 매개변수를 지원한다. 이는 편향이 높아지는 대신 분산이 줄고 훈련 속도도 상당히 빨라진다. 이를 **확률적 그레이디언트 부스팅**이라고 한다.

### 7.5.3 히스토그램 기반 그레이디언트 부스팅
사이킷런은 대규모 데이터셋에 최적화된 또 다른 그레이디언트 부스팅 결정 트리 구현인 **히스토그램 기반 그레이디언트 부스팅(HGB)** 도 제공한다. 이는 **입력 특성을 구간으로 나누어 정수로 대체하는 방식**으로 작동한다. (ex : 키(height)값들을 몇 개의 구간으로 나누는 행위)
구간의 개수는 max_bins 하이퍼파라미터에 의해 제어되며, 기본값은 255이고 이보다 높게 설정할 수 없다. 

장점 :

1. 학습 알고리즘이 평가해야 하는 가능한 임곗값의 수를 크게 줄일 수 있음 ( 0, 1, 2의 bin 값으로 축소)
2. 정수로 작업하여 더 빠르고 메모리 효율적인 데이터 구조를 사용할 수 있음
3. 각 트리를 학습할 때 특성을 정렬할 필요 없음 (정렬 비용 = (특성 개수) × (데이터 크기 × log(데이터 크기))이므로, 특성이 많아질수록 연산량이 기하급수적으로 증가하는 것을 방지)

정리하자면, 이 구현의 계산 복잡도는 **정렬 비용 = (특성 개수) × (데이터 크기 × log(데이터 크기))** 가 아닌, **연산량 = (특성 개수) × (bin 개수)** 이다. 그러나, 구간 분할은 규제처럼 작동해 정밀도 손실을 유발하므로 데이터셋에 따라 과대적합을 줄이는 데 도움이 될 수도 있고, 과소적합을 유발할 수도 있다.

사이킷런은 HGB를 위한 두 가지 클래스 HistGradientBoostingRegressor과 HistGradientBoostingClassifier을 제공한다. 이 클래스들은 GradientBoostingRegressor, GradeintBoostingClassifier과 유사하지만 몇 가지 차이가 존재한다. 

1. 인스턴스 수가 10000개보다 많으면 조기 종료가 자동으로 활성화됨 (early_stopping 매개변수를 True 또는 False로 설정하여 조기종료를 항상 켜거나 끌 수 있다.)
2. subsample 매개변수 지원 X (샘플 비율 지정에 대한 지원 X)
3. n_estimators 매개변수가 max_iter으로 바뀜
4. 조정할 수 있는 결정 트리 하이퍼파라미터는 max_leaf_nodes, min_samples_leaf, max_depth 뿐

이 HGB는 범주형 특성과 누락된 값을 지원한다. 이로 인해 전처리가 상당히 간소화된다. 그러나 범주형 특성은 0~max_bins 사이의 정수로 표현해야 한다.
다음 코드를 살펴보자.
```python
from sklearn.pipeline import make_pipeline
from sklearn.compose import make_column_transformer
from sklearn.ensemble import HistGradientBoostingRegressor
from sklearn.preprocessing import OrdinalEncoder
from sklearn.datasets import fetch_california_housing # import to get housing data

# Load the California housing dataset
housing, housing_labels = fetch_california_housing(return_X_y=True, as_frame=True)
print(housing.columns)
hgb_reg = make_pipeline (make_column_transformer((OrdinalEncoder(), ['HouseAge']), remainder = 'passthrough'), HistGradientBoostingRegressor(categorical_features=[0],random_state = 42))
hgb_reg.fit(housing, housing_labels)
```
전체 파이프라인이 임포트 구문만큼 짧다. 누락된 값을 채우고, 스케일을 조정하고, 원-핫 인코딩을 처리하지 않아도 되므로 정말 편리하다. 범주형 열의 인덱스는 categorical_features를 설정해야 한다는 점에 유의하자. 하이퍼파라미터 튜닝 없이도 이 모델은 약 47600의 RMSE를 산출하는데 나쁘지 않은 수치이다.

## 7.6 스태킹
마지막 앙상블 모델인 **스태킹**을 살펴보겠다. 이는 *앙상블에 속한 모든 예측기의 예측을 취합하는 (직접 투표 같은) 간단한 함수를 사용하는 대신 취합하는 모델을 훈련시킬 수 있을까?* 라는 아이디어에서 출발한다. 

복습을 통해 기존 앙상블 모델의 예측 취합 방식을 살펴보자.

1. 배깅 (Bagging) -> 다수결 투표 or 평균 (모델을 여러 개 만들고, 그 예측을 평균 내거나 다수결 투표로 결정)
2. 부스팅 (Boosting) -> 가중치 조정 후 최종 예측 (보완 후 최종 예측값 산출)

스태킹은 개별 모델(베이스 모델)의 예측을 또 다른 모델(메타 모델)이 학습하도록 하는 방식으로, 예측기를 하나 더 학습시킨다.

![image](https://github.com/user-attachments/assets/6e133f33-c8d4-492b-a91d-9fc6c1b58a5c)

![image](https://github.com/user-attachments/assets/f3df09f0-a6a6-4af7-aadf-c29638461d13)

위 그림은 새로운 샘플에 회귀 작업을 수행하는 앙상블을 보여준다. 아래 세 예측기는 각각 다른 값 (3.1, 2.7, 2.9)을 예측하고 마지막 예측기 (**블렌더** 또는 **메타 학습기**)가 이 예측을 입력으로 받아 최종 예측 3.0을 만들어낸다.

![image](https://github.com/user-attachments/assets/e6f4b299-03c0-4fe7-a1cd-efca1b10690e)

블렌더를 훈련하려면 먼저 블렌딩 훈련 세트(**원본 데이터 : 입력 특성과 타깃 포함**)를 만들어야 한다. 앙상블의 모든 예측기(**선형 회귀, 결정 트리, 신경망 등**)에서 cross_val_predict()를 사용(**데이터를 여러개의 폴드로 나누어 일부를 검증용 데이터로 사용하면서 예측 수행, 그리고 그 예측값을 사용하여 블렌딩 훈련세트를 만듦, 각 샘플에 대해 기본 예측기들이 예측한 값들이 새로운 입력 특성이 됨**)하여 원본 훈련 세트에 있는 각 샘플에 대한 표본 외 예측을 얻는다. 이를 블렌더를 훈련하기 위한 입력 특성으로 사용하고, 타깃(**정답**)은 원본 훈련 세트에서 간단하게 복사(**원본 세트의 타깃**)할 수 있다. 원본 훈련 세트의 특성 개수에 관계없이 블렌딩 훈련 세트에는 예측기 당 하나의 입력 특성이 포함된다. (이 예시에서는 세 개) 블렌더가 학습되면 기본 예측기는 전체 원본 훈련 세트로 마지막에 한 번 더 재훈련된다. (블렌더가 학습되었으면, **기본 예측기들을 원본 훈련 세트로 다시 한 번 훈련**. **그림에서 훈련 세트에서 예측기로 다시 연결된 경로**)

위의 그림은 단일 블렌더였다면, 다음 그림은 성능을 좀 더 끌어올릴 수 있는 다중 블렌더이다.

![image](https://github.com/user-attachments/assets/e6dc1e1f-6715-43b7-b595-285d1618c872)

실제로 여러 가지 블랜더 (선형 회귀 블랜더, 랜덤 포레스트 블랜더)를 이러한 방식으로 훈련하여 전체 블렌더 계층을 얻고, 그 위에 다른 블렌더를 추가하여 최종 예측을 생성하는 것이 가능하다.  (**블랜더의 앙상블** 또는 **다중 블렌더 계층**)

![image](https://github.com/user-attachments/assets/34aa13ff-2aef-4eb4-8e4d-503e9f97e04d)

사이킷런은 스태킹 앙상블을 위한 StackingClassifier와 StackingRegressor 클래스를 제공한다. 예를 들어, 이 장의 시작 부분의 moons 데이터셋에 사용한 VotingClassifier을 StackingClassifer으로 대체할 수 있다.












