---
layout : post
title : 4주차
categories : ML 기초
---
# 쿠다 ML 기초 4주차
이번 주차에서는 앙상블 학습과 랜덤 포레스트에 대하여 공부한다.

일련의 예측기 (분류나 회귀 모델)로부터 예측을 수집하면 가장 좋은 모델 하나보다 더 좋은 예측을 얻을 수 있다. 이 일련의 예측기를 **앙상블**이라고 하며, 이러한 예측과정을 **앙상블 학습**이라고 한다.

앙상블 방법 중 하나의 예시 : 훈련 세트로부터 랜덤으로 각기 다른 서브셋을 만들어 일련의 결정 트리 분류기를 훈련 시킴. 개별 트리 예측을 모아 가장 많은 선택을 받은 클래스를 앙상블의 예측으로 삼음. 결정트리의 앙상블을 **랜덤 포레스트**라고 함

실제로 앙상블 방법을 사용하여 여러 괜찮은 예측기를 연결하여 더 좋은 예측기를 만든다.

## 7.1 투표 기반 분류기

![image](https://github.com/user-attachments/assets/29063c51-27c9-4562-a54b-8c4732e7eb11)

위와 같이 정확도가 80%인 분류기 여러 개를 훈련시켰다고 가정하자. 더 좋은 분류기를 만들 수 없을까?

이에 대한 간단한 방법은 각 분류기의 예측을 집계하는 것이다. 가장 많은 표를 얻은 클래스가 앙상블의 예측이 된다. 이러한 분류기를 **직접 투표 분류기**라고 한다.

![image](https://github.com/user-attachments/assets/83357205-cb3c-4ee5-87c1-8cf21d406ee5)

각 분류기가 **약한 학습기**일지라도 앙상블에 있는 약한 학습기가 충분하게 많고 다양하다면 앙상블은 **강한 학습기**가 될 수 있다. ***큰수의 법칙***을 생각한다면 연결지어 이해하기 쉬울 것이다.

앙상블 방법은 예측기가 가능한 한 서로 독립적이고 오차에 상관관계가 없을 때 최고의 성능을 발휘한다. 즉, 각 분류기가 서로 다른 방식으로 틀려야 보완적으로 최적의 앙상블 효과를 볼 수 있게 되는 것이다.

사이킷런은 이름/예측기 쌍의 리스트를 제공하기만 하면 일반 분류기처럼 쉽게 사용할 수 있는 **VotingClassifier** 클래스를 제공한다. moons 데이터셋을 사용하여 3가지 다양한 분류기로 구성된 투표 기반 분류기를 생성하고 훈련하겠다.

```python
from sklearn.datasets import make_moons
from sklearn.ensemble import RandomForestClassifier, VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

X,y = make_moons(n_samples=500, noise=0.30, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42) 

voting_clf = VotingClassifier (
    estimators=[('lr', LogisticRegression(random_state = 42)), ('rf', RandomForestClassifier(random_state = 42)), ('svc', SVC(random_state = 42))]
)
voting_clf.fit(X_train, y_train)  
```
이에 대한 결괏값은 다음과 같다.

![image](https://github.com/user-attachments/assets/1066ea9f-6596-49e5-b9bd-0ffa8728a3bb)

VotingClassifier을 훈련할 때 이 클래스는 모든 추정기를 복제하여 복제된 추정기를 훈련한다. 원본 추정기는 estimators 속성을 통해 참조할 수 있으며 훈련된 복제본은 estimators_속성에 저장된다. 테스트 세트에서 훈련된 각 분류기의 정확도를 살펴보자.

```python
for name, clf in voting_clf.named_estimators_.items() :
  print (name, '=', clf.score(X_test, y_test))
```
이에 대한 결괏값은 아래와 같다.

![image](https://github.com/user-attachments/assets/0dc19dda-5465-4ab8-864e-8c9fc424c688)

투표 기반의 predict() 메소드를 호출하여 직접 투표를 수행할 수 있다. 아래의 코드를 통해 세 분류기 중 두 개가클래스 1을 예측하여 전체적으로 첫번째 샘플에 대해 클래스 1을 예측하는 작업을 살펴보자.

```python
print (voting_clf.predict(X_test[:1]))
print ([clf.predict(X_test[:1]) for clf in voting_clf.estimators_])
```
![image](https://github.com/user-attachments/assets/1ad486e9-98ca-4ae1-adfc-dc7076738a40)

다음은 테스트 세트에서 투표 기반 분류기의 성능을 살펴보겠다.

```python
voting_clf.score (X_test, y_test)
```
값은 0.912로 예상대로 나온다. 투표 기반 분류기가 다른 개별 분류기보다 성능이 더 높다.

모든 분류기가 클래스의 확률을 예측할 수 있으면, 개별 분류기의 예측을 평균 내어 확률이 가장 높은 클래스를 예측할 수 있다. 이를 **간접 투표**라고 한다. 이는 확률이 높은 투표에 비중을 두기에 직접 투표 방식보다 성능이 높다. 투표 기반 분류기의 voting 매개변수를 soft로 바꾸고 모든 분류기가 클래스의 확률을 추정할 수 있도록 하면 된다.SVC는 probability = True로 지정하면 된다. 살펴보자.

```python
voting_clf.voting = 'soft'
voting_clf.named_estimators['svc'].probability = True 
voting_clf.fit(X_train, y_train)
print (voting_clf.score(X_test, y_test))
```
간접 투표 방식을 사용하여 92%의 정확도를 달성하게 되었다. (결괏값)

## 7.2 배깅과 페이스팅
각기 다른 훈련 알고리즘을 사용할 수도 있지만, 같은 알고리즘을 사용하고 훈련세트의 서브셋을 랜덤으로 구성하여 분류기를 각기 다르게 학습시키는 방법도 있다. 훈련 세트에서 중복을 허용하여 샘플링하는 방식을 **배깅**이라고 하며, 중복을 허용하지 않고 샘플링하는 방식을 **페이스팅**이라고 한다.

이 두 방법은 같은 훈련 샘플을 여러 개의 예측기에 걸쳐 사용할 수 있다. 하지만 배깅만이 한 예측기를 위해 같은 훈련 샘플을 여러 번 샘플링할 수 있다. 이 샘플링과 훈련 과정을 아래에 나타내었다.

![image](https://github.com/user-attachments/assets/098543c8-a838-42d2-b07a-b9a96845e471)

모든 예측기가 훈련을 마치면 앙상블은 모든 예측기의 예측을 모아서 새로운 샘플에 대한 예측을 만든다. 집계 함수는 일반적으로 분류일 때는 **통계적 최빈값** (직접 투표 분류기처럼 가장 많은 예측 결과)을, 회귀에 대해서는 평균을 계산한다. 개별 예측기는 원본 훈련 세트로 훈련시킨 것보다 훨씬 편향되어 있으나, 집계함수를 통과하면 편향과 분산이 모두 감소한다.

위 그림에서 예측이나 학습을 병렬로 수행할 수 있는 확장성을 볼 수 있다. 이러한 점 덕분에 배깅과 페이스팅의 인기가 높다.

### 7.2.1 사이킷런의 배깅과 페이스팅
사이킷런은 배깅과 페이스팅을 위해 BaggingClassifier (회귀의 경우 BaggingRegressor)를 제공한다. 아래의 코드는 결정 트리 분류기 500개의 앙상블을 훈련시키는 코드이다. 각 분류기는 훈련 세트에서 중복을 허용하여 랜덤으로 선택된 100개의 샘플로 훈련된다. (배깅의 경우이고, 페이스팅의 경우에는 bootstrap=False로)
```python
from sklearn.ensemble import BaggingClassifier
from sklearn.tree import DecisionTreeClassifier

bag_clf = BaggingClassifier(
    DecisionTreeClassifier(), n_estimators=500,
    max_samples=100, bootstrap=True, n_jobs=-1, random_state=42
) # n_jobs : 사이킷런이 훈련과 예측에 사용할 CPU 코어 수 지정 (-1이면 가용한 모든 코어)
bag_clf.fit(X_train, y_train)
```

![image](https://github.com/user-attachments/assets/98063a79-ef76-43f4-9e4f-9db14e016a91)

그림을 살펴보자. 단일 결정 트리보다 앙상블이 훨씬 일반화가 잘되어있다. 앙상블은 비슷한 편향에서 더 작은 분산을 만들어낸다. 

배깅은 각 예측기가 학습하는 서브셋에 다양성을 추가하므로 배깅이 페이스팅보다 편향이 조금 더 높지만, 다양성을 추가함으로써 예측기들의 상관관계를 줄이므로 앙상블의 분산도 줄어든다. 시간과 여유가 있다면 교차검증으로 배깅과 페이스팅을 모두 평가해서 나은쪽을 선택하자. (전반적으로는 배깅이 더 나은 모델을 만듦)

### 7.2.2 OOB 평가
OOB 평가는 배깅에서 훈련에 사용되지 않은 데이터(OOB 샘플)을 이용해 모델의 성능을 평가하는 방법이다. 평균적으로 37%의 샘플이 남는데, 이를 가지고 예측을 수행한다. 그리하여 모든 모델에 대해 OOB 샘플 예측 결과를 평균 내어 최종 OOB 평가를 수행, 이를 종합하여 모델 성능을 평가한다.

사이킷런에서 BaggingClassifier을 만들 때 oob_score = True로 지정하면 훈련이 끝난 후 자동으로 OOB 평가를 수행한다. 코드를 살펴보자.

```python
bag_clf = BaggingClassifier (DecisionTreeClassifier(), n_estimators=500, bootstrap=True, n_jobs=-1, oob_score=True, random_state=40)
bag_clf.fit(X_train, y_train)
print (bag_clf.oob_score_)
```
값은 0.896이 나온다. OOB 평가 결과를 보면 이 BaggingClassifier이 테스트 세트에서 약 89.6%의 정확도를 얻을 것으로 보인다. 확인해보자.

```python
from sklearn.metrics import accuracy_score
y_pred = bag_clf.predict(X_test)
print (accuracy_score(y_test, y_pred)) 
```
값은 0.912. 테스트 세트에서 91.2%의 정확도를 얻었다. OOB 평가는 2% 정도 낮아 조금 비관적이었다.

OOB 샘플에 대한 결정 함수의 값도 oob_decision_function_ 변수에서 확인할 수 있다. 결정함수는 각 훈련 샘플의 클래스 확률을 반환한다. 아래의 결과를 보면 OOB 평가는 첫 번째 훈련 샘플이 양성 클래스에 속할 확률을 67%, 음성 클래스를 32%로 추정하고 있다.

```python
bag_clf.oob_decision_function_[:3]
```
![image](https://github.com/user-attachments/assets/02fc0e96-9a69-4140-9ff8-352000d3d34f)






