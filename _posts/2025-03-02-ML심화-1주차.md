---
layout : post
title : ML 심화 1주차
categories : ML 심화
---

**퍼셉트론 : 가장 간단한 인공 신경망 구조. 인공 뉴런의 한 종류 (TLU 또는 LTU 등 인공 뉴런 기반)**

**퍼셉트론의 기본 구조**

(1) 입력과 가중치

![image](https://github.com/user-attachments/assets/ae0a582c-df51-49b4-9cfc-cc68f20dfcc3)

입력의 선형 함수 z 계산 후, 결과에 **계단 함수** 적용 (**헤비사이드 계단 함수**)

(2) 활성화 함수 (계단 함수)

![image](https://github.com/user-attachments/assets/94e1e0d1-259a-40c9-b112-e4986e9dfbbd)

z>=0 : 출력 1 (양성클래스) / z<0 : 출력 0 (음성 클래스)

**퍼셉트론의 학습 과정**
주어진 데이터에 대해 가중치를 조정하는 과정을 통해 학습 (퍼셉트론 학습 규칙 사용)
(1) 가중치 업데이트
예측값과 실제값이 다를 경우에만 가중치를 업데이트함

![image](https://github.com/user-attachments/assets/61d67fcb-c3f6-470a-810e-59139a98d1c2)

(2) 학습 알고리즘

-가중치 w와 편향 b를 초기화 

-데이터 샘플을 하나씩 가져와서 예측값 계산

-예측값과 실제값이 다르면 가중치 업데이트

-모든 데이터 반복하며 학습 진행

![image](https://github.com/user-attachments/assets/3fdd0648-2299-42f0-ac3a-0f9ec413ff45)

※ 각 출력 뉴런의 결정 경계 : 선형. 훈련 샘플이 선형적으로 구분될 수 있다면 알고리즘이 정답에 수렴함. (**퍼셉트론 수렴 이론**)

사이킷런 Perceptron 클래스를 통해 붓꽃 데이터셋을 적용하는 과정
```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris (as_frame = True)
X = iris.data[['petal length (cm)', 'petal width (cm)']].values
y = (iris.target == 0)

per_clf = Perceptron(random_state=42)
per_clf.fit(X,y)

X_new = [[2,0.5],[3,1]]
y_pred = per_clf.predict(X_new)
```
**퍼셉트론의 한계와 보완**

(1) 선형 분리 문제

단순한 선형 결정 경계 특성을 가진 퍼셉트론 ☞ XOR (배타적 논리합) 문제와 같은 **비선형 문제** 해결 불가

(2) 다층 퍼셉트론 (MLP) 등장 ☞ 복잡한 문제 해결 가능

(다중 퍼셉트론이 XOR을 해결하는 과정)

![image](https://github.com/user-attachments/assets/2e3ccd5d-e236-4d3e-8315-dfaf7e9161bc)

**역전파** : 오차를 출력층에서 입력층으로 전파하여 가중치를 조정하는 알고리즘 (MLP 및 신경망 학습의 핵심 알고리즘. 경사 하강법 이용하여 가중치 최적화)

**후진 모드 자동 미분 & 경사 하강법을 결합한 알고리즘**

(1) 역전파가 필요한 이유

은닉층이 존재하는 MLP의 특성으로 인해 직접적인 오차 계산 어려움 ☞ **역전파를 통해 출력층에서 발생한 오차를 은닉층 바향으로 거꾸로 전파하면서 가중치 업데이트**

(2) 역전파의 주요 과정

▶ 입력값 받아 순차적으로 예측을 만듦 (정방향 계산)

▶ 역방향으로 각 층 거치면서 각 연결이 오차에 기여한 정도 측정 (역방향 계산)

▶ 오차 감소를 위해 가중치와 편향 조정 (경사 하강법)

***※ 신경망으로 무엇을 할 수 있는가? : 회귀, 분류 등***

**회귀를 위한 다층 퍼셉트론**

▶ 입력층 - 은닉층 - 출력층 거쳐 연속적인 값 예측

▶ 출력층에서 활성화 함수 사용 X (연속적이며 자유로운 값 보장) 

▶ 손실 함수로 MSE 사용. 훈련 세트에 이상치가 많은 경우 평균 절대 오차 대신 사용 또는 두 가지를 조합한 **후버 손실** 사용 가능

사이킷런 MLPRegressor 클래스를 통해 캘리포니아 주택 데이터셋 훈련
```python
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler 

housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state = 42)

mlp_reg = MLPRegressor(hidden_layer_sizes=[50,50,50],random_state=42)
pipeline = make_pipeline(StandardScaler(), mlp_reg)
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_valid)
mse = mean_squared_error(y_valid, y_pred) # 약 0.505 랜덤 포레스트 분류기 결괏값과 유사
```

**분류를 위한 다층 퍼셉트론**

▶ 입력층 - 은닉층 - 출력층 거쳐 클래스 확률 예측

▶ 출력층에서 활성화 함수 (Sigmoid, Softmax) 적용하여 분류 문제를 해결

이진분류 : 시그모이드 활성화 함수 사용. 다중 분류 : 소프트맥스 활성화 함수

▶ 손실함수로 크로스 엔트로피 사용. 역전파를 통해 가중치 업데이트.

※ 회귀에서 출력층에 활성화 함수를 사용하지 않는 이유 : 출력값이 연속적인 실수 값이므로 특정 범위로 제한하면 안 됨. 분류는 특정 범위 (0~1)로 변환하여 확률 형태로 해석할 수 있어야 함. (가장 높은 확률 가진 클래스 선택)

**케라스로 다층 퍼셉트론 구현하기**

케라스 : 모든 종류의 신경망을 쉽게 만들고 훈련, 평가, 실행할 수 있는 텐서플로의 고수준 딥러닝 API

**시퀀셜 API로 이미지 분류기 만들기**

(1) 케라스로 데이터셋 적재하기

```python
import tensorflow as tf

fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist
X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]
X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal","Shirt", "Sneaker","Bag","Ankle boot # 레이블에 해당하는 아이템 나타내기
```
훈련 세트 (60000) & 테스트 세트 (10000) 중 검증세트를 위한 5000개 이미지

(2) 시퀀셜 API로 모델 만들기
```python
tf.random.set_seed(42)
model = tf.keras.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape=[28,28]))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(300, activation='relu')) #은닉층 추가
model.add(tf.keras.layers.Dense(100, activation='relu')) #은닉층 추가
model.add(tf.keras.layers.Dense(10, activation='softmax')) #출력층 추가
model.summary() 
```
**※Dense 층은 대칭성을 깨기 위해 연결 가중치를 랜덤으로 초기화 (편향은 0으로 초기화)**

(3) 모델 컴파일

사용할 손실 함수와 옵티마이저 (**신경망이 학습할 때 가중치(Weight)와 편향(Bias)를 업데이트하는 역할**) 지정 (부가적으로 훈련과 평가 시 계산할 지표 지정 가능)

```python
model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd", metrics=["accuracy"])
```

(4) 모델 훈련과 평가

fit() 메소드 사용하여 모델 훈련

```python
history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))
```
History 객체에는 훈련 파라미터, 수행된 에포크 리스트가 포함됨. 에포크가 끝날 때마다 훈련 세트와 검증 세트에 대한 손실과 측정한 지표를 담은 딕셔너리 존재. 

다음은 이 딕셔너리를 시각화한 그래프와 코드.
```python
import matplotlib.pyplot as plt
import pandas as pd

pd.DataFrame(history.history).plot(figsize=(8,5), xlim=[0,29],ylim=[0,1],grid=True, xlabel='epoque', style = ['r--','r--','b-','b-*'])
plt.show()
```
![image](https://github.com/user-attachments/assets/0b29a324-f9ca-4bda-acba-491df599d2a0)

훈련, 검증 정확도 ↑ / 훈련, 검증 손실 ↓

성능이 만족스럽지 않으면 처음으로 되돌아가서 하이퍼파라미터, 학습률 튜닝. 확인 시 층 개수, 층에 있는 뉴런 개수, 은닉 층이 사용하는 활성화 함수와 같은 모델의 하이퍼파라미터 튜닝

(5) 모델로 예측 만들기

predict() 메소드를 사용해 새로운 샘플에 대해 예측을 만들 수 있음. 
```python
X_new = X_test[:3]
y_proba = model.predict(X_new)
y_proba.round(2)
```

**시퀀셜 API로 회귀용 다층 퍼셉트론 만들기**

회귀 문제에서는 연속적인 실수 값을 예측하는 것이 목표
```python
import tensorflow as tf

tf.random.set_seed(42)

# 데이터 정규화 레이어
norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])

# 회귀용 MLP 모델 생성
model = tf.keras.Sequential([
    norm_layer,
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(1)  # 회귀이므로 출력층에
```






