---
layout : post
title : ML 심화 1주차
categories : ML 심화
---

**퍼셉트론 : 가장 간단한 인공 신경망 구조. 인공 뉴런의 한 종류 (TLU 또는 LTU 등 인공 뉴런 기반)**

**퍼셉트론의 기본 구조**

(1) 입력과 가중치

![image](https://github.com/user-attachments/assets/ae0a582c-df51-49b4-9cfc-cc68f20dfcc3)

입력의 선형 함수 z 계산 후, 결과에 **계단 함수** 적용 (**헤비사이드 계단 함수**)

(2) 활성화 함수 (계단 함수)

![image](https://github.com/user-attachments/assets/94e1e0d1-259a-40c9-b112-e4986e9dfbbd)

z>=0 : 출력 1 (양성클래스) / z<0 : 출력 0 (음성 클래스)

**퍼셉트론의 학습 과정**
주어진 데이터에 대해 가중치를 조정하는 과정을 통해 학습 (퍼셉트론 학습 규칙 사용)
(1) 가중치 업데이트
예측값과 실제값이 다를 경우에만 가중치를 업데이트함

![image](https://github.com/user-attachments/assets/61d67fcb-c3f6-470a-810e-59139a98d1c2)

(2) 학습 알고리즘

-가중치 w와 편향 b를 초기화 

-데이터 샘플을 하나씩 가져와서 예측값 계산

-예측값과 실제값이 다르면 가중치 업데이트

-모든 데이터 반복하며 학습 진행

![image](https://github.com/user-attachments/assets/3fdd0648-2299-42f0-ac3a-0f9ec413ff45)

※ 각 출력 뉴런의 결정 경계 : 선형. 훈련 샘플이 선형적으로 구분될 수 있다면 알고리즘이 정답에 수렴함. (**퍼셉트론 수렴 이론**)

사이킷런 Perceptron 클래스를 통해 붓꽃 데이터셋을 적용하는 과정
```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.linear_model import Perceptron

iris = load_iris (as_frame = True)
X = iris.data[['petal length (cm)', 'petal width (cm)']].values
y = (iris.target == 0)

per_clf = Perceptron(random_state=42)
per_clf.fit(X,y)

X_new = [[2,0.5],[3,1]]
y_pred = per_clf.predict(X_new)
```
**퍼셉트론의 한계와 보완**

(1) 선형 분리 문제

단순한 선형 결정 경계 특성을 가진 퍼셉트론 ☞ XOR (배타적 논리합) 문제와 같은 **비선형 문제** 해결 불가

(2) 다층 퍼셉트론 (MLP) 등장 ☞ 복잡한 문제 해결 가능

(다중 퍼셉트론이 XOR을 해결하는 과정)

![image](https://github.com/user-attachments/assets/2e3ccd5d-e236-4d3e-8315-dfaf7e9161bc)

**역전파** : 오차를 출력층에서 입력층으로 전파하여 가중치를 조정하는 알고리즘 (MLP 및 신경망 학습의 핵심 알고리즘. 경사 하강법 이용하여 가중치 최적화)

**후진 모드 자동 미분 & 경사 하강법을 결합한 알고리즘**

(1) 역전파가 필요한 이유

은닉층이 존재하는 MLP의 특성으로 인해 직접적인 오차 계산 어려움 ☞ **역전파를 통해 출력층에서 발생한 오차를 은닉층 바향으로 거꾸로 전파하면서 가중치 업데이트**

(2) 역전파의 주요 과정

▶ 입력값 받아 순차적으로 예측을 만듦 (정방향 계산)

▶ 역방향으로 각 층 거치면서 각 연결이 오차에 기여한 정도 측정 (역방향 계산)

▶ 오차 감소를 위해 가중치와 편향 조정 (경사 하강법)

***※ 신경망으로 무엇을 할 수 있는가? : 회귀, 분류 등***

**회귀를 위한 다층 퍼셉트론**

▶ 입력층 - 은닉층 - 출력층 거쳐 연속적인 값 예측

▶ 출력층에서 활성화 함수 사용 X (연속적이며 자유로운 값 보장) 

▶ 손실 함수로 MSE 사용. 훈련 세트에 이상치가 많은 경우 평균 절대 오차 대신 사용 또는 두 가지를 조합한 **후버 손실** 사용 가능

사이킷런 MLPRegressor 클래스를 통해 캘리포니아 주택 데이터셋 훈련
```python
from sklearn.datasets import fetch_california_housing
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPRegressor
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler 

housing = fetch_california_housing()
X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)
X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state = 42)

mlp_reg = MLPRegressor(hidden_layer_sizes=[50,50,50],random_state=42)
pipeline = make_pipeline(StandardScaler(), mlp_reg)
pipeline.fit(X_train, y_train)

y_pred = pipeline.predict(X_valid)
mse = mean_squared_error(y_valid, y_pred) # 약 0.505 랜덤 포레스트 분류기 결괏값과 유사
```

**분류를 위한 다층 퍼셉트론**

▶ 입력층 - 은닉층 - 출력층 거쳐 클래스 확률 예측

▶ 출력층에서 활성화 함수 (Sigmoid, Softmax) 적용하여 분류 문제를 해결

이진분류 : 시그모이드 활성화 함수 사용. 다중 분류 : 소프트맥스 활성화 함수

▶ 손실함수로 크로스 엔트로피 사용. 역전파를 통해 가중치 업데이트.

※ 회귀에서 출력층에 활성화 함수를 사용하지 않는 이유 : 출력값이 연속적인 실수 값이므로 특정 범위로 제한하면 안 됨. 분류는 특정 범위 (0~1)로 변환하여 확률 형태로 해석할 수 있어야 함. (가장 높은 확률 가진 클래스 선택)

**케라스로 다층 퍼셉트론 구현하기**

케라스 : 모든 종류의 신경망을 쉽게 만들고 훈련, 평가, 실행할 수 있는 텐서플로의 고수준 딥러닝 API

**시퀀셜 API로 이미지 분류기 만들기**

(1) 케라스로 데이터셋 적재하기

```python
import tensorflow as tf

fashion_mnist = tf.keras.datasets.fashion_mnist.load_data()
(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist
X_train, y_train = X_train_full[:-5000], y_train_full[:-5000]
X_valid, y_valid = X_train_full[-5000:], y_train_full[-5000:]
X_train, X_valid, X_test = X_train / 255., X_valid / 255., X_test / 255.
class_names = ["T-shirt/top", "Trouser", "Pullover", "Dress", "Coat", "Sandal","Shirt", "Sneaker","Bag","Ankle boot # 레이블에 해당하는 아이템 나타내기
```
훈련 세트 (60000) & 테스트 세트 (10000) 중 검증세트를 위한 5000개 이미지

(2) 시퀀셜 API로 모델 만들기
```python
tf.random.set_seed(42)
model = tf.keras.Sequential()
model.add(tf.keras.layers.InputLayer(input_shape=[28,28]))
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(300, activation='relu')) #은닉층 추가
model.add(tf.keras.layers.Dense(100, activation='relu')) #은닉층 추가
model.add(tf.keras.layers.Dense(10, activation='softmax')) #출력층 추가
model.summary() 
```
**※Dense 층은 대칭성을 깨기 위해 연결 가중치를 랜덤으로 초기화 (편향은 0으로 초기화)**

(3) 모델 컴파일

사용할 손실 함수와 옵티마이저 (**신경망이 학습할 때 가중치(Weight)와 편향(Bias)를 업데이트하는 역할**) 지정 (부가적으로 훈련과 평가 시 계산할 지표 지정 가능)

```python
model.compile(loss="sparse_categorical_crossentropy", optimizer="sgd", metrics=["accuracy"])
```

(4) 모델 훈련과 평가

fit() 메소드 사용하여 모델 훈련

```python
history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))
```
History 객체에는 훈련 파라미터, 수행된 에포크 리스트가 포함됨. 에포크가 끝날 때마다 훈련 세트와 검증 세트에 대한 손실과 측정한 지표를 담은 딕셔너리 존재. 

다음은 이 딕셔너리를 시각화한 그래프와 코드.
```python
import matplotlib.pyplot as plt
import pandas as pd

pd.DataFrame(history.history).plot(figsize=(8,5), xlim=[0,29],ylim=[0,1],grid=True, xlabel='epoque', style = ['r--','r--','b-','b-*'])
plt.show()
```
![image](https://github.com/user-attachments/assets/0b29a324-f9ca-4bda-acba-491df599d2a0)

훈련, 검증 정확도 ↑ / 훈련, 검증 손실 ↓

성능이 만족스럽지 않으면 처음으로 되돌아가서 하이퍼파라미터, 학습률 튜닝. 확인 시 층 개수, 층에 있는 뉴런 개수, 은닉 층이 사용하는 활성화 함수와 같은 모델의 하이퍼파라미터 튜닝

(5) 모델로 예측 만들기

predict() 메소드를 사용해 새로운 샘플에 대해 예측을 만들 수 있음. 
```python
X_new = X_test[:3]
y_proba = model.predict(X_new)
y_proba.round(2)
```

**시퀀셜 API로 회귀용 다층 퍼셉트론 만들기**

회귀 문제에서는 연속적인 실수 값을 예측하는 것이 목표
```python
import tensorflow as tf

tf.random.set_seed(42)

# 데이터 정규화 레이어
norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])

# 회귀용 MLP 모델 생성
model = tf.keras.Sequential([
    norm_layer,
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(1)  # 회귀이므로 출력층에

# 정규화 레이어 생성
norm_layer = tf.keras.layers.Normalization(input_shape=X_train.shape[1:])

# 모델 정의
model = tf.keras.Sequential([
    norm_layer,
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(50, activation="relu"),
    tf.keras.layers.Dense(1)
])

# 옵티마이저 설정
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)

# 모델 컴파일
model.compile(loss="mse", optimizer=optimizer, metrics=["RootMeanSquaredError"])

# 정규화 레이어를 훈련 데이터에 맞게 적응
norm_layer.adapt(X_train)

# 모델 학습
history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))

# 모델 평가
mse_test, rmse_test = model.evaluate(X_test, y_test)

# 새로운 데이터에 대한 예측
X_new = X_test[:3]
y_pred = model.predict(X_new)

# 예측 결과 출력
print("Predictions:", y_pred)

```
위에서는 출력층에 하나의 뉴런만 존재한다. 

입력과 출력이 여러 개이거나 더 복잡한 네트워크 토폴로지를 갖는 신경망을 만들어야 할 때가 있다. 이를 위해 케라스는 함수형 API를 제공한다.

**함수형 API로 복잡한 모델 만들기**

일반적인 MLP : 네트워크 층 전체에 모든 데이터 통과 → 연속된 변환으로 인한 데이터 왜곡 가능성 존재

와이드&딥 신경망 : 순차적 X. 신경망이 복잡한 패턴과 간단한 규칙 모두 학습 가능

1. 층을 만들고 이를 함수처럼 사용하여 입력에서 출력으로 이동, 입력과 출력을 지정하여 케라스 모델 객체 생성하는 코드
```python
import tensorflow as tf

# 첫 번째 모델
normalization_layer = tf.keras.layers.Normalization()
hidden_layer1 = tf.keras.layers.Dense(30, activation="relu")
hidden_layer2 = tf.keras.layers.Dense(30, activation="relu")
concat_layer = tf.keras.layers.Concatenate()
output_layer = tf.keras.layers.Dense(1)

input_ = tf.keras.layers.Input(shape=X_train.shape[1:])
normalized = normalization_layer(input_)
hidden1 = hidden_layer1(normalized)
hidden2 = hidden_layer2(hidden1)
concat = concat_layer([normalized, hidden2])
output = output_layer(concat)

model1 = tf.keras.Model(inputs=[input_], outputs=[output])
```

2. 일부 특성을 짧은 경로로 전달, 다른 특성들을 깊은 경로로 전달하고 싶을 때, 여러 입력을 사용하는 코드 
```python
# 두 번째 모델
input_wide = tf.keras.layers.Input(shape=[5])  # 특성 인덱스 0부터 4까지
input_deep = tf.keras.layers.Input(shape=[6])  # 특성 인덱스 2부터 7까지

norm_layer_wide = tf.keras.layers.Normalization()
norm_layer_deep = tf.keras.layers.Normalization()

norm_wide = norm_layer_wide(input_wide)
norm_deep = norm_layer_deep(input_deep)

hidden1 = tf.keras.layers.Dense(30, activation="relu")(norm_deep)
hidden2 = tf.keras.layers.Dense(30, activation="relu")(hidden1)

concat = tf.keras.layers.Concatenate()([norm_wide, hidden2])
output = tf.keras.layers.Dense(1)(concat)

model2 = tf.keras.Model(inputs=[input_wide, input_deep], outputs=[output])
```

**서브 클래싱 API로 동적 모델 만들기**

시퀀셜 API는 모든 레이어가 순서대로 연결되는 구조 → 유연성 부족

서브클래싱 API 활용 시 모델을 보다 자유롭게 정의 가능

tf.keras.Model을 상속받아 직접 클래스를 정의하고, call() 메소드를 구현하여 입력 데이터의 흐름 조절 가능

예제코드에서는 WideAndDeepModel이라는 클래스를 정의하여 두 개의 입력 레이어를 결합하는 방식을 소개함
```python

```

**모델 저장과 복원하기**

훈련된 케라스 모델을 저장하는 방법을 설명

model.save ("파일명", save_format = 'tf')을 사용하여 모델 저장 가능

save_format = 'h5' 설정 시 HDF5 포맷으로 저장 가능

저장된 모델은 tf.keras.models.load_model (filename)으로 사용하여 복원 가능

save_weights () 사용시 모델의 가중치만 저장 가능

**콜백 사용하기**

훈련 과정 중 자동 저장 및 조기 종료를 위해 콜백 기능 활용

ModelCheckpoint를 사용 시 특정 조건에서 모델을 저장 가능

EarlyStopping 콜백은 일정 에포크 동안 개선이 없을 경우 조기 종료할 수 있도록 설정 가능

사용자가 직접 콜백을 정의하는 방법도 예제 코드로 제공

**텐서보드 사용하기**

텐서보드 : 훈련 과정을 시각화하는 돋구 → 학습 곡선 모니터링에 유용

tf.summary.create_file_writer() 사용하여 로그 저장 가능

: 텐서보드 실행 명령어 → tensorboard --logdir = 경로

SCALARS, GRAPHS, PROJECTOR, PROFILE 등의 기능 활용하여 모델의 다양한 특성을 시각적으로 분석 가능

**신경망 하이퍼파라미터 튜닝하기**

신경망 모델의 성능은 하이퍼파라미터에 영향 ↑ : 적절한 값 조정 필요

GridSearchCV 또는 RandomizeedSearchCV 사용하여 하이퍼파라미터 튜닝 가능

KerasTuner 라이브러리 활용하면 신경망에 적합한 최적의 하이퍼파라미터 탐색 가능

kt.HyperParameters 클래스 활용하여 튜닝 대상 변수 정의 & 최적의 조합 탐색

하이퍼파라미터 튜닝은 연구 및 실무에서 중요한 과정이며, 자동화된 AutoML 기술과 결합할 수 있음.

BayesianOptimization을 활용하여 보다 효율적인 탐색 가능.

Neuroevolution과 같은 기법도 활용 가능하며, 이는 유전 알고리즘을 기반으로 최적의 신경망을 탐색하는 방법.

AutoML 시스템을 사용하면 머신러닝 전문가가 아니더라도 최적의 신경망을 구성하는 데 도움을 받을 수 있음.

**은닉층 개수와 은닉 층의 뉴런 개수**

은닉 층의 수 & 뉴런 개수 : 모델의 표현력과 일반화 성능에 영향을 미침

적절한 은닉층 수와 뉴런 개수를 결정하는 방법 & 과적한 방지 전략 존재

Wide ResNet, Vincent Vanhoucke 등의 연구를 인용하여 은닉층이 너무 많으면 오히려 성능이 저하될 수 있음

**※ 뉴런 개수도 적절한 값으로 설정하는 것이 중요하다.**

**학습률, 배치 크기 그리고 다른 하이퍼파라미터**

신경망의 성능을 최적화하기 위해 중요한 하이퍼파라미터 조정 방법

1. 학습률 (Learning Rate)

학습률은 신경망 학습에서 가장 중요한 하이퍼파라미터 중 하나

일반적으로 학습률을 일정한 값으로 설정하기보다는 점진적으로 변화시키는 것이 효과적

낮은 학습률로 시작해 점차 증가하거나, 10배씩 변화시키는 방법이 많이 사용

특정 방식으로 학습률을 변화시키면 손실 함수의 감소 속도와 안정성이 달라질 수 있음

3. 옵티마이저 (Optimizer)

전통적인 방법인 미니배치 경사 하강법보다 적응형 옵티마이저를 사용하는 것이 효과적

특히 Adam 옵티마이저는 널리 사용되는 강력한 방법 중 하나

(**손실함수를 최소화하기 위해 모델의 가중치를 업데이트하는 역할. 학습과정에서 미분 계산 & 가중치 수정하여 최적의 값 도출**)

옵티마이저 선택은 학습 과정의 속도와 성능에 큰 영향을 미칠 수 있음

5. 배치 크기 (Batch Size)

배치 크기는 모델 성능과 훈련 시간에 직접적인 영향을 미침

큰 배치를 사용하면 GPU 자원을 보다 효율적으로 활용할 수 있지만, 작은 배치가 일반적으로 더 안정적인 학습을 제공

작은 배치 크기는 정규화 효과(**데이터 분포를 일정 범위로 조정하여 신경망 학습을 안정적으로 수행하도록 하는 기법**)를 제공하여 일반화 성능을 향상시킬 수 있음

7. 활성화 함수 (Activation Function)

활성화 함수는 뉴런의 출력을 결정하는 함수로, 신경망의 성능에 큰 영향을 미침

일반적으로 ReLU 함수가 기본적으로 사용되지만, 특정 상황에서는 다른 활성화 함수가 더 나을 수도 있음

9. 반복 횟수 (Epochs)

대부분의 경우 반복 횟수는 튜닝할 필요가 없지만, 특정 조건에서는 최적의 반복 횟수를 찾기 위해 조정해야 할 수도 있음

## 11. 심층 신경망 훈련

심층 신경망 훈련 중 문제 발생 가능성 존재

(출력 층에서 멀어질수록 그레이디언트 점점 감소 or 커지는 문제, 훈련 데이터 불충분성 또는 비용 문제, 훈련 속도 급저하, 수백만 개 파라미터 보유 모델의 훈련 세트 과대적합 위험성)

어떻게 해결할까?

**그레이디언트 소실과 폭주 문제**

*그레디언트 소실 문제*

: 역전파 과정에서 하위 층으로 갈수록 기울기 점점 작아지는 현상

깊은 신경망에서 기울기가 0에 가까워지면 가중치 업데이트가 거의 이루어지지 않음

(특히 시그모이드나 Tanh 활성화 함수 사용 시 심각하게 발생)

*그래디언트 폭주 문제*

: 반대로 기울기가 너무 커져서 가중치가 비정상적으로 커지는 현상

네트워크 매우 불안정 & 학습 속도 느려지고 수렴 어려워짐

*해결 방법*

가중치 초기화 전략 (Glorot 초기화, He 초기화) : 적절한 가중치 초기화로 기울기 문제 완화 가능

배치 정규화 : 각 층의 입력을 정규화하여 학습 안정화

적절한 활성화 함수 선택 : ReLU나 Leaky ReLU 같은 함수를 사용하면 기울기 소실 문제 완화 가능

Glorot (Xavier) 초기화 

- 평균 0, 분산 1/n을 따르는 가중치 분포 사용

- 시그모이드, Tanh 같은 활성화 함수에서 효과적

He 초기화

- ReLU, Leaky ReLU 등에 적합하도록 설계됨

- 분산을 2/n으로 설정하여 기울기 소실 문제 줄임

케라스에서 초기화 적용 예제

```python
import tensorflow as tf
dense = tf.keras.layers.Dense (50, activation = 'relu', kernel_initializer = 'he_normal')
```

Leaky ReLU

- 음수 입력값을 0ㅇ로 만드는 문제 있음 (죽은 뉴런 문제)

- Leaky ReLU는 음수 입력에도 작은 기울기를 유지하여 해결

ex:
```python
leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)
dense = tf.keras.layers.Dense(50, activation=leaky_relu, kernel_initializer='he_normal')
```

ELU

- 음수 입력값에 대해 지수 함수 기반으로 변환하여 기울기 소실 문제 완화

- ReLU보다 학습 속도 빠르고, 깊은 신경망에서도 효과적

SELU

- 자동으로 입력값을 정규화하는 활성화 함수

- 심층 신경망에서 성능 탁월

ELU VS SELU

ELU는 음수 영역에서 지수적 변화를 주어 기울기 소실을 방지하므로, 학습이 더 안정적이고 빠름

SELU는 ELU와 비슷하지만 자동으로 입력을 정규화하여 네트워크를 더욱 안정적으로 유지함. 평균과 분산을 유지하도록 설계되었고, 사용 시 kernel_initializer = 'lecun_normal' 설정 필요

```python
dense = tf.keras.layers.Dense(50, activation='selu', kernel_initializer='lecun_normal')
```

GELU

- 입력을 확률적 방법으로 조정하여 더욱 부드러운 활성화 곡선을 가짐

- Swish 함수와 유사하며, 최근 딥러닝 모델에서 자주 사용

Swish 

- f(x) = x * sigma (x) (시그모이드 곱)

- ReLU보다 부드러운 변화로 학습 속도 향상

Mish

- Swish와 비슷하지만, 더 부드러운 활성화 곡선 가짐

- 일부 신경망에서 더 나은 성능 보임

**배치 정규화**

정의 : 신경망 층의 입력값을 평균 0, 분산 1로 정규화하여 학습을 안정화하는 기법

그레이디언트 소실/폭주 문제를 완화하고 학습 속도를 향상

과정 :

1. 미니배치에서 평균과 분산 계산

2. 정규화 수행

3. 학습 가능한 파라미터 gamma, beta 이용하여 조정

배치 정규화 공식

![image](https://github.com/user-attachments/assets/70b92afb-fc8e-4f37-a2cd-7ecc405012d0)

배치 정규화의 효과

▶ 장점 

그레이디언트 소실 문제 완화

학습 속도 향상

과적합 방지 효과

▶ 단점

추가적인 연산 비용 발생 가능

미니배치 크기가 너무 작으면 효과가 떨어질 수 있음 (평균, 분산 신뢰도 문제, 노이즈 증가, 일반화 성능 저하 등)


**그레이디언트 클리핑**

역전파될 때 특정 임곗ㄱ밧을 넘어서지 못하게 그레이디언트를 잘라내는 것

그레이디언트 소실/폭주 문제를 해결하기 위해서는 적절한 활성화 함수 및 가중치 초기화 기법을 사용해야 한다. 배치 정규화는 딥러닝 모델의 학습을 안정화하고 속도를 높이는 중요한 기법이다. 최근에는 Swish, Mish, GELU와 같은 활성화 함수가 기존의 ReLU보다 더 나은 성능을 보이는 경우가 많다.


**사전 훈련된 층 재사용하기**

기존의 신경망을 새 데이터셋에서 재사용 → 훈련 시간 단축

전이 학습 : 기존에 훈련된 모델의 일부 활용하여 새로운 모델 구축

ex) 동물, 차량, 자연, 생활용품 분류를 학습한 DNN을 활용해 자동 분류 시스템 구축

**케라스를 사용한 전이 학습**

사전 훈련된 모델을 불러와 특정 층을 재사용하는 방식

새로운 데이터셋에서 기존 모델의 일부 층을 고정 (trainable = False)하여 사용

특정 층을 재사용하면서 새로운 분류기 추가, 미세 조정 후 학습

**비지도 사전 훈련**

레이블이 없는 데이터에서 사전 훈련을 수행하는 방법

GAN이나 오토인코더 활용

사전 훈련을 통해 데이터 구조 학습 후, 분류 작업을 위한 최종 모델 훈련

**보조 작업에서 사전 훈련**

직접적인 목표가 아닌 관련된 작업에서 사전 훈련을 수행하는 방식

ex) 자율주행 자동차의 객체 인식을 위한 신경망이 교통 표지판 인식을 먼저 학습

NLP에서는 텍스트 문서의 주제 예측과 같은 작업에서 유용함

**고속 옵티마이저**

신경망 학습 속도 향상을 위한 최적화 알고리즘 소개

SGD보다 빠르고 효율적인 다양한 옵티마이저 활용

*모멘텀 최적화*

경사 하강법의 속도를 높이기 위해 속도 개념 도입

이전 단계의 변화량을 일부 유지하여 방향성 강화 (기본적으로 0.9 모멘텀 값 설정)

*네스테로프 가속 경사*

모멘텀 최적화의 개선된 버전. 미래의 기울기를 예측하여 더욱 빠르고 정확한 경사 이동 수행

경사를 이동하기 전에 먼저 위치를 업데이트하여 최적화 효율 증가

*AdaGrad*

학습률을 자동으로 조정하는 알고리즘

매개변수마다 학습률 조정 → 매개변수마다 학습률 조정하여 자주 업데이트되는 방향 감소, 덜 업데이트되는 방향은 학습률 유지

(학습률이 너무 작아지는 문제 발생 가능)

*RMSProp*

AdaGrad의 문제 (위의 문제) 해결

최근 기울기 변화만 반영하여 학습률 조정 수행

*Adam*

모멘텀 최적화와 RMSProp의 장점을 결합한 최적화 기법

가중치 업데이트에 일관성 유지 + 학습 속도 빠르게 조정

*AdaMax*

Adam의 변형 버전. 무한 norm 기반으로 가중치 업데이트

데이터가 크거나 이상치가 많은 경우 Adam보다 안정적으로 동작.

*Nadam*

Adam과 네스테로프 가속 경사를 결합한 알고리즘

일반적인 Adam보다 빠르게 수렴할 가능성 있음

*AdamW*

Adam의 변형. 가중치 감소를 추가하여 일반화 성능 향상

Adam의 단점 (과적합) 해결하기 위한 접근 방식

**최소 모델 훈련**

모델의 최적화 알고리즘은 학습 속도와 메모리 효율성을 고려해야 함.

가중치 업데이트 방식을 조정하여 메모리를 적게 사용하면서도 성능을 유지.

다양한 옵티마이저를 비교하여 모델과 데이터에 맞는 최적의 방식 선택.

**규제를 사용해 과대적합 방지**

L1 & L2 규제: 가중치의 크기를 제한하여 모델 복잡도 감소.

드롭아웃(Dropout): 일부 뉴런을 무작위로 제거하여 학습을 방해, 일반화 성능 향상.

맥스-노름 규제: 가중치 벡터의 크기를 제한하여 안정적인 학습 유도.

**요약 및 가이드라인**

일반적인 DNN 설정:

가중치 초기화: He 초기화.

활성화 함수: ReLU, Swish.

정규화: 배치 정규화(Batch Normalization), 드롭아웃.

옵티마이저: Adam, RMSProp.

학습률 스케줄링: 지수 감소 또는 1사이클 스케줄링.

**학습률 스케줄링**

거듭제곱 기반, 지수 기반 스케줄링: 학습률을 점진적으로 감소시켜 최적화 안정화.

1사이클 스케줄링: 학습 초기와 후반부에 학습률을 조정하여 빠른 수렴 유도.



