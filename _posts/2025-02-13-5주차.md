---
layout : post
title : 쿠다 ML 기초 5주차
categories : ML 기초
---
#쿠다 ML 기초 5주차

이번 장에서는 차원 축소와 비지도 학습에 대하여 알아보고자 한다.
머신러닝 문제는 훈련 샘플 각각이 수천~수백만 개의 특성을 가지고 있다.
☞ 많은 특성 : 훈련 속도 저하 + 좋은 솔루션 탐색 어려움 : **차원의 저주**
☞ 해결 방법 : 특성 수를 줄여서 가능한 범위로 변경 가능 : **차원 축소**
차원 축소의 장점 : 훈련 속도 향상 (일반적) + 성능 향상의 가능성 (일반적인 것은 아님) + 데이터 시각화 유용성

## 8.1 차원의 저주
3차원 이상의 고차원 공간을 직관적으로 상상할 수 있는가? 어렵다.

![image](https://github.com/user-attachments/assets/49acdbd9-ae6e-4ebe-8cf7-7240f723366d)

고차원 공간에서는 많은 것이 상당히 다르게 작동한다. 
→ 저차원과 달리 고차원 초입방체에 있는 대다수의 점은 경계와 매우 가까이 있다.

→ 고차원은 많은 공간을 가지고 있어, 고차원 데이터셋이 매우 희박할 위험이 있다. (=새로운 샘플도 훈련 샘플과 멀리 떨어져 있을 가능성이 높다.) 
※ 예측을 위해 외삽을 해야 하기에 저차원일 때보다 예측이 더 불안정하며, 훈련 세트의 차원이 클수록 과대적합 위험이 커진다.

## 8.2 차원 축소를 위한 접근법
차원을 감소시키는 두 가지 주요한 접근법인 투영과 매니폴드 학습을 살펴보자.
### 8.2.1 투영
차원 축소에 중점을 맞춰둔 상태로 이해해보자. 대부분의 실전 문제에서는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져 있지 않다. 그러나 많은 특성은 MNIST 데이터셋의 숫자 그림처럼, 모든 훈련 샘플이 고차원 공간 안의 저차원 **부분 공간**에 놓여 있다. 

![image](https://github.com/user-attachments/assets/80021984-ee10-41e3-8243-527aa8321602)

위 그림과 같이 작은 공으로 표현된 3차원 데이터셋이 있다. 
☞ 모든 훈련 샘플이 거의 평면 형태로 존재 (고차원 공간에 있는 저차원 부분 공간)
☞ 모든 훈련 샘플을 이 부분 공간에 수직으로 투영 시 아래와 같은 2D 데이터셋 얻음

![image](https://github.com/user-attachments/assets/52304ce6-aa45-4fef-b796-4e42ce335d44)

차원 축소에 있어 투영이 언제나 최선의 방법은 아니다. 많은 경우 아래의 그림에 표현된 **스위스 롤** 데이터셋처럼 부분 공간이 뒤틀리거나 휘어 있기도 하다.

![image](https://github.com/user-attachments/assets/dddbda38-db7d-44b4-9c67-13ce418f5fce)

이를 그냥 평면에 투영시키면 왼쪽처럼 겹쳐서 나오지만, 우리가 원하는 것은 펼쳐진 오른쪽 2D 데이터셋이다.

![image](https://github.com/user-attachments/assets/5ea8e35b-1131-42fb-bda7-24ff5bee8efe)

### 8.2.2 매니폴드 학습

매니폴드란? : 어떤 공간이 d차원 매니폴드라는 것은, 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부 (d<n)
많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 **매니폴드**를 모델링하는 식으로 작동 : 매니폴드 학습

자유도 : **데이터를 만들 때 가질 수 있느 ㄴ가능한 변형의 수**
(ex : 28*28 픽셀의 흑백 이미지라면, 총 784개의 픽셀 조정 가능 & 각각의 픽셀은 0~255까지의 값을 가질 수 있어 **이론적으로 매우 많은 이미지 생성 가능**, 그러나 관심 있는 것은 숫자 이미지. 즉 특정한 패턴을 가진 이미지이므로 변형의 자유도가 낮아진다.)
데이터셋이 저차원 매니폴드로 압축될 수 있다는 의미 : 숫자들은 784차원 공간에 저차원 구조(매니폴드) 위에 놓여 있다.
즉, MNIST 데이터는 **784차원 공간 전체를 차지하는 게 아닌, 훨씬 작은 차원의 매니폴드에 놓여 있다.**
이를 활용하여 훨씬 적은 차원의 공간에서 효과적으로 표현할 수 있다.

매니폴드 가정이 저차원 매니폴드 공간에 표현되면 항상 더 간단해질까? 가정이 유효할까? 항상 그렇지 않다.

![image](https://github.com/user-attachments/assets/9fe57211-5982-4afb-b1de-7e49d50160e1)

1행의 그림은 결정 경계가 2D 공간에서 단순한 직선으로 표현되나, 2행의 그림은 3D 공간에서 단순한 수직 평면의 결정 경계가 펼쳐진 매니폴드에서는 더 복잡해졌다. 
즉, **전적으로 데이터셋에 달려있다.**

## 8.3 주성분 분석
가장 인기 있는 차원 축소 알고리즘인 **주성분 분석**에 대하여 알아보자. 먼저, 데이터에 가장 가까운 초평면을 정의한 후, 데이터를 이 평면에 투영시킨다.
### 8.3.1 분산 보존
먼저 올바른 초평면을 선택해야 한다. 예를 들어보자.

![image](https://github.com/user-attachments/assets/f010b978-6ccb-42d1-96cd-dc7cd305dd2d)

☞ 왼쪽 : 간단한 2D 데이터셋이 세 개의 축과 함께 표현됨
☞ 오른쪽 : 각 축이 투영된 결과

분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되므로 합리적이다. 
(=원본 데이터셋과 투영된 것 사이의 **평균 제곱 거리 최소화**)
### 8.3.2 주성분
PCA는 훈련 세트에서 분산이 최대인 축을 찾는다. (**데이터의 정보 최대한 유지**) 또한, 첫 번째 축에 직교하고 남은 분산을 최대한 보존하는 두 번째 축을 찾는다. (위 그림에서는 선택의 여지가 없어 점선이 됨)
※ 고차원 데이터셋은 PCA가 이전의 두 축에 직교하는 세 번째 축을 찾으며 데이터셋에 있는 차원의 수만큼 4,5,...n번째 축을 찾는다.

i번째 주성분 : 데이터의 i번째 축 (PC)
그림을 살펴보자.
첫 번째 PC : 벡터 C1 축
두 번째 PC : 벡터 C2 축
다시 돌아와서,

![image](https://github.com/user-attachments/assets/24592241-bd08-4c46-b231-c0324ce253af)

이 그림에서는 처음 두 개의 PC가 투영 평면에 있으며, 세 번째 PC가 이 평면에 수직인 축이다.
투영된 후, 

![image](https://github.com/user-attachments/assets/2be0d18b-dcd5-4f4c-b3c5-58d6f3cab4d7)

첫 번째 PC : Z1
두 번째 PC : Z2

훈련 세트의 주성분은 **특잇값 분해(SVD)** 라는 표준 행렬 분해 기술로 찾는다. 

아래의 파이썬 코드는 넘파이의 svd() 함수를 사용해 아래 그림의 3D 훈련 세트의 모든 주성분을 구한 후 처음 두 개의 PC를 정의하는 두 개의 단위 벡터를 추출한다.
```python
import numpy as np

# 3D 데이터 생성 (2D 평면에 가깝게)
np.random.seed(42)
n_samples = 100

# x1, x2는 [-1, 1] 범위에서 균등 분포를 따르는 점들
x1 = np.random.uniform(-1, 1, n_samples)
x2 = np.random.uniform(-1, 1, n_samples)

# x3는 특정 평면 (예: x3 ≈ 0) 주변에 노이즈를 추가하여 생성
x3 = 0.05 * np.random.randn(n_samples)  # 작은 노이즈 추가

# 데이터셋을 (n_samples, 3) 형태의 배열로 변환
X = np.vstack((x1, x2, x3)).T

# 중앙 정렬 (평균을 0으로 맞춤)
X_centered = X - X.mean(axis=0)

# SVD를 이용한 주성분 분석 (PCA)
U, s, Vt = np.linalg.svd(X_centered)

# 첫 번째와 두 번째 주성분 벡터 추출
c1 = Vt[0]  # 첫 번째 PC
c2 = Vt[1]  # 두 번째 PC
```

### 8.3.3 d차원으로 투영하기
주성분을 모두 추출했다면, 처음 d개의 주성분으로 정의한 초평면에 투영하여 데이터셋의 차원을 d차원으로 축소시킬 수 있다. 
예를 들어,

![image](https://github.com/user-attachments/assets/10ba14ba-6aab-4a0f-a644-ebf3ab3a96a0)

3D 데이터셋은 데이터셋의 분산이 가장 큰 두 개의 주성분으로 구성된 2D 평면에 투영되었다. 이 2D 투영은 원본 3D 데이터셋과 매우 비슷해보인다.

초평면에 훈련 세트를 투영하고 d차원으로 축소된 데이터셋 Xd-proj를 얻기 위해서는 아래 식과 같이 행렬 X와 V의 첫 d열로 구성된 행렬 Wd를 행렬 곱셈하면 된다.

![image](https://github.com/user-attachments/assets/681b906a-12fa-41fd-b8b4-d826fed702a4)

아래의 파이썬 코드는 첫 두 개의 주성분으로 정의된 평면에 훈련 세트를 투영한다.
```python
W2 = Vt[:2].T
X2D = X_centered @ W2
```

PCA 변환이 되었다. 지금까지 분산을 가능한 한 최대로 유지하면서 어떻게 데이터셋의 차원을 특정 차원을 ㅗ축소하는지 보았다.

### 8.3.4 사이킷런 이용하기
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X2D = pca.fit_transform(X)
```
사이킷런의 PCA 모델은 자동으로 데이터를 중앙에 맞춰준다. 이는 PCA 모델을 사용해 데이터셋의 차원을 2로 줄이는 코드이다.
PCA 변환기를 데이터셋에 학습시키고 나면 components_ 속성에 Wd의 전치가 담겨 있다. 이 배열의 행은 처음 d개의 주성분에 해당한다.

### 8.3.5 설명된 분산의 비율
explined_variance_ratio_ 변수에 저장된 주성분의 **설명된 분산의 비율**도 유용한 정보다. 이는 각 주성분의 축을 따라 있는 데이터셋의 분산 비율을 나타낸다. 
```python
pca.explained_variance_ratio_
```
### 8.3.6 적절한 차원 수 선택
축소할 차원 수를 임의로 정하기보다는 충분한 분산이 될 때까지 더해야 할 차원 수를 선택하는 것이 좋다.
PCA 차원 축소 목표 : 데이터의 중요한 정보를 최대한 유지하면서 불필요한 차원 제거 (**최소한의 차원으로 최대한의 분산 유지**)

(주성분 벡터 개수 = 최종적으로 선택한 차원의 수)

MNIST 데이터셋을 로드하고 분할한 후, 차원을 줄이지 않고 PCA를 수행해보자. 그 후 훈련 집합의 분산 95%를 보존하는 데 필요한 최소 차원 수를 계산하자.

```python
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', as_frame=False)
X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]
X_test, y_test = mnist.data[60_000:],mnist.target[60_000:]

pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1
```
그 후, n_components = d로 설정하여 PCA를 다시 실행한다. 보존하려는 분산의 비율을 n_components에서 0~1 사이로 설정하는 게 좋다.

```python
pca = PCA (n_components = 0.95)
X_reduced = pca.fit_transform (X_train)
```
실제 주성분 개수는 훈련 중 결정되며, n_components_ 속성에 저장된다.
```python
pca.n_components_
```
다른 방법은 설명된 분산을 차원 수에 대한 함수로 그리는 것이다. 

![image](https://github.com/user-attachments/assets/46ff1141-4293-4ef9-a9a9-65173d9b2c1b)

이 그래프에서는 설명된 분산의 빠른 성장이 멈추는 변곡점이 있다. 여기서는 차원을 약 100으로 축소해도 설명된 분산 손해를 보지 않을 것이다.

지도 학습 작업 (분류)의 전처리 단계로 차원 축소를 사용하는 경우, 다른 하이퍼파라미터와 마찬가지로 차원 수를 튜닝할 수 있다. 

아래의 코드에서는 두 단계로 구성된 파이프라인을 생성한다. 먼저 PCA를 사용하여 차원을 줄인 후, 랜덤 포레스트를 사용하여 분류를 수행한다. 그 후 RandomizedSearchCV를 사용해 PCA와 랜덤 포레스트 분류기에 잘 맞는 하이퍼파라미터 조합을 찾는다. 

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.pipeline import make_pipeline

clf = make_pipeline (PCA(random_state=42), RandomForestClassifier(random_state=42)) 
param_distrib = {
    "pca__n_components": np.arange(10,80),
    "randomforestclassifier__n_estimators": np.arange(10,100)
}
rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3, random_state=42)
rnd_search.fit(X_train[:1000], y_train[:1000])
```
앞에서 찾은 최상의 하이퍼파라미터를 살펴보자.
```python 
print (rnd_search.best_params_)
```
### 8.3.7 압축을 위한 PCA
차원 축소 후 훈련 세트는 훨씬 적은 공간을 차지한다. 즉, 상당한 압축률을 보여주며 이런 크기 축소는 분류 알고리즘의 속도를 크게 높일 수 있다. 또한 압축된 데이터셋에 PCA 투영의 변환을 반대로 적용하여 원래의 차원으로 되돌릴 수도 있다. 투영 중 일정량의 정보를 잃어버렸다면 원본 데이터셋을 얻을 수는 없을 것이지만, 원본 데이터와 매우 비슷할 것이다.
원본 데이터와 재구성된 데이터 사이의 평균 제곱 거리를 **재구성 오차**라고 한다. 
inverse_transform() 메소드를 사용하여 복원 가능하다. 
```python
X_recovered = pca.inverse_transform(X_reduced)
```
### 8.3.8 랜덤 PCA
svd_solver 매개변수를 randomized로 지정하면 사이킷런은 랜덤 PCA라 부르는 확률적 알고리즘을 사용하여 처음 d개의 주성분에 대한 근삿값을 빠르게 찾는다. 이는 d가 n보다 많이 작을 때 완전 SVD보다 계산이 훨씬 빠르다.
```python
rnd_pca = PCA (n_components = 154, svd_solver = 'randomized',random_state = 42)
X_reduced = rnd_pca.fit_transform (X_train)
```
svd_solver의 기본값은 auto이다. max(m,n)>500이고 n_components가 min(m,n)의 80%보다 작은 정수면 사이킷런은 자동으로 PCA 알고리즘을 사용한다. 그렇지 않으면 완전한 SVD 방식을 사용한다. 앞의 코드는 154 < 0.8 * 784이므로 매개변수 없이도 random pca 알고리즘을 사용한다. 완전한 SVD를 이용하려면 매개변수를 full로 설정하면 된다.

### 8.3.9 점진적 PCA
SVD 알고리즘을 실행하기 위해 전체 훈련 세트를 메모리에 올려야 하는 문제를 해결하기 위해 **점진적 PCA** 알고리즘이 개발되었다. 이는 훈련세트를 미니배치로 나눈 뒤 점진적 PCA 알고리즘에 한 번에 하나씩 주입한다. 이런 방식은 훈련세트가 클 때 유용하고 온라인으로 PCA를 적용할 수 있다.

아래의 코드느 MNIST 훈련 세트를 100개의 미니배치로 나누고 사이킷런의 IncrementalPCA 파이썬 클래스에 주입하여 데이터셋을 이전과 같이 154개로 줄인다. 미니배치마다 partial_fit()메소드를 호출해야 한다.

```python
from sklearn.decomposition import IncrementalPCA

n_batches = 100
inc_pca = IncrementalPCA(n_components=154)
for X_batch in np.array_split(X_train, n_batches):
    inc_pca.partial_fit(X_batch)  

X_reduced = inc_pca.transform(X_train)
```
또는 디스크의 이진파일에 저장된 대규모 배열을 마치 메모리에 있는 것처럼 조작할 수 있는 넘파이 memmap 클래스를 사용할 수 있다. 이 클래스는 필요할 때 원하는 데이터만 메모리에 로드한다. 아래의 코드를 보자. 먼저 memmap을 생성하고 훈련 세트를 복사한 후 flush()를 호출하여 캐시에 남아 있는 모든 데이터가 디스크에 저장되도록 해보겠다.
```python
filename = 'my_mnist.mmap'
X_mmap = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)
X_mmap[:] = X_train
X_mmap.flush()
```
다음으로 memmap 파일을 로드하고 일반적인 넘파이 배열처럼 사용할 수 있다. IncrementalPCA로 차원을 줄여보자. 이는 특정 순간에 배열의 작은 부분만 사용하기에 메모리 부족 문제가 안일어난다. 따라서 일반적인 fit() 메소드를 호출할 수 있어 편리하다.
```python
X_mmap = np.memmap (filename, dtype = 'float32', mode = 'readonly').reshape(-1,784)
batch_size = X_mmap.shape[0] // n_batches
inc_pca = IncrementalPCA (n_components = 154, batch_size = batch_size)
inc_pca.fit(X_mmap)
```

## 8.4 랜덤 투영
랜덤 투영 알고리즘은 랜덤한 선형 투영을 사용하여 데이터를 저차원 공간에 투영한다. 랜덤한 투영은 실제로 거리를 상당히 잘 보존할 가능성이 매우 높다.
매우 고차원인 데이터셋의 경우 PCA가 느려질 수 있으므로 랜덤 투영을 사용하는 것을 고려해볼 수 있다.

최적의 차원 수는 어떻게 선택할 수 있을까?
거리가 주어진 허용 오차 이상으로 변하지 않도록 보장하기 위해 보존할 최소 차원 수를 결정하는 방향으로 생각해야 한다.
이러한 방정식은 johnson_lindenstrauss_min_dim() 함수에 구현되어 있다.

```python
from sklearn.random_projection import johnson_lindenstrauss_min_dim
m, e = 5_000, 0.1
d = johnson_lindenstrauss_min_dim(m, eps=e)
d
```
이제 각 항목을 평균 0, 분산 1/d 가우스 분포에서 랜덤 샘플링한 [d,n]크기의 랜덤 행렬 P를 생성, 이를 사용하여 데이터셋을 n에서 d차원으로 투영할 수 있다.
```python
n = 20_000
np.random.seed(42)
P = np.random.randn (d,n) / np.sqrt(d) # 표준 편차 = 분산의 제곱근

X = np.random.randn (m,n)
X_reduced = X @ P.T
```
알고리즘이 랜덤한 행렬을 생성하는 데 필요한 것을 데이터셋의 크기 뿐이므로 간단, 효율적이며 훈련이 불필요하다. 데이터는 사용되지 않는다.

사이킷런은 방금과 동일한 작업을 수행할 수 있는 GaussianRandomProjection 클래스를 제공한다. 이 클래스의 fit() 메소드를 호출하면 johnson_lindenstrauss_min_dim() 을 사용해 출력 차원을 결정한 후, 랜덤한 행렬을 생성하여 components_ 속성에 저장. 그 후 transform()을 호출 시 이 행렬을 사용해 투영을 수행한다. 
코드를 살펴보자.
```python
from sklearn.random_projection import GaussianRandomProjection

gaussian_rnd_proj = GaussianRandomProjection(eps=e, random_state=42)
X_reduced = gaussian_rnd_proj.fit_transform(X)
```
사이킷런은 SparseRandomProjection이라는 두 번째 랜덤 투영 변환기도 제공한다. 이는 동일한 방식으로 타깃 차원을 결정, 동일한 크기의 랜덤 행렬 생성, 그리고 투영을 동일하게 수행한다. 차이점은 랜덤 행렬의 희소성이다. 메모리가 적게 사용된다는 뜻이다. 랜덤 행렬을 생성하고 차원을 줄이는 속도도 빠르며, 입력이 희소할 경우 희소성을 유지하는 특징도 있다. 마지막으로 이전 접근 방식과 동일한 거리 보존 속성을 가지며 차원 축소 품질도 비슷하다. 규모가 크거나 희박한 데이터셋이면 2번째 변환기를 사용할 것.

희소한 랜덤 행렬에서 0 아닌 항목의 비율 r을 밀도라고 한다. 기본적으로 밀도는 1/root n이지만, 원하는 경우 density 매개변수를 다른 값으로 설정할 수 있다. 희소 랜덤 행렬의 각 항목은 0이 아닐 확률 r을 가지며, 이는 -v또는 +v이고, 여기서 v= 1/root (dr)이다.

역변환을 수행할 땐 사이파이의 pinv () 함수를 사용해 성분 행렬의 유사역행렬을 계산후, 축소된 데이터에 유사역행렬의 전치를 곱해야 한다.

```python
components_pinv = np.linalg.pinv (gaussian_rnd_proj.components_)
X_recovered = X @ components_pinv.T
```
랜덤 투영은 간단하고, 빠르며, 메모리 효율이 높고 강력한 차원 축소 알고리즘으로, 고차원 데이터셋을 다룰 때 염두에 두자.

## 8.5 지역 선형 임베딩
지역선형 임베딩은 비선형 차원 축소 기술이다. PCA나 랜덤투영과 달리 투영에 의존하지 않는 매니폴드 학습이다. 이는 먼저 각 훈련 샘플이 최근접 이웃에 얼마나 선형적으로 연관되어 있는지 측정 후, 국부적인 관계가 가장 잘 보존되는 훈련세트의 저차원 표현을 찾는다. 이는 특히 잡음이 너무 많지 않은 경우 꼬인 매니폴드를 펼치는 데 좋다.
```python
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import LocallyLinearEmbedding

X_swiss, t = make_swiss_roll(n_samples = 1000, noise = 0.2, random_state = 42)
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)
X_unrolled = lle.fit_transform(X_swiss)
```
이는 스위스롤을 만든 후 사이킷런의 LocallyLinearEmbedding을 사용해 이를 펼친 것.
이에 대한 그림은 아래를 통해 확인할 수 있다. 

![image](https://github.com/user-attachments/assets/371c07a3-9d26-4bf3-9b2c-66cbde76aaca)

스위스 롤이 완전히 펼쳐졌고 지역적으로는 샘플 간 거리가 잘 보존되어 있다. 그러나 크게 보면 샘플 간 거리가 잘 유지되어 있지 않다. 펼쳐진 스위스롤은 이런식으로 늘어나거나 꼬인 밴드가 아닌 직사각형이어야 하지만, LLE는 매니폴드를 모델링하는 데 잘 작동한다.

# 비지도 학습
알고리즘이 레이블이 없는 데이터를 바로 사용하기 위해서는 어떻게 해야할까? 바로 **비지도 학습**이 필요하다. 지난 시간에 가장 널리 사용되는 비지도 학습 방법인 차원 축소를 살펴보았다. 이제 몇 가지 비지도 학습 작업을 추가로 알아보자.

군집 : 비슷한 샘플을 **클러스터**로 모음. 

이상치 탐지 : '정상' 데이터가 어떻게 보이는지 학습 후 비정상 샘플 감지에 사용 (정상 샘플 = 정상치, 이상 샘플 = 이상치)

밀도 추정 : 데이터셋 생성 확률 과정의 **확률 밀도 함수**를 추정. 이는 이상치 탐지에 널리 사용됨. 밀도가 매우 낮은 영역에 놓인 샘플이 이상치일 가능성 ↑

## 9.1 군집
**군집**이란 비슷한 샘플을 구별해 하나의 **클러스터** 또는 비슷한 샘플의 그룹으로 할당하는 작업이다. 분류와 마찬가지로 각 샘플은 하나의 그룹에 할당되지만, 군집은 비지도 학습이다. 그림을 살펴보자.

![image](https://github.com/user-attachments/assets/bb747309-1621-4beb-aade-a7b3beaf9fd5)

왼쪽 그림은 레이블로 잘 분류되어 있다. (로지스틱 회귀, SVM, 랜덤 포레스트 등의 분류기 같은 분류 알고리즘에 적합)
그러나 오른쪽 그림은 레이블이 없다. 군집 알고리즘이 필요한 경우다. 모든 특성을 사용하면 군집 알고리즘이 클러스터 세 개를 매우 잘 구분할 수 있다.

군집은 다양한 애플리케이션에서 사용된다.
고객분류 - 추천시스템
데이터분석 - 새로운 데이터셋 분석 시 각 클러스터 각각 분석
차원 축소 기법 - 친화성 (샘플이 클러스터에 얼마나 잘 맞는지)
특성 공학 - 클러스터 친화성으로 더 나은 성능 획득 가능
이상치 탐지 - 친화성 낮은 샘플을 이상치로 간주할 가능성
준지도 학습 - 레이블 증가시켜 성능 향상
검색 엔진 - 제시된 이미지와 유사한 이미지 제공
이미지 분할 - 색 기반 픽셀을 클러스터로 모은 후 색을 평균색으로 바꿔 윤곽 감지 용이

유명한 군집 알고리즘인 k-평균과 DBSCAN을 살펴보자. 그 후, 비선형 차원 축소, 준지도 학습, 이상치 탐지와 같은 애플리케이션을 알아보자.

### 9.1.1 k-평균

![image](https://github.com/user-attachments/assets/d7a4c098-8d0f-467f-828b-9e385dad6656)

레이블이 없는 데이터셋을 보자. 샘플 덩어리 5개가 잘 보인다. k-ㅠㅕㅇ균은 반복 몇 번으로 이런 종류의 데이터셋을 빠르고 효율적으로 클러스터로 묶을 수 있는 간단한 알고리즘이다. 
이 데이터셋에 k-평균 알고리즘을 훈련해보자. 이 알고리즘은 각 클러스터의 중심을 찾고 가장 가까운 클러스터에 샘플을 할당한다.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# 데이터 생성: 5개의 클러스터로 구성된 데이터셋
X, y = make_blobs(n_samples=1500, centers=[(-3, 3), (-3, 1.5), (-2, 2), (0, 2), (1, 2)],
                  cluster_std=[0.2, 0.2, 0.5, 0.7, 0.7], random_state=42)
k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)
# 데이터 시각화
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], s=10, color='navy')
plt.xlabel("$X_1$")
plt.ylabel("$X_2$")
plt.title("샘플 덩어리 다섯 개로 이루어진 레이블 없는 데이터셋")
plt.grid(True)
plt.show()
```
알고리즘이 찾을 클러스터 개수 k를 지정한다. 여기서는 데이터를 보고 k를 5로 지정해야 한다고 알 수 있으나, 일반적으로 어렵다. 
각 샘플은 다섯 개의 클러스터 중 하나에 할당된다. 군집에서 각 샘플의 **레이블**은 알고리즘이 샘플에 할당한 클러스터의 인덱스다. KMeans 클래스의 인스턴스는 labels_ 인스턴스 변수에 훈련된 샘플의 예측 레이블을 가지고 있다.
```python
print (y_pred)
print (y_pred is kmeans.labels_)
```
이 알고리즘이 찾은 센트로이드 다섯 개도 확인해보자.
```python
kmeans.cluster_centers_
```
새로운 샘플에 가장 가까운 센트로이드의 클러스터를 할당할 수 있다.
```python
import numpy as np
X_new = np.array([[0,2],[3,2],[-3,3],[-3,2.5]])
kmeans.predict(X_new)
```
클러스터의 결정 경계를 그려보면 보로노이 다이어그램을 얻을 수 있다.

![image](https://github.com/user-attachments/assets/9c8a6dec-d846-489d-898f-c34faaa23665)

**샘플**은 대부분 적절한 클러스터에 잘 할당되었다. 하지만 샘플 몇 개는 레이블이 잘못 부여되었다. (왼쪽 위 클러스터와 가운데 클러스터의 경계 부근)
실제 k-평균 알고리즘은 클러스터의 크기가 많이 다르면 잘 작동하지 않는다. 샘플을 클러스터에 할당할 때 센트로이드까지 거리를 고려하는 것이 전부이기 때문이다.

샘플을 하나의 클러스터에 할당하는 **하드 군집**보다 클러스터마다 샘플에 점수를 부여하는 **소프트 군집**이 유용할 수 있다. 점수는 샘플과 센트로이드 사이의 거리 또는 유사도 점수 (친화성 점수)가 될 수 있다. KMeans 클래스의 transfrom() 메소드는 샘플과 각 센트로이드 사이의 거리를 반환한다.
```python
kmeans.transform(X_new).round(2)
```
이 결과에서는 하나의 리스트에 5개의 요소가 들어가는데, 각각은 n번째 샘플이 m번째 센트로이드에서 얼마인지를 나타낸다.

**k-평균 알고리즘**
센트로이드가 주어진다고 가정해보자. 데이터셋에 있는 모든 샘플에 가장 가까운 센트로이드의 클러스터를 할당할 수 있다. 반대로 모든 샘플의 레이블이 주어진다면 각 클러스터에 속한 샘플의 평균을 계산하여 모든 센트로이드를 쉽게 구할 수 있다. 
하지만, 레이블이나 센트로이드가 주어지지 않는다면?
처음에는 센트로이드를 랜덤하게 선정한다. 그다음 샘플에 레이블을 할당하고 센트로이드를 업데이트하고, 샘플에 레이블을 할당하고 센트로이드를 업데이트를 하는 식으로 센트로이드에 변화가 없을 때까지 계속한다. 이 알고리즘은 제한된 횟수 안에 수렴하는 것을 보장한다. 샘플과 가장 가까운 센트로이드 사이의 평균 제곱거리는 각 단계마다 내려갈 수만 있고 음수는 없기에 수렴성이 보장된다.

![image](https://github.com/user-attachments/assets/9534bafc-eccd-49f3-98a0-3e830584fef6)

알고리즘의 작동을 보면 위의 그림과 같다. 위 그림은 반복 세 번만에 최적으로 보이는 클러스터에 도달했다.
이 알고리즘의 수렴은 보장되나 지역 최적점으로 수렴하는 등 적절한 솔루션으로 수렴하지 못할 수 있다. 이 여부는 센트로이드 초기화에 달려있다. 아래 그림의 두 가지 예는 랜덤한 초기화 단계에 운이 없을 때 알고리즘이 수렴할 수 있는 최적이 아닌 솔루션을 보여준다.

![image](https://github.com/user-attachments/assets/c64f6e98-2938-419b-88e9-8785be63b6e0)

센트로이드 초기화를 개선하여 이런 위험을 줄일 수 있는 방법을 알아보자.

**센트로이드 초기화 방법**
센트로이드 위치를 근사하게 알 수 있다면 init 매개변수에 센트로이드 리스트를 담은 넘파이 배열을 지정하고 n_init을 1로 설정할 수 있다.
```python
good_init = np.array([[-3,3],[-3,2],[-3,1],[-1,2],[0,2]])
kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)
kmeans.fit(X)
```
또 다른 방법은 랜덤 초기화를 다르게 하여 여러 번 알고리즘을 실행하고 가장 좋은 솔루션을 선택하는 것이다. 랜덤 초기화 횟수는 n_init 매개변수로 조절한다. 기본값은 10이다. 이는 fit() 메소드를 호출할 때 앞ㄷ서 설명한 전체 알고리즘이 10번 실행된다는 뜻이다. 사이킷런은 이 중에 최선의 솔루션을 반환한다. 어떻게? 
사용하는 성능지표가 있다. 이를 모델의 inertia라고 부른다. (각 샘플과 가장 가까운 센트로이드 사이의 제곱거리 합)
KMeans 클래스는 알고리즘을 n_init번 실행하여 이너셔가 가장 낮은 모델을 반환한다.
score() 메소드는 이너셔의 음숫값을 반환한다. 예측기의 score() 메소드는 사이킷런의 '큰 값이 좋은 것이다.'라는 규칙을 따라야하기 때문이다. 

**k-평균 속도 개선과 미니배치 k-평균**
클러스터가 많은 일부 대규모 데이터셋에서 불필요한 거리 계산을 피함으로써 알고리즘의 속도를 높일 수 있는 k-평균 속도 계산을 알아보자. 이를 위해서는 삼각 부등식을 사용한다. (두 점 사이의 직선은 항상 짧은 거리가 됨) 그리고 샘플과 센트로이드 사이의 거리를 위한 하한선과 상한선을 유지한다. 
그러나 이 알고리즘이 항상 훈련 속도를 높일 수 있는 건 아니고, 때로는 훈련 속도가 상당히 느려질 수 있다. 사용해보고 싶다면 algorithm = 'elkan'으로 지정할 것.

k-평균 알고리즘의 또 다른 중요한 변형이 제시되었다. 이 알고리즘은 전체 데이터셋을 사용해 반복하지 않고 각 반복마다 미니배치를 사용해 센트로이드를 조금씩 이동한다. 이는 속도를 높이고, 메모리에 들어가지 않는 대량의 데이터셋에 군집 알고리즘을 적용할 수 있다. 사이킷런의 MiniBatchKMeans 클래스에 이 알고리즘이 구현되어 있다.
```python
from sklearn.cluster import MiniBatchKMeans

minibatch_kmeans = MiniBatchKMeans (n_clusters = 5, random_state = 42) 
minibatch_kmeans.fit(X)
```
데이터셋이 메모리에 들어가지 않는 경우 가장 간단한 방법은 점진적 PCA 방법처럼 memmap클래스를 이용하는 것이다.
또는 MiniBatchKMeans 클래스의 partial_fit() 메소드에 한 번에 하나의 미니배치를 전달할 수 있다. 하지만 초기화를 여러 번 수행하고 만들어진 결과에서 가장 좋은 것을 직접 골라야 해서 할 일이 많다.

미니배치 k-평균 알고리즘이 일반보다는 빨라도 이너셔는 일반적으로 좀 더 나쁘다.

![image](https://github.com/user-attachments/assets/bb44941a-ac25-437e-b8f0-3268b5fc7a64)

**최적의 클러스터 개수 찾기**
지금까지 클러스터 개수 k를 5로 지정했다. 하지만 일반적으로 k를 어떻게 설정할지 쉽게 알 수 없다. 만약 올바르게 지정하지 않으면 결과는 매우 나쁠 수 있다. 아래처럼 말이다.

![image](https://github.com/user-attachments/assets/67ae4826-5cf6-480d-b8d6-9783919baa49)

가장 작은 이너셔를 가진 모델을 선택하면 어떨까? 간단하지 않다. 이너셔는 k가 증가함에 따라 점점 작아지기에 k를 선택할 때 좋은 성능 지표가 아니다. (클러스터가 늘어날수록 각 샘플은 가까운 센트로이드에 더 가깝게 된다. 그래서 이너셔는 더 작아질 것.) 그래프를 그려보면 종종 그래프에 엘보로 보이는 굴곡 지점이 나타난다.

![image](https://github.com/user-attachments/assets/3bee0732-e552-416b-960c-b2dd88999255)

이너셔는 k=4까지 증가할 때 빠르게 줄어든다. 하지만 k가 계속 증가하면 이너셔는 느리게 감소한다. 따라서 k에 대한 정답을 모르면 4는 좋은 선택이 된다. 이보다 작은 값은 변화가 심하고 더 큰 값은 크게 도움이 되지 않는다. 
근데, 최선의 클러스터 개수를 선택하는 이 방법은 너무 엉성하다. 더 정확한 (계산 비용은 많이 들긴 함) 방법은 **실루엣 점수**이다. 이 값은 모든 샘플에 대한 **실루엣 계수**의 평균이다. 이는 (b-a)/max(a,b)로 계산된다. 
a = 동일한 클러스터에 있는 다른 샘플까지의 평균 거리 (클러스터 내부의 평균 거리)
b = 가장 가까운 클러스터까지 평균 거리 (가장 가까운 클러스터의 샘플까지 평균 거리. 샘플과 가장 가까운 클러스터는 자신이 속한 클러스터 제외하고 b가 최소인 클러스터)
실루엣 계수는 -1에서 +1까지 바뀔 수 있다. 
+1에 가까우면 자신의 클러스터 안에 잘 속해 있음 & 다른 클러스터와는 멀리 떨어져 있음
0에 가까우면 클러스터의 경계에 위치
-1에 가까우면 이 샘플이 잘못된 클러스터에 할당됨

실루엣 점수 계산 : 사이킷런의 silhouette_score() 함수 사용
```python
from sklearn.metrics import silhouette_score
silhouette_score (X,kmeans.labels_)
```
클러스터 개수를 달리해서 점수를 비교해볼까?

![image](https://github.com/user-attachments/assets/ae063c9e-7c3b-4e9e-86d4-f677402606eb)

이 그래프는 전보다 훨씬 많은 정보를 준다. k=4도 좋지만 k=5도 꽤 괜찮다. k=6,7보다 훨씬 좋다. 이는 이너셔를 비교했을 때는 드러나지 않았다.
모든 샘플의 실루엣 계수를 할당된 클러스터와 계수 값으로 정렬하여 그리면 더 많은 정보가 있는 그래프를 얻을 수 있다.

![image](https://github.com/user-attachments/assets/9c816deb-2dde-4f42-ab42-35866d582754)

이를 **실루엣 다이어그램**이라고 한다. 

높이 :클러스터가 포함하는 샘플 수
너비 : 클러스터에 포함된 샘플의 정렬된 실루엣 계수 (넓을수록 굿)
수직 파선 : 실루엣 점수
한 클러스터의 샘플 대부분이 이 점수보다 낮은 계수를 가지면 크러스터의 샘플이 다른 클러스터랑 너무 가깝다는 걸 의미하므로 나쁜 클러스터다. k=4일때 인덱스 1의 클러스터가 매우 큰 반면, 5일 때는 모든 클러스터의 크기가 비슷하다. 비슷한 크기의 클러스터를 얻을 수 있는 k를 선택하는 게 좋다.

**k-평균의 한계**
k-평균이 완벽한가? 최적이 아닌 솔루션을 피하려면 알고리즘을 여러 번 실행해야 하며, 클러스터 개수를 지정해야 하므로 번거로운 작업일 수 있다. 또한, k-평균은 클러스터의 크기 또는 밀집도가 서로 다르거나 원형이 아니면 잘 작동 안한다. 

![image](https://github.com/user-attachments/assets/82e26b2f-a43c-4782-a60d-384270e91222)

좋지 않다.이런 타원형 클러스터는 가우스 혼합 모델이 잘 작동한다. (그래서 k-평균 실행 전에 스케일을 맞추는 게 중요)
군집에서 도움을 받을 수 있는 몇 가지 방법을 살펴보자.

### 9.1.3 군집을 사용한 이미지 분할
**이미지 분할**은 이미지를 여러 개의 **세그먼트**로 분할하는 작업이다. 여러 가지 변형을 살펴보자.
색상 분할 : 동일한 색상을 가진 픽셀을 같은 세그먼트에 할당
시맨틱 분할 : 동일한 종류의 물체에 속한 모든 픽셀을 같은 세그먼트에 할당
인스턴스 분할 : 개별 객체에 속한 모든 픽셀을 같은 세그먼트에 할당

k-평균을 사용하는 간단한 색상 분할에 중점을 두자.

Pillow 패키지 (파이썬 이미지 처리 라이브러리) import 후, 이를 사요해 ladybug.png를 로드하자. 경로는 내 파일로.
```python
import PIL
import numpy as np
image = np.asarray(PIL.Image.open('/content/sample_data/ladybug.jpg'))
image.shape
```
3D 배열로 표시된다. 첫 번째 차원 크기는 높이, 두 번째 차원 크기는 너비, 세 번째 차원은 색상 채널의 수다. 이 경우 RGB이다. 다른 말로 하면 각 픽셀에 대해 0~255 사이의 부호 없는 8비트 정수로 RGB의 강도를 담고 있는 3D 벡터가 있다. 
일부 이미지 (ex : 흑백) - 채널 수 少
일부 이미지 (ex : 투명도 위해 알파 채널 추가 이미지 or 추가적 전자기파 채널 포함 위성 이미지) - 채널 수 多

다음 코드는 배열의 크기를 바꾸어 긴 RGD 색상 리스트로 만든 후 k-평균을 사용하여 8개의 클러스터로 모은다. 
각 픽셀에 대해 가장 가까운 클러스터 중심을 포함한 segmented_img 배열 생성 후, 마지막으로 원래 이미지 크기로 바꾼다.
```python
X = image.reshape (-1,3) # 1차원 배열로 변환 (height*width, 3) 형태의 RGB가 되어, 각 행이 하나의 픽셀을 나타내는 형태
kmeans = KMeans (n_cluster = 8, random_state=42).fit(X) # 8개의 대표적인 색상 그룹 (클러스터) 찾고, 랜덤 초기값 고정으로 실행 시 동일 결과 나오며, K-Means를 학습시킴 
segmented_img = kmeans.cluster_centers_[kmeans.labels_] # 각 픽셀이 속한 클러스터 레이블 0~7을 나타내고, K-Means가 찾은 8개의 대표적인 색상 값 (RGB 평균값)을 저장한다. 기존 이미지는 대표 색상으로 변환됨. 비슷한 놈들은 같은 색으로 변환.
segmented_img = segmented_img.reshape(image.shape) # 원래의 이미지 형태 (h,w,3)으로 재변환
```

위 코드는 8개의 대표 색상만 이용하여 단순화하는 작업이다. 
k-평균이 비슷한 크기의 클러스터를 만드는 경향이 있어, 클러스터 개수가 8보다 작으면 무당벌레의 색이 독자적인 클러스터를 만들지 못하고 주위에 합쳐진다.

![image](https://github.com/user-attachments/assets/a1cbb457-bc13-4d5f-951b-83eaee28e062)

### 9.1.4 군집을 사용한 준지도 학습
군집을 사용하는 또 다른 사례인 준지도 학습을 알아보자. 레이블이 없는 데이터가 많고 레이블이 있는 데이터가 적을 때 사용한다. 여기서는 숫자 데이터셋을 사용하겠다. 데이터셋부터 로드하고 분할하자.

```python
from sklearn.datasets import load_digits

X_digits, y_digits = load_digits(return_X_y = True)
X_train, y_train = X_digits[:1400], y_digits[:1400]
X_test, y_test = X_digits[1400:], y_digits[1400:]
```
50개의 샘플에 대한 레이블만 있다고 가정하자. 기준 성능을 얻기 위해 레이블이 존재하는 50개 샘플에서 로지스틱 회귀 모델부터 훈련하자.
```python
from sklearn.linear_model import LogisticRegression

n_labeled = 50
log_reg = LogisticRegression(max_iter = 10_000) #최대 반복 횟수 10000
log_reg.fit(X_train[:n_labeled], y_train[:n_labeled]) #50개의 샘플만 학습에 사용
log_reg.score(X_test, y_test)
```
**★ 추가 질문 : 이 코드에서 선택된 50개의 샘플이 라벨이 있는 데이터인지 어떻게 보장되는가?**

우리가 y_train[:n_labeled]를 사용할 때 이미 라벨이 포함된 데이터라고 가정하였다. 그래서 보장 된다고 할 수 있다. 하지만 현실에서는 X_train에 라벨이 없는 데이터도 포함될 가능성이 있다. 그러면 우리가 선택한 50개의 데이터 중 일부가 라벨이 없는 샘플일 수도 있다.

결론적으로 라벨이 없는 데이터가 포함될 가능성이 있다면 이를 필터링하는 과정이 필요하다.

ㅇㅋ. 위에서 score이 75%가 나왔다. 정확도가 이정도밖에 안된다는 뜻이다. 실제로 전체 훈련 세트로 모델을 훈련하면 약 90.7%의 정확도에 도달한다. 이걸 어떻게 개선할까?

먼저 훈련 세트를 50개의 클러스터로 모은 후, 각 클러스터에서 센트로이드에 가장 가까운 이미지를 찾는다. 이를 **대표 이미지**라고 부른다.

```python
k = 50
kmeans = KMeans (n_clusters = k, random_state = 42)
X_digits_dist = kmeans.fit_transform(X_train)
representative_digit_idx = np.argmin(X_digits_dist, axis = 0)
X_representative_digits = X_train[representative_digit_idx]
```

![image](https://github.com/user-attachments/assets/724b2983-fee4-4808-8a3c-1ad99811d9bf)

위 이미지가 대표 이미지 50개이다.
수동으로 레이블을 할당해볼까?

```python
y_representative_digits = np.array([1,3,6,0,7,9,2,...,5,1,9,9,3,7])
```
이제 레이블된 50개의 샘플로 이루어진 데이터셋이 준비되었다. 이 이미지들은 랜덤으로 고른 샘플이 아니며 각 클러스터를 대표하는 이미지다. 성능이 조금이라도 높은지 확인해보자.
```python
log_reg = LogisticRegression(max_iter = 10_000)
log_reg.fit(X_representative_digits, y_representative_digits)

log_reg.score(X_test, y_test)
```
뭐... 좀 오르긴 올랐다. 샘플에 레이블을 부여하는 건 비용이 많이 들고 어렵다. 따라서 랜덤 샘플 대신 대표 샘플에 레이블을 할당하는 것이 좋은 방법이다.

이 레이블을 동일한 클러스터에 있는 모든 샘플로 전파하면 어떨까? 이를 **레이블 전파**라고 부른다.

```python
y_train_propagated = np.empty(len(X_train),dtype = np.int64)
for i in range (k) :
  y_train_propagated[kmeans.labels_ == i] = y_representative_digits[i]
```
다시 훈련해보자.
```python
log_reg = LogisticRegression(max_iter = 10_000)
log_reg.fit(X_train, y_train_propagated)
log_reg.score(X_test, y_test)
```
정확도가 점점 오르고 있다!!
클러스터 중심에서 가장 먼 1%의 샘플을 무시하면 더 나은 결과를 얻을까? (일부 이상치 제거될 것)
아래의 코드는 먼저 각 샘플에서 가장 가까운 클러스터 중심까지의 거리를 계산 후 각 클러스터에 대해 가장 큰 1%의 거리를 -1로 설정한다. 마지막으로 -1 거리로 표싣괸 샘플이 없는 데이터셋을 생성한다.

```python
percentile_closest = 99

X_cluster_dist = X_digits_dist[np.arange(len(X_train)), kmeans.labels_]
for i in range (k) :
  in_cluster = (kmeans.labels_ == i)
  cluster_dist = X_cluster_dist[in_cluster]  
  cutoff_distance = np.percentile(cluster_dist, percentile_closest)
  above_cutoff = (X_cluster_dist > cutoff_distance)
  X_cluster_dist[in_cluster & above_cutoff] = -1   

partially_propagated = (X_cluster_dist != -1)
X_train_partially_propagated = X_train[partially_propagated]
y_train_partially_propagated = y_train_propagated[partially_propagated]
```
이제 부분적으로 전파한 데이터셋에서 모델을 재훈련하고 어떤 정확도를 얻을 수 있는지 확인해보자.
```python
log_reg = LogisticRegression(max_iter = 10_000)
log_reg.fit(X_train_partially_propagated, y_train_partially_propagated)
log_reg.score(X_test, y_test)
```
다음으로 레이블의 정확도를 살펴보자.
```python
(y_train_partially_propagated == y_train[partially_propagated]).mean()
```
모델과 훈련 세트를 지속적으로 향상시키기 위해 다음 단계로 **능동 학습**을 몇 번 반복할 수 있다. 이 방법은 전문가가 학습 알고리즘과 상호작용하여 알고리즘이 요청할 때 특정 샘플의 레이블을 제공한다. 가장 널리 사용되는 건 **불확실성 샘플링**이다. 
1. 지금까지 수집한 레이블된 샘플에서 모델 훈련 후 이를 이용해 레이블되지 않은 모든 샘플에 대한 예측을 만듦
2. 추정 확률이 낮은 샘플을 전문가에게 보내 레이블을 붙임
3. 성능 향상 안될때까지 반복

### 9.1.5 DBSCAN
DBSCAN 알고리즘은 밀집된 연속적 지역을 클러스터로 정의한다. 
1. 알고리즘이 각 샘플에서 작은 거리인 epsilon 내에 샘플이 몇 개 놓여 있는지 셈 (**샘플의 epsilon-이웃**)
2. 자기 자신 포함 이 이웃 내에 적어도 min_samples개 샘플이 있다면 이를 **핵심 샘플**로 간주 (밀집된 지역에 있는 샘플)
3. 이 이웃에 있는 모든 샘플은 동일 클러스터에 속함. 이웃에는 다른 핵심 샘플이 포함될 수 있음. 따라서 핵심 샘플의 이웃의 이웃은 계속해서 하나의 클러스터를 형성.

이 알고리즘은 모든 클러스터가 밀집되지 않은 지역과 잘 구분될 때 좋은 성능을 낸다. 
```python
from sklearn.cluster import DBSCAN
from sklearn.datasets import make_moons

X, y = make_moons(n_samples = 1000, noise = 0.05)
dbscan = DBSCAN (eps = 0.05, min_samples = 5)
dbscan.fit(X)
```
모든 샘플의 레이블은 인스턴스 변수 labels_에 저장되어 있다.
일부 샘플의 클러스터 인텍스는 -1이다. 이는 알고리즘이 이 샘플을 이상치로 판단했다는 의미이다. 핵심 샘플의 인덱스는 인스턴스 변수 core_sample_indices_ 에서 확인할 수 있다. 핵심 샘플 자체는 인스턴스 변수 components_에 저장되어 있다.

군집 결과는 왼쪽 그래프에서 확인하면 된다 클러스터를 7개 만들었고 많은 샘플을 이상치로 판단했다.

![image](https://github.com/user-attachments/assets/aa57cfe7-1003-4235-86e8-e34cce34eb5e) 

eps = 0.2로 증가시켜 샘플의 이웃범위를 넓히면 이상치가 줄고 오른쪽처럼 완벽한 군집을 얻는다.

DBCAN 클래스는 predict() 메소드를 제공하지 않고 fit_predict() 메소드를 제공한다. 이는 새 샘플에 대해 클러스터를 예측할 수 없다. 이런 구현 결정은 다른 분류 알고리즘이 작업을 더 잘 수행할 수 있기 때문이다. 따라서 사용자가 필요한 예측기를 선택해야 한다. 
```python
from sklearn.neighbors import KNeighborsClassifier

knn = KNeighborsClassifier(n_neighbors = 50)
knn.fit (dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])
```
이제 샘플 몇 개를 전달하여 어떤 클러스터에 속할 가능성이 높은지 예측 후 각 클러스터에 대한 확률을 추정해보자.
```python
X_new = np.array([[-0.5,0],[0,0.5],[1,-0.1],[2,1]])
knn.predict (X_new)
knn.predict_proba (X_new) 
```
이 결정 경계가 아래 그림에 나타나 있다. 

![image](https://github.com/user-attachments/assets/0b720a98-d018-48f1-a53c-2ca4071cdcfb)

### 9.1.6 다른 군집 알고리즘

1️. 병합 군집 (Agglomerative Clustering)
바닥에서부터 차근차근 클러스터를 합쳐 나가는 방식(병합적 접근).
**클러스터 계층(트리 구조, Dendrogram)** 을 생성하여 데이터 간 유사성을 시각화할 수 있음.
특정 개수의 클러스터를 선택하는 데 도움.
단점: 대규모 데이터셋에는 확장하기 어려움.

2️. BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)
대규모 데이터를 위한 군집 알고리즘.
샘플을 빠르게 분류하고 트리 구조에 저장하여 압축.
메모리 효율적.
단점: 제한된 메모리 환경에서만 유리하며, 일부 복잡한 데이터에는 적용 어려움.

3️. 평균-이동 (Mean-Shift)
데이터의 밀도가 높은 방향으로 점을 이동시키면서 군집 형성.
밀도가 높은 영역을 자동으로 탐색하여 클러스터를 결정(K 값을 미리 지정할 필요 없음).
단점: 고차원 데이터에서는 계산량이 많고 느림.

4️. 유사도 전파 (Affinity Propagation)
모든 샘플이 서로를 대표로 선택하는 방식으로 군집 형성.
클러스터 개수를 자동으로 결정할 수 있음.
단점: 계산량이 많아 대규모 데이터셋에서는 비효율적.

5️. 스펙트럼 군집 (Spectral Clustering)
데이터 간 유사성을 기반으로 그래프 형태로 표현한 후 군집화 진행.
복잡한 형태의 클러스터를 찾는 데 유용.
단점: 대규모 데이터에서는 비효율적.

## 9.2 가우스 혼합
GMM은 샘플이 파라미터가 알려지지 않은 여러 개의 혼합된 가우스 분포에서 생성되었다고 가정하는 확률 모델이다. 하나의 가우스 분포에서 생성된 모든 샘플은 하나의 클러스터를 형성한다. 일반적으로 모양은 타원형이다. 각 클러스터는 모양, 크기, 밀집도, 방향이 다르다. 샘플이 주어지면 가우스 분포 중 하나에서 생성되었다는 건 알지만 어떤 분포인지, 이 분포의 파라미터는 뭔지 알지 못한다. 

가장 간단한 버전은 GaussianMixture 클래스에 구현되어있다. 사전에 가우스 분포 개수 k를 알아야 한다. 
```python
from sklearn.mixture import GaussianMixture
gm = GaussianMixture (n_components = 3, n_init=10, random_state = 42)
gm.fit (X)
```
이 알고리즘이 추정한 파라미터는 다음과 같다. 
```python
gm.weights_
gm.means_
gm.convariances_
```
클러스터 예측
predict(): 데이터가 속할 가장 가능성이 높은 클러스터를 예측.
predict_proba(): 데이터가 각 클러스터에 속할 확률을 반환.
```python
gm.predict(X_new)  # 각 데이터의 클러스터 예측
gm.predict_proba(X_new)  # 클러스터에 속할 확률 반환
```

밀도 기반 이상치 탐지
score_samples(): 각 데이터 포인트의 확률 밀도 값(PDF: Probability Density Function) 로그 값을 반환.
확률 밀도가 낮으면 이상치로 간주 가능.

``python
densities = gm.score_samples(X)
density_threshold = np.percentile(densities, 2)  # 하위 2%를 이상치로 설정
anomalies = X[densities < density_threshold]  # 이상치 탐지
```
3. GMM의 클러스터 개수 선택
적절한 클러스터 개수를 찾기 위해 AIC (Akaike Information Criterion), BIC (Bayesian Information Criterion) 사용.
```python
gm.bic(X)  # BIC 값 계산
gm.aic(X)  # AIC 값 계산
```
BIC가 최소인 K 값을 선택하는 것이 일반적 (K 선택 기준).

4. GMM의 다양한 공분산 유형
covariance_type에 따라 클러스터의 모양이 달라짐.
"spherical": 원형 클러스터.
"diag": 각 차원별 분산만 고려한 타원형 클러스터.
"tied": 모든 클러스터가 동일한 공분산 행렬을 공유.
"full" (기본값): 각 클러스터별로 자유로운 공분산 행렬 적용(가장 유연).
```python
gm = GaussianMixture(n_components=3, covariance_type="full", random_state=42)
```

5. 베이지안 가우스 혼합 모델 (Bayesian GMM)
클러스터 개수를 자동으로 결정하는 GMM 변형 모델.
BayesianGaussianMixture를 사용하여 불필요한 클러스터를 제거할 수 있음.
```python
from sklearn.mixture import BayesianGaussianMixture

bgm = BayesianGaussianMixture(n_components=10, n_init=10, random_state=42)
bgm.fit(X)
print(bgm.weights_.round(2))  # 사용된 클러스터 확인 (0에 가까운 클러스터는 제거됨)
```
6. 이상치 탐지 및 특이점 탐지를 위한 알고리즘
사이킷런에는 이상치 탐지와 특이점(outlier) 탐지를 위한 다양한 알고리즘이 있음.
```python
from sklearn.ensemble import IsolationForest

iso_forest = IsolationForest(contamination=0.02, random_state=42)
iso_forest.fit(X)
anomalies = X[iso_forest.predict(X) == -1]  # 이상치 감지
```




