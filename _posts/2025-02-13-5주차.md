---
layout : post
title : 쿠다 ML 기초 5주차
categories : ML 기초
---
#쿠다 ML 기초 5주차

이번 장에서는 차원 축소와 비지도 학습에 대하여 알아보고자 한다.
머신러닝 문제는 훈련 샘플 각각이 수천~수백만 개의 특성을 가지고 있다.
☞ 많은 특성 : 훈련 속도 저하 + 좋은 솔루션 탐색 어려움 : **차원의 저주**
☞ 해결 방법 : 특성 수를 줄여서 가능한 범위로 변경 가능 : **차원 축소**
차원 축소의 장점 : 훈련 속도 향상 (일반적) + 성능 향상의 가능성 (일반적인 것은 아님) + 데이터 시각화 유용성

## 8.1 차원의 저주
3차원 이상의 고차원 공간을 직관적으로 상상할 수 있는가? 어렵다.

![image](https://github.com/user-attachments/assets/49acdbd9-ae6e-4ebe-8cf7-7240f723366d)

고차원 공간에서는 많은 것이 상당히 다르게 작동한다. 
→ 저차원과 달리 고차원 초입방체에 있는 대다수의 점은 경계와 매우 가까이 있다.

→ 고차원은 많은 공간을 가지고 있어, 고차원 데이터셋이 매우 희박할 위험이 있다. (=새로운 샘플도 훈련 샘플과 멀리 떨어져 있을 가능성이 높다.) 
※ 예측을 위해 외삽을 해야 하기에 저차원일 때보다 예측이 더 불안정하며, 훈련 세트의 차원이 클수록 과대적합 위험이 커진다.

## 8.2 차원 축소를 위한 접근법
차원을 감소시키는 두 가지 주요한 접근법인 투영과 매니폴드 학습을 살펴보자.
### 8.2.1 투영
차원 축소에 중점을 맞춰둔 상태로 이해해보자. 대부분의 실전 문제에서는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져 있지 않다. 그러나 많은 특성은 MNIST 데이터셋의 숫자 그림처럼, 모든 훈련 샘플이 고차원 공간 안의 저차원 **부분 공간**에 놓여 있다. 

![image](https://github.com/user-attachments/assets/80021984-ee10-41e3-8243-527aa8321602)

위 그림과 같이 작은 공으로 표현된 3차원 데이터셋이 있다. 
☞ 모든 훈련 샘플이 거의 평면 형태로 존재 (고차원 공간에 있는 저차원 부분 공간)
☞ 모든 훈련 샘플을 이 부분 공간에 수직으로 투영 시 아래와 같은 2D 데이터셋 얻음

![image](https://github.com/user-attachments/assets/52304ce6-aa45-4fef-b796-4e42ce335d44)

차원 축소에 있어 투영이 언제나 최선의 방법은 아니다. 많은 경우 아래의 그림에 표현된 **스위스 롤** 데이터셋처럼 부분 공간이 뒤틀리거나 휘어 있기도 하다.

![image](https://github.com/user-attachments/assets/dddbda38-db7d-44b4-9c67-13ce418f5fce)

이를 그냥 평면에 투영시키면 왼쪽처럼 겹쳐서 나오지만, 우리가 원하는 것은 펼쳐진 오른쪽 2D 데이터셋이다.

![image](https://github.com/user-attachments/assets/5ea8e35b-1131-42fb-bda7-24ff5bee8efe)

### 8.2.2 매니폴드 학습

매니폴드란? : 어떤 공간이 d차원 매니폴드라는 것은, 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부 (d<n)
많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 **매니폴드**를 모델링하는 식으로 작동 : 매니폴드 학습

자유도 : **데이터를 만들 때 가질 수 있느 ㄴ가능한 변형의 수**
(ex : 28*28 픽셀의 흑백 이미지라면, 총 784개의 픽셀 조정 가능 & 각각의 픽셀은 0~255까지의 값을 가질 수 있어 **이론적으로 매우 많은 이미지 생성 가능**, 그러나 관심 있는 것은 숫자 이미지. 즉 특정한 패턴을 가진 이미지이므로 변형의 자유도가 낮아진다.)
데이터셋이 저차원 매니폴드로 압축될 수 있다는 의미 : 숫자들은 784차원 공간에 저차원 구조(매니폴드) 위에 놓여 있다.
즉, MNIST 데이터는 **784차원 공간 전체를 차지하는 게 아닌, 훨씬 작은 차원의 매니폴드에 놓여 있다.**
이를 활용하여 훨씬 적은 차원의 공간에서 효과적으로 표현할 수 있다.

매니폴드 가정이 저차원 매니폴드 공간에 표현되면 항상 더 간단해질까? 가정이 유효할까? 항상 그렇지 않다.

![image](https://github.com/user-attachments/assets/9fe57211-5982-4afb-b1de-7e49d50160e1)

1행의 그림은 결정 경계가 2D 공간에서 단순한 직선으로 표현되나, 2행의 그림은 3D 공간에서 단순한 수직 평면의 결정 경계가 펼쳐진 매니폴드에서는 더 복잡해졌다. 
즉, **전적으로 데이터셋에 달려있다.**

## 8.3 주성분 분석
가장 인기 있는 차원 축소 알고리즘인 **주성분 분석**에 대하여 알아보자. 먼저, 데이터에 가장 가까운 초평면을 정의한 후, 데이터를 이 평면에 투영시킨다.
### 8.3.1 분산 보존
먼저 올바른 초평면을 선택해야 한다. 예를 들어보자.

![image](https://github.com/user-attachments/assets/f010b978-6ccb-42d1-96cd-dc7cd305dd2d)

☞ 왼쪽 : 간단한 2D 데이터셋이 세 개의 축과 함께 표현됨
☞ 오른쪽 : 각 축이 투영된 결과

분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되므로 합리적이다. 
(=원본 데이터셋과 투영된 것 사이의 **평균 제곱 거리 최소화**)
### 8.3.2 주성분
PCA는 훈련 세트에서 분산이 최대인 축을 찾는다. (**데이터의 정보 최대한 유지**) 또한, 첫 번째 축에 직교하고 남은 분산을 최대한 보존하는 두 번째 축을 찾는다. (위 그림에서는 선택의 여지가 없어 점선이 됨)
※ 고차원 데이터셋은 PCA가 이전의 두 축에 직교하는 세 번째 축을 찾으며 데이터셋에 있는 차원의 수만큼 4,5,...n번째 축을 찾는다.

i번째 주성분 : 데이터의 i번째 축 (PC)
그림을 살펴보자.
첫 번째 PC : 벡터 C1 축
두 번째 PC : 벡터 C2 축
다시 돌아와서,

![image](https://github.com/user-attachments/assets/24592241-bd08-4c46-b231-c0324ce253af)

이 그림에서는 처음 두 개의 PC가 투영 평면에 있으며, 세 번째 PC가 이 평면에 수직인 축이다.
투영된 후, 

![image](https://github.com/user-attachments/assets/2be0d18b-dcd5-4f4c-b3c5-58d6f3cab4d7)

첫 번째 PC : Z1
두 번째 PC : Z2

훈련 세트의 주성분은 **특잇값 분해(SVD)** 라는 표준 행렬 분해 기술로 찾는다. 

아래의 파이썬 코드는 넘파이의 svd() 함수를 사용해 아래 그림의 3D 훈련 세트의 모든 주성분을 구한 후 처음 두 개의 PC를 정의하는 두 개의 단위 벡터를 추출한다.
```python
import numpy as np

# 3D 데이터 생성 (2D 평면에 가깝게)
np.random.seed(42)
n_samples = 100

# x1, x2는 [-1, 1] 범위에서 균등 분포를 따르는 점들
x1 = np.random.uniform(-1, 1, n_samples)
x2 = np.random.uniform(-1, 1, n_samples)

# x3는 특정 평면 (예: x3 ≈ 0) 주변에 노이즈를 추가하여 생성
x3 = 0.05 * np.random.randn(n_samples)  # 작은 노이즈 추가

# 데이터셋을 (n_samples, 3) 형태의 배열로 변환
X = np.vstack((x1, x2, x3)).T

# 중앙 정렬 (평균을 0으로 맞춤)
X_centered = X - X.mean(axis=0)

# SVD를 이용한 주성분 분석 (PCA)
U, s, Vt = np.linalg.svd(X_centered)

# 첫 번째와 두 번째 주성분 벡터 추출
c1 = Vt[0]  # 첫 번째 PC
c2 = Vt[1]  # 두 번째 PC
```

### 8.3.3 d차원으로 투영하기
주성분을 모두 추출했다면, 처음 d개의 주성분으로 정의한 초평면에 투영하여 데이터셋의 차원을 d차원으로 축소시킬 수 있다. 
예를 들어,

![image](https://github.com/user-attachments/assets/10ba14ba-6aab-4a0f-a644-ebf3ab3a96a0)

3D 데이터셋은 데이터셋의 분산이 가장 큰 두 개의 주성분으로 구성된 2D 평면에 투영되었다. 이 2D 투영은 원본 3D 데이터셋과 매우 비슷해보인다.

초평면에 훈련 세트를 투영하고 d차원으로 축소된 데이터셋 Xd-proj를 얻기 위해서는 아래 식과 같이 행렬 X와 V의 첫 d열로 구성된 행렬 Wd를 행렬 곱셈하면 된다.

![image](https://github.com/user-attachments/assets/681b906a-12fa-41fd-b8b4-d826fed702a4)

아래의 파이썬 코드는 첫 두 개의 주성분으로 정의된 평면에 훈련 세트를 투영한다.
```python
W2 = Vt[:2].T
X2D = X_centered @ W2
```

PCA 변환이 되었다. 지금까지 분산을 가능한 한 최대로 유지하면서 어떻게 데이터셋의 차원을 특정 차원을 ㅗ축소하는지 보았다.

### 8.3.4 사이킷런 이용하기
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X2D = pca.fit_transform(X)
```
사이킷런의 PCA 모델은 자동으로 데이터를 중앙에 맞춰준다. 이는 PCA 모델을 사용해 데이터셋의 차원을 2로 줄이는 코드이다.
PCA 변환기를 데이터셋에 학습시키고 나면 components_ 속성에 Wd의 전치가 담겨 있다. 이 배열의 행은 처음 d개의 주성분에 해당한다.

### 8.3.5 설명된 분산의 비율
explined_variance_ratio_ 변수에 저장된 주성분의 **설명된 분산의 비율**도 유용한 정보다. 이는 각 주성분의 축을 따라 있는 데이터셋의 분산 비율을 나타낸다. 
```python
pca.explained_variance_ratio_
```
### 8.3.6 적절한 차원 수 선택
축소할 차원 수를 임의로 정하기보다는 충분한 분산이 될 때까지 더해야 할 차원 수를 선택하는 것이 좋다.
PCA 차원 축소 목표 : 데이터의 중요한 정보를 최대한 유지하면서 불필요한 차원 제거 (**최소한의 차원으로 최대한의 분산 유지**)

(주성분 벡터 개수 = 최종적으로 선택한 차원의 수)

MNIST 데이터셋을 로드하고 분할한 후, 차원을 줄이지 않고 PCA를 수행해보자. 그 후 훈련 집합의 분산 95%를 보존하는 데 필요한 최소 차원 수를 계산하자.

```python
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', as_frame=False)
X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]
X_test, y_test = mnist.data[60_000:],mnist.target[60_000:]

pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1
```
그 후, n_components = d로 설정하여 PCA를 다시 실행한다. 보존하려는 분산의 비율을 n_components에서 0~1 사이로 설정하는 게 좋다.

```python
pca = PCA (n_components = 0.95)
X_reduced = pca.fit_transform (X_train)
```
실제 주성분 개수는 훈련 중 결정되며, n_components_ 속성에 저장된다.
```python
pca.n_components_
```
다른 방법은 설명된 분산을 차원 수에 대한 함수로 그리는 것이다. 

![image](https://github.com/user-attachments/assets/46ff1141-4293-4ef9-a9a9-65173d9b2c1b)

이 그래프에서는 설명된 분산의 빠른 성장이 멈추는 변곡점이 있다. 여기서는 차원을 약 100으로 축소해도 설명된 분산 손해를 보지 않을 것이다.

지도 학습 작업 (분류)의 전처리 단계로 차원 축소를 사용하는 경우, 다른 하이퍼파라미터와 마찬가지로 차원 수를 튜닝할 수 있다. 

아래의 코드에서는 두 단계로 구성된 파이프라인을 생성한다. 먼저 PCA를 사용하여 차원을 줄인 후, 랜덤 포레스트를 사용하여 분류를 수행한다. 그 후 RandomizedSearchCV를 사용해 PCA와 랜덤 포레스트 분류기에 잘 맞는 하이퍼파라미터 조합을 찾는다. 

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.pipeline import make_pipeline

clf = make_pipeline (PCA(random_state=42), RandomForestClassifier(random_state=42)) 
param_distrib = {
    "pca__n_components": np.arange(10,80),
    "randomforestclassifier__n_estimators": np.arange(10,100)
}
rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3, random_state=42)
rnd_search.fit(X_train[:1000], y_train[:1000])
```
앞에서 찾은 최상의 하이퍼파라미터를 살펴보자.
```python 
print (rnd_search.best_params_)
```
### 8.3.7 압축을 위한 PCA
차원 축소 후 훈련 세트는 훨씬 적은 공간을 차지한다. 즉, 상당한 압축률을 보여주며 이런 크기 축소는 분류 알고리즘의 속도를 크게 높일 수 있다. 또한 압축된 데이터셋에 PCA 투영의 변환을 반대로 적용하여 원래의 차원으로 되돌릴 수도 있다. 투영 중 일정량의 정보를 잃어버렸다면 원본 데이터셋을 얻을 수는 없을 것이지만, 원본 데이터와 매우 비슷할 것이다.
원본 데이터와 재구성된 데이터 사이의 평균 제곱 거리를 **재구성 오차**라고 한다. 
inverse_transform() 메소드를 사용하여 복원 가능하다. 
```python
X_recovered = pca.inverse_transform(X_reduced)
```
### 8.3.8 랜덤 PCA
svd_solver 매개변수를 randomized로 지정하면 사이킷런은 랜덤 PCA라 부르는 확률적 알고리즘을 사용하여 처음 d개의 주성분에 대한 근삿값을 빠르게 찾는다. 이는 d가 n보다 많이 작을 때 완전 SVD보다 계산이 훨씬 빠르다.
```python
rnd_pca = PCA (n_components = 154, svd_solver = 'randomized',random_state = 42)
X_reduced = rnd_pca.fit_transform (X_train)
```
svd_solver의 기본값은 auto이다. max(m,n)>500이고 n_components가 min(m,n)의 80%보다 작은 정수면 사이킷런은 자동으로 PCA 알고리즘을 사용한다. 그렇지 않으면 완전한 SVD 방식을 사용한다. 앞의 코드는 154 < 0.8 * 784이므로 매개변수 없이도 random pca 알고리즘을 사용한다. 완전한 SVD를 이용하려면 매개변수를 full로 설정하면 된다.

### 8.3.9 점진적 PCA
SVD 알고리즘을 실행하기 위해 전체 훈련 세트를 메모리에 올려야 하는 문제를 해결하기 위해 **점진적 PCA** 알고리즘이 개발되었다. 이는 훈련세트를 미니배치로 나눈 뒤 점진적 PCA 알고리즘에 한 번에 하나씩 주입한다. 이런 방식은 훈련세트가 클 때 유용하고 온라인으로 PCA를 적용할 수 있다.

아래의 코드느 MNIST 훈련 세트를 100개의 미니배치로 나누고 사이킷런의 IncrementalPCA 파이썬 클래스에 주입하여 데이터셋을 이전과 같이 154개로 줄인다. 미니배치마다 partial_fit()메소드를 호출해야 한다.

```python
from sklearn.decomposition import IncrementalPCA

n_batches = 100
inc_pca = IncrementalPCA(n_components=154)
for X_batch in np.array_split(X_train, n_batches):
    inc_pca.partial_fit(X_batch)  

X_reduced = inc_pca.transform(X_train)
```
또는 디스크의 이진파일에 저장된 대규모 배열을 마치 메모리에 있는 것처럼 조작할 수 있는 넘파이 memmap 클래스를 사용할 수 있다. 이 클래스는 필요할 때 원하는 데이터만 메모리에 로드한다. 아래의 코드를 보자. 먼저 memmap을 생성하고 훈련 세트를 복사한 후 flush()를 호출하여 캐시에 남아 있는 모든 데이터가 디스크에 저장되도록 해보겠다.
```python
filename = 'my_mnist.mmap'
X_mmap = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)
X_mmap[:] = X_train
X_mmap.flush()
```
다음으로 memmap 파일을 로드하고 일반적인 넘파이 배열처럼 사용할 수 있다. IncrementalPCA로 차원을 줄여보자. 이는 특정 순간에 배열의 작은 부분만 사용하기에 메모리 부족 문제가 안일어난다. 따라서 일반적인 fit() 메소드를 호출할 수 있어 편리하다.
```python
X_mmap = np.memmap (filename, dtype = 'float32', mode = 'readonly').reshape(-1,784)
batch_size = X_mmap.shape[0] // n_batches
inc_pca = IncrementalPCA (n_components = 154, batch_size = batch_size)
inc_pca.fit(X_mmap)
```

## 8.4 랜덤 투영
랜덤 투영 알고리즘은 랜덤한 선형 투영을 사용하여 데이터를 저차원 공간에 투영한다. 랜덤한 투영은 실제로 거리를 상당히 잘 보존할 가능성이 매우 높다.
매우 고차원인 데이터셋의 경우 PCA가 느려질 수 있으므로 랜덤 투영을 사용하는 것을 고려해볼 수 있다.

최적의 차원 수는 어떻게 선택할 수 있을까?
거리가 주어진 허용 오차 이상으로 변하지 않도록 보장하기 위해 보존할 최소 차원 수를 결정하는 방향으로 생각해야 한다.
이러한 방정식은 johnson_lindenstrauss_min_dim() 함수에 구현되어 있다.

```python
from sklearn.random_projection import johnson_lindenstrauss_min_dim
m, e = 5_000, 0.1
d = johnson_lindenstrauss_min_dim(m, eps=e)
d
```
이제 각 항목을 평균 0, 분산 1/d 가우스 분포에서 랜덤 샘플링한 [d,n]크기의 랜덤 행렬 P를 생성, 이를 사용하여 데이터셋을 n에서 d차원으로 투영할 수 있다.
```python
n = 20_000
np.random.seed(42)
P = np.random.randn (d,n) / np.sqrt(d) # 표준 편차 = 분산의 제곱근

X = np.random.randn (m,n)
X_reduced = X @ P.T
```
알고리즘이 랜덤한 행렬을 생성하는 데 필요한 것을 데이터셋의 크기 뿐이므로 간단, 효율적이며 훈련이 불필요하다. 데이터는 사용되지 않는다.

사이킷런은 방금과 동일한 작업을 수행할 수 있는 GaussianRandomProjection 클래스를 제공한다. 이 클래스의 fit() 메소드를 호출하면 johnson_lindenstrauss_min_dim() 을 사용해 출력 차원을 결정한 후, 랜덤한 행렬을 생성하여 components_ 속성에 저장. 그 후 transform()을 호출 시 이 행렬을 사용해 투영을 수행한다. 
코드를 살펴보자.
```python
from sklearn.random_projection import GaussianRandomProjection

gaussian_rnd_proj = GaussianRandomProjection(eps=e, random_state=42)
X_reduced = gaussian_rnd_proj.fit_transform(X)
```
사이킷런은 SparseRandomProjection이라는 두 번째 랜덤 투영 변환기도 제공한다. 이는 동일한 방식으로 타깃 차원을 결정, 동일한 크기의 랜덤 행렬 생성, 그리고 투영을 동일하게 수행한다. 차이점은 랜덤 행렬의 희소성이다. 메모리가 적게 사용된다는 뜻이다. 랜덤 행렬을 생성하고 차원을 줄이는 속도도 빠르며, 입력이 희소할 경우 희소성을 유지하는 특징도 있다. 마지막으로 이전 접근 방식과 동일한 거리 보존 속성을 가지며 차원 축소 품질도 비슷하다. 규모가 크거나 희박한 데이터셋이면 2번째 변환기를 사용할 것.

희소한 랜덤 행렬에서 0 아닌 항목의 비율 r을 밀도라고 한다. 기본적으로 밀도는 1/root n이지만, 원하는 경우 density 매개변수를 다른 값으로 설정할 수 있다. 희소 랜덤 행렬의 각 항목은 0이 아닐 확률 r을 가지며, 이는 -v또는 +v이고, 여기서 v= 1/root (dr)이다.

역변환을 수행할 땐 사이파이의 pinv () 함수를 사용해 성분 행렬의 유사역행렬을 계산후, 축소된 데이터에 유사역행렬의 전치를 곱해야 한다.

```python
components_pinv = np.linalg.pinv (gaussian_rnd_proj.components_)
X_recovered = X @ components_pinv.T
```
랜덤 투영은 간단하고, 빠르며, 메모리 효율이 높고 강력한 차원 축소 알고리즘으로, 고차원 데이터셋을 다룰 때 염두에 두자.

## 8.5 지역 선형 임베딩
지역선형 임베딩은 비선형 차원 축소 기술이다. PCA나 랜덤투영과 달리 투영에 의존하지 않는 매니폴드 학습이다. 이는 먼저 각 훈련 샘플이 최근접 이웃에 얼마나 선형적으로 연관되어 있는지 측정 후, 국부적인 관계가 가장 잘 보존되는 훈련세트의 저차원 표현을 찾는다. 이는 특히 잡음이 너무 많지 않은 경우 꼬인 매니폴드를 펼치는 데 좋다.
```python
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import LocallyLinearEmbedding

X_swiss, t = make_swiss_roll(n_samples = 1000, noise = 0.2, random_state = 42)
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)
X_unrolled = lle.fit_transform(X_swiss)
```
이는 스위스롤을 만든 후 사이킷런의 LocallyLinearEmbedding을 사용해 이를 펼친 것.
이에 대한 그림은 아래를 통해 확인할 수 있다. 

![image](https://github.com/user-attachments/assets/371c07a3-9d26-4bf3-9b2c-66cbde76aaca)

스위스 롤이 완전히 펼쳐졌고 지역적으로는 샘플 간 거리가 잘 보존되어 있다. 그러나 크게 보면 샘플 간 거리가 잘 유지되어 있지 않다. 펼쳐진 스위스롤은 이런식으로 늘어나거나 꼬인 밴드가 아닌 직사각형이어야 하지만, LLE는 매니폴드를 모델링하는 데 잘 작동한다.
