---
layout : post
title : 쿠다 ML 기초 5주차
categories : ML 기초
---
#쿠다 ML 기초 5주차

이번 장에서는 차원 축소와 비지도 학습에 대하여 알아보고자 한다.
머신러닝 문제는 훈련 샘플 각각이 수천~수백만 개의 특성을 가지고 있다.
☞ 많은 특성 : 훈련 속도 저하 + 좋은 솔루션 탐색 어려움 : **차원의 저주**
☞ 해결 방법 : 특성 수를 줄여서 가능한 범위로 변경 가능 : **차원 축소**
차원 축소의 장점 : 훈련 속도 향상 (일반적) + 성능 향상의 가능성 (일반적인 것은 아님) + 데이터 시각화 유용성

## 8.1 차원의 저주
3차원 이상의 고차원 공간을 직관적으로 상상할 수 있는가? 어렵다.

![image](https://github.com/user-attachments/assets/49acdbd9-ae6e-4ebe-8cf7-7240f723366d)

고차원 공간에서는 많은 것이 상당히 다르게 작동한다. 
→ 저차원과 달리 고차원 초입방체에 있는 대다수의 점은 경계와 매우 가까이 있다.

→ 고차원은 많은 공간을 가지고 있어, 고차원 데이터셋이 매우 희박할 위험이 있다. (=새로운 샘플도 훈련 샘플과 멀리 떨어져 있을 가능성이 높다.) 
※ 예측을 위해 외삽을 해야 하기에 저차원일 때보다 예측이 더 불안정하며, 훈련 세트의 차원이 클수록 과대적합 위험이 커진다.

## 8.2 차원 축소를 위한 접근법
차원을 감소시키는 두 가지 주요한 접근법인 투영과 매니폴드 학습을 살펴보자.
### 8.2.1 투영
차원 축소에 중점을 맞춰둔 상태로 이해해보자. 대부분의 실전 문제에서는 훈련 샘플이 모든 차원에 걸쳐 균일하게 퍼져 있지 않다. 그러나 많은 특성은 MNIST 데이터셋의 숫자 그림처럼, 모든 훈련 샘플이 고차원 공간 안의 저차원 **부분 공간**에 놓여 있다. 

![image](https://github.com/user-attachments/assets/80021984-ee10-41e3-8243-527aa8321602)

위 그림과 같이 작은 공으로 표현된 3차원 데이터셋이 있다. 
☞ 모든 훈련 샘플이 거의 평면 형태로 존재 (고차원 공간에 있는 저차원 부분 공간)
☞ 모든 훈련 샘플을 이 부분 공간에 수직으로 투영 시 아래와 같은 2D 데이터셋 얻음

![image](https://github.com/user-attachments/assets/52304ce6-aa45-4fef-b796-4e42ce335d44)

차원 축소에 있어 투영이 언제나 최선의 방법은 아니다. 많은 경우 아래의 그림에 표현된 **스위스 롤** 데이터셋처럼 부분 공간이 뒤틀리거나 휘어 있기도 하다.

![image](https://github.com/user-attachments/assets/dddbda38-db7d-44b4-9c67-13ce418f5fce)

이를 그냥 평면에 투영시키면 왼쪽처럼 겹쳐서 나오지만, 우리가 원하는 것은 펼쳐진 오른쪽 2D 데이터셋이다.

![image](https://github.com/user-attachments/assets/5ea8e35b-1131-42fb-bda7-24ff5bee8efe)

### 8.2.2 매니폴드 학습

매니폴드란? : 어떤 공간이 d차원 매니폴드라는 것은, 국부적으로 d차원 초평면으로 보일 수 있는 n차원 공간의 일부 (d<n)
많은 차원 축소 알고리즘이 훈련 샘플이 놓여 있는 **매니폴드**를 모델링하는 식으로 작동 : 매니폴드 학습

자유도 : **데이터를 만들 때 가질 수 있느 ㄴ가능한 변형의 수**
(ex : 28*28 픽셀의 흑백 이미지라면, 총 784개의 픽셀 조정 가능 & 각각의 픽셀은 0~255까지의 값을 가질 수 있어 **이론적으로 매우 많은 이미지 생성 가능**, 그러나 관심 있는 것은 숫자 이미지. 즉 특정한 패턴을 가진 이미지이므로 변형의 자유도가 낮아진다.)
데이터셋이 저차원 매니폴드로 압축될 수 있다는 의미 : 숫자들은 784차원 공간에 저차원 구조(매니폴드) 위에 놓여 있다.
즉, MNIST 데이터는 **784차원 공간 전체를 차지하는 게 아닌, 훨씬 작은 차원의 매니폴드에 놓여 있다.**
이를 활용하여 훨씬 적은 차원의 공간에서 효과적으로 표현할 수 있다.

매니폴드 가정이 저차원 매니폴드 공간에 표현되면 항상 더 간단해질까? 가정이 유효할까? 항상 그렇지 않다.

![image](https://github.com/user-attachments/assets/9fe57211-5982-4afb-b1de-7e49d50160e1)

1행의 그림은 결정 경계가 2D 공간에서 단순한 직선으로 표현되나, 2행의 그림은 3D 공간에서 단순한 수직 평면의 결정 경계가 펼쳐진 매니폴드에서는 더 복잡해졌다. 
즉, **전적으로 데이터셋에 달려있다.**

## 8.3 주성분 분석
가장 인기 있는 차원 축소 알고리즘인 **주성분 분석**에 대하여 알아보자. 먼저, 데이터에 가장 가까운 초평면을 정의한 후, 데이터를 이 평면에 투영시킨다.
### 8.3.1 분산 보존
먼저 올바른 초평면을 선택해야 한다. 예를 들어보자.

![image](https://github.com/user-attachments/assets/f010b978-6ccb-42d1-96cd-dc7cd305dd2d)

☞ 왼쪽 : 간단한 2D 데이터셋이 세 개의 축과 함께 표현됨
☞ 오른쪽 : 각 축이 투영된 결과

분산이 최대로 보존되는 축을 선택하는 것이 정보가 가장 적게 손실되므로 합리적이다. 
(=원본 데이터셋과 투영된 것 사이의 **평균 제곱 거리 최소화**)
### 8.3.2 주성분
PCA는 훈련 세트에서 분산이 최대인 축을 찾는다. (**데이터의 정보 최대한 유지**) 또한, 첫 번째 축에 직교하고 남은 분산을 최대한 보존하는 두 번째 축을 찾는다. (위 그림에서는 선택의 여지가 없어 점선이 됨)
※ 고차원 데이터셋은 PCA가 이전의 두 축에 직교하는 세 번째 축을 찾으며 데이터셋에 있는 차원의 수만큼 4,5,...n번째 축을 찾는다.

i번째 주성분 : 데이터의 i번째 축 (PC)
그림을 살펴보자.
첫 번째 PC : 벡터 C1 축
두 번째 PC : 벡터 C2 축
다시 돌아와서,

![image](https://github.com/user-attachments/assets/24592241-bd08-4c46-b231-c0324ce253af)

이 그림에서는 처음 두 개의 PC가 투영 평면에 있으며, 세 번째 PC가 이 평면에 수직인 축이다.
투영된 후, 

![image](https://github.com/user-attachments/assets/2be0d18b-dcd5-4f4c-b3c5-58d6f3cab4d7)

첫 번째 PC : Z1
두 번째 PC : Z2

훈련 세트의 주성분은 **특잇값 분해(SVD)** 라는 표준 행렬 분해 기술로 찾는다. 

아래의 파이썬 코드는 넘파이의 svd() 함수를 사용해 아래 그림의 3D 훈련 세트의 모든 주성분을 구한 후 처음 두 개의 PC를 정의하는 두 개의 단위 벡터를 추출한다.
```python
import numpy as np

# 3D 데이터 생성 (2D 평면에 가깝게)
np.random.seed(42)
n_samples = 100

# x1, x2는 [-1, 1] 범위에서 균등 분포를 따르는 점들
x1 = np.random.uniform(-1, 1, n_samples)
x2 = np.random.uniform(-1, 1, n_samples)

# x3는 특정 평면 (예: x3 ≈ 0) 주변에 노이즈를 추가하여 생성
x3 = 0.05 * np.random.randn(n_samples)  # 작은 노이즈 추가

# 데이터셋을 (n_samples, 3) 형태의 배열로 변환
X = np.vstack((x1, x2, x3)).T

# 중앙 정렬 (평균을 0으로 맞춤)
X_centered = X - X.mean(axis=0)

# SVD를 이용한 주성분 분석 (PCA)
U, s, Vt = np.linalg.svd(X_centered)

# 첫 번째와 두 번째 주성분 벡터 추출
c1 = Vt[0]  # 첫 번째 PC
c2 = Vt[1]  # 두 번째 PC
```

### 8.3.3 d차원으로 투영하기
주성분을 모두 추출했다면, 처음 d개의 주성분으로 정의한 초평면에 투영하여 데이터셋의 차원을 d차원으로 축소시킬 수 있다. 
예를 들어,

![image](https://github.com/user-attachments/assets/10ba14ba-6aab-4a0f-a644-ebf3ab3a96a0)

3D 데이터셋은 데이터셋의 분산이 가장 큰 두 개의 주성분으로 구성된 2D 평면에 투영되었다. 이 2D 투영은 원본 3D 데이터셋과 매우 비슷해보인다.

초평면에 훈련 세트를 투영하고 d차원으로 축소된 데이터셋 Xd-proj를 얻기 위해서는 아래 식과 같이 행렬 X와 V의 첫 d열로 구성된 행렬 Wd를 행렬 곱셈하면 된다.

![image](https://github.com/user-attachments/assets/681b906a-12fa-41fd-b8b4-d826fed702a4)

아래의 파이썬 코드는 첫 두 개의 주성분으로 정의된 평면에 훈련 세트를 투영한다.
```python
W2 = Vt[:2].T
X2D = X_centered @ W2
```

PCA 변환이 되었다. 지금까지 분산을 가능한 한 최대로 유지하면서 어떻게 데이터셋의 차원을 특정 차원을 ㅗ축소하는지 보았다.

### 8.3.4 사이킷런 이용하기
```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X2D = pca.fit_transform(X)
```
사이킷런의 PCA 모델은 자동으로 데이터를 중앙에 맞춰준다. 이는 PCA 모델을 사용해 데이터셋의 차원을 2로 줄이는 코드이다.
PCA 변환기를 데이터셋에 학습시키고 나면 components_ 속성에 Wd의 전치가 담겨 있다. 이 배열의 행은 처음 d개의 주성분에 해당한다.

### 8.3.5 설명된 분산의 비율
explined_variance_ratio_ 변수에 저장된 주성분의 **설명된 분산의 비율**도 유용한 정보다. 이는 각 주성분의 축을 따라 있는 데이터셋의 분산 비율을 나타낸다. 
```python
pca.explained_variance_ratio_
```
### 8.3.6 적절한 차원 수 선택
축소할 차원 수를 임의로 정하기보다는 충분한 분산이 될 때까지 더해야 할 차원 수를 선택하는 것이 좋다.
PCA 차원 축소 목표 : 데이터의 중요한 정보를 최대한 유지하면서 불필요한 차원 제거 (**최소한의 차원으로 최대한의 분산 유지**)

(주성분 벡터 개수 = 최종적으로 선택한 차원의 수)

MNIST 데이터셋을 로드하고 분할한 후, 차원을 줄이지 않고 PCA를 수행해보자. 그 후 훈련 집합의 분산 95%를 보존하는 데 필요한 최소 차원 수를 계산하자.

```python
from sklearn.datasets import fetch_openml

mnist = fetch_openml('mnist_784', as_frame=False)
X_train, y_train = mnist.data[:60_000], mnist.target[:60_000]
X_test, y_test = mnist.data[60_000:],mnist.target[60_000:]

pca = PCA()
pca.fit(X_train)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.95) + 1
```
그 후, n_components = d로 설정하여 PCA를 다시 실행한다. 보존하려는 분산의 비율을 n_components에서 0~1 사이로 설정하는 게 좋다.

```python
pca = PCA (n_components = 0.95)
X_reduced = pca.fit_transform (X_train)
```
실제 주성분 개수는 훈련 중 결정되며, n_components_ 속성에 저장된다.
```python
pca.n_components_
```
다른 방법은 설명된 분산을 차원 수에 대한 함수로 그리는 것이다. 

![image](https://github.com/user-attachments/assets/46ff1141-4293-4ef9-a9a9-65173d9b2c1b)

이 그래프에서는 설명된 분산의 빠른 성장이 멈추는 변곡점이 있다. 여기서는 차원을 약 100으로 축소해도 설명된 분산 손해를 보지 않을 것이다.

지도 학습 작업 (분류)의 전처리 단계로 차원 축소를 사용하는 경우, 다른 하이퍼파라미터와 마찬가지로 차원 수를 튜닝할 수 있다. 

아래의 코드에서는 두 단계로 구성된 파이프라인을 생성한다. 먼저 PCA를 사용하여 차원을 줄인 후, 랜덤 포레스트를 사용하여 분류를 수행한다. 그 후 RandomizedSearchCV를 사용해 PCA와 랜덤 포레스트 분류기에 잘 맞는 하이퍼파라미터 조합을 찾는다. 

```python
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.pipeline import make_pipeline

clf = make_pipeline (PCA(random_state=42), RandomForestClassifier(random_state=42)) 
param_distrib = {
    "pca__n_components": np.arange(10,80),
    "randomforestclassifier__n_estimators": np.arange(10,100)
}
rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=10, cv=3, random_state=42)
rnd_search.fit(X_train[:1000], y_train[:1000])
```
앞에서 찾은 최상의 하이퍼파라미터를 살펴보자.
```python 
print (rnd_search.best_params_)
```
### 8.3.7 압축을 위한 PCA
차원 축소 후 훈련 세트는 훨씬 적은 공간을 차지한다. 즉, 상당한 압축률을 보여주며 이런 크기 축소는 분류 알고리즘의 속도를 크게 높일 수 있다. 또한 압축된 데이터셋에 PCA 투영의 변환을 반대로 적용하여 원래의 차원으로 되돌릴 수도 있다. 투영 중 일정량의 정보를 잃어버렸다면 원본 데이터셋을 얻을 수는 없을 것이지만, 원본 데이터와 매우 비슷할 것이다.
원본 데이터와 재구성된 데이터 사이의 평균 제곱 거리를 **재구성 오차**라고 한다. 
inverse_transform() 메소드를 사용하여 복원 가능하다. 
```python
X_recovered = pca.inverse_transform(X_reduced)
```
### 8.3.8 랜덤 PCA
svd_solver 매개변수를 randomized로 지정하면 사이킷런은 랜덤 PCA라 부르는 확률적 알고리즘을 사용하여 처음 d개의 주성분에 대한 근삿값을 빠르게 찾는다. 이는 d가 n보다 많이 작을 때 완전 SVD보다 계산이 훨씬 빠르다.
```python
rnd_pca = PCA (n_components = 154, svd_solver = 'randomized',random_state = 42)
X_reduced = rnd_pca.fit_transform (X_train)
```
svd_solver의 기본값은 auto이다. max(m,n)>500이고 n_components가 min(m,n)의 80%보다 작은 정수면 사이킷런은 자동으로 PCA 알고리즘을 사용한다. 그렇지 않으면 완전한 SVD 방식을 사용한다. 앞의 코드는 154 < 0.8 * 784이므로 매개변수 없이도 random pca 알고리즘을 사용한다. 완전한 SVD를 이용하려면 매개변수를 full로 설정하면 된다.

### 8.3.9 점진적 PCA
SVD 알고리즘을 실행하기 위해 전체 훈련 세트를 메모리에 올려야 하는 문제를 해결하기 위해 **점진적 PCA** 알고리즘이 개발되었다. 이는 훈련세트를 미니배치로 나눈 뒤 점진적 PCA 알고리즘에 한 번에 하나씩 주입한다. 이런 방식은 훈련세트가 클 때 유용하고 온라인으로 PCA를 적용할 수 있다.

아래의 코드느 MNIST 훈련 세트를 100개의 미니배치로 나누고 사이킷런의 IncrementalPCA 파이썬 클래스에 주입하여 데이터셋을 이전과 같이 154개로 줄인다. 미니배치마다 partial_fit()메소드를 호출해야 한다.

```python
from sklearn.decomposition import IncrementalPCA

n_batches = 100
inc_pca = IncrementalPCA(n_components=154)
for X_batch in np.array_split(X_train, n_batches):
    inc_pca.partial_fit(X_batch)  

X_reduced = inc_pca.transform(X_train)
```
또는 디스크의 이진파일에 저장된 대규모 배열을 마치 메모리에 있는 것처럼 조작할 수 있는 넘파이 memmap 클래스를 사용할 수 있다. 이 클래스는 필요할 때 원하는 데이터만 메모리에 로드한다. 아래의 코드를 보자. 먼저 memmap을 생성하고 훈련 세트를 복사한 후 flush()를 호출하여 캐시에 남아 있는 모든 데이터가 디스크에 저장되도록 해보겠다.
```python
filename = 'my_mnist.mmap'
X_mmap = np.memmap(filename, dtype='float32', mode='write', shape=X_train.shape)
X_mmap[:] = X_train
X_mmap.flush()
```
다음으로 memmap 파일을 로드하고 일반적인 넘파이 배열처럼 사용할 수 있다. IncrementalPCA로 차원을 줄여보자. 이는 특정 순간에 배열의 작은 부분만 사용하기에 메모리 부족 문제가 안일어난다. 따라서 일반적인 fit() 메소드를 호출할 수 있어 편리하다.
```python
X_mmap = np.memmap (filename, dtype = 'float32', mode = 'readonly').reshape(-1,784)
batch_size = X_mmap.shape[0] // n_batches
inc_pca = IncrementalPCA (n_components = 154, batch_size = batch_size)
inc_pca.fit(X_mmap)
```

## 8.4 랜덤 투영
랜덤 투영 알고리즘은 랜덤한 선형 투영을 사용하여 데이터를 저차원 공간에 투영한다. 랜덤한 투영은 실제로 거리를 상당히 잘 보존할 가능성이 매우 높다.
매우 고차원인 데이터셋의 경우 PCA가 느려질 수 있으므로 랜덤 투영을 사용하는 것을 고려해볼 수 있다.

최적의 차원 수는 어떻게 선택할 수 있을까?
거리가 주어진 허용 오차 이상으로 변하지 않도록 보장하기 위해 보존할 최소 차원 수를 결정하는 방향으로 생각해야 한다.
이러한 방정식은 johnson_lindenstrauss_min_dim() 함수에 구현되어 있다.

```python
from sklearn.random_projection import johnson_lindenstrauss_min_dim
m, e = 5_000, 0.1
d = johnson_lindenstrauss_min_dim(m, eps=e)
d
```
이제 각 항목을 평균 0, 분산 1/d 가우스 분포에서 랜덤 샘플링한 [d,n]크기의 랜덤 행렬 P를 생성, 이를 사용하여 데이터셋을 n에서 d차원으로 투영할 수 있다.
```python
n = 20_000
np.random.seed(42)
P = np.random.randn (d,n) / np.sqrt(d) # 표준 편차 = 분산의 제곱근

X = np.random.randn (m,n)
X_reduced = X @ P.T
```
알고리즘이 랜덤한 행렬을 생성하는 데 필요한 것을 데이터셋의 크기 뿐이므로 간단, 효율적이며 훈련이 불필요하다. 데이터는 사용되지 않는다.

사이킷런은 방금과 동일한 작업을 수행할 수 있는 GaussianRandomProjection 클래스를 제공한다. 이 클래스의 fit() 메소드를 호출하면 johnson_lindenstrauss_min_dim() 을 사용해 출력 차원을 결정한 후, 랜덤한 행렬을 생성하여 components_ 속성에 저장. 그 후 transform()을 호출 시 이 행렬을 사용해 투영을 수행한다. 
코드를 살펴보자.
```python
from sklearn.random_projection import GaussianRandomProjection

gaussian_rnd_proj = GaussianRandomProjection(eps=e, random_state=42)
X_reduced = gaussian_rnd_proj.fit_transform(X)
```
사이킷런은 SparseRandomProjection이라는 두 번째 랜덤 투영 변환기도 제공한다. 이는 동일한 방식으로 타깃 차원을 결정, 동일한 크기의 랜덤 행렬 생성, 그리고 투영을 동일하게 수행한다. 차이점은 랜덤 행렬의 희소성이다. 메모리가 적게 사용된다는 뜻이다. 랜덤 행렬을 생성하고 차원을 줄이는 속도도 빠르며, 입력이 희소할 경우 희소성을 유지하는 특징도 있다. 마지막으로 이전 접근 방식과 동일한 거리 보존 속성을 가지며 차원 축소 품질도 비슷하다. 규모가 크거나 희박한 데이터셋이면 2번째 변환기를 사용할 것.

희소한 랜덤 행렬에서 0 아닌 항목의 비율 r을 밀도라고 한다. 기본적으로 밀도는 1/root n이지만, 원하는 경우 density 매개변수를 다른 값으로 설정할 수 있다. 희소 랜덤 행렬의 각 항목은 0이 아닐 확률 r을 가지며, 이는 -v또는 +v이고, 여기서 v= 1/root (dr)이다.

역변환을 수행할 땐 사이파이의 pinv () 함수를 사용해 성분 행렬의 유사역행렬을 계산후, 축소된 데이터에 유사역행렬의 전치를 곱해야 한다.

```python
components_pinv = np.linalg.pinv (gaussian_rnd_proj.components_)
X_recovered = X @ components_pinv.T
```
랜덤 투영은 간단하고, 빠르며, 메모리 효율이 높고 강력한 차원 축소 알고리즘으로, 고차원 데이터셋을 다룰 때 염두에 두자.

## 8.5 지역 선형 임베딩
지역선형 임베딩은 비선형 차원 축소 기술이다. PCA나 랜덤투영과 달리 투영에 의존하지 않는 매니폴드 학습이다. 이는 먼저 각 훈련 샘플이 최근접 이웃에 얼마나 선형적으로 연관되어 있는지 측정 후, 국부적인 관계가 가장 잘 보존되는 훈련세트의 저차원 표현을 찾는다. 이는 특히 잡음이 너무 많지 않은 경우 꼬인 매니폴드를 펼치는 데 좋다.
```python
from sklearn.datasets import make_swiss_roll
from sklearn.manifold import LocallyLinearEmbedding

X_swiss, t = make_swiss_roll(n_samples = 1000, noise = 0.2, random_state = 42)
lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)
X_unrolled = lle.fit_transform(X_swiss)
```
이는 스위스롤을 만든 후 사이킷런의 LocallyLinearEmbedding을 사용해 이를 펼친 것.
이에 대한 그림은 아래를 통해 확인할 수 있다. 

![image](https://github.com/user-attachments/assets/371c07a3-9d26-4bf3-9b2c-66cbde76aaca)

스위스 롤이 완전히 펼쳐졌고 지역적으로는 샘플 간 거리가 잘 보존되어 있다. 그러나 크게 보면 샘플 간 거리가 잘 유지되어 있지 않다. 펼쳐진 스위스롤은 이런식으로 늘어나거나 꼬인 밴드가 아닌 직사각형이어야 하지만, LLE는 매니폴드를 모델링하는 데 잘 작동한다.

# 비지도 학습
알고리즘이 레이블이 없는 데이터를 바로 사용하기 위해서는 어떻게 해야할까? 바로 **비지도 학습**이 필요하다. 지난 시간에 가장 널리 사용되는 비지도 학습 방법인 차원 축소를 살펴보았다. 이제 몇 가지 비지도 학습 작업을 추가로 알아보자.

군집 : 비슷한 샘플을 **클러스터**로 모음. 

이상치 탐지 : '정상' 데이터가 어떻게 보이는지 학습 후 비정상 샘플 감지에 사용 (정상 샘플 = 정상치, 이상 샘플 = 이상치)

밀도 추정 : 데이터셋 생성 확률 과정의 **확률 밀도 함수**를 추정. 이는 이상치 탐지에 널리 사용됨. 밀도가 매우 낮은 영역에 놓인 샘플이 이상치일 가능성 ↑

## 9.1 군집
**군집**이란 비슷한 샘플을 구별해 하나의 **클러스터** 또는 비슷한 샘플의 그룹으로 할당하는 작업이다. 분류와 마찬가지로 각 샘플은 하나의 그룹에 할당되지만, 군집은 비지도 학습이다. 그림을 살펴보자.

![image](https://github.com/user-attachments/assets/bb747309-1621-4beb-aade-a7b3beaf9fd5)

왼쪽 그림은 레이블로 잘 분류되어 있다. (로지스틱 회귀, SVM, 랜덤 포레스트 등의 분류기 같은 분류 알고리즘에 적합)
그러나 오른쪽 그림은 레이블이 없다. 군집 알고리즘이 필요한 경우다. 모든 특성을 사용하면 군집 알고리즘이 클러스터 세 개를 매우 잘 구분할 수 있다.

군집은 다양한 애플리케이션에서 사용된다.
고객분류 - 추천시스템
데이터분석 - 새로운 데이터셋 분석 시 각 클러스터 각각 분석
차원 축소 기법 - 친화성 (샘플이 클러스터에 얼마나 잘 맞는지)
특성 공학 - 클러스터 친화성으로 더 나은 성능 획득 가능
이상치 탐지 - 친화성 낮은 샘플을 이상치로 간주할 가능성
준지도 학습 - 레이블 증가시켜 성능 향상
검색 엔진 - 제시된 이미지와 유사한 이미지 제공
이미지 분할 - 색 기반 픽셀을 클러스터로 모은 후 색을 평균색으로 바꿔 윤곽 감지 용이

유명한 군집 알고리즘인 k-평균과 DBSCAN을 살펴보자. 그 후, 비선형 차원 축소, 준지도 학습, 이상치 탐지와 같은 애플리케이션을 알아보자.

### 9.1.1 k-평균

![image](https://github.com/user-attachments/assets/d7a4c098-8d0f-467f-828b-9e385dad6656)

레이블이 없는 데이터셋을 보자. 샘플 덩어리 5개가 잘 보인다. k-ㅠㅕㅇ균은 반복 몇 번으로 이런 종류의 데이터셋을 빠르고 효율적으로 클러스터로 묶을 수 있는 간단한 알고리즘이다. 
이 데이터셋에 k-평균 알고리즘을 훈련해보자. 이 알고리즘은 각 클러스터의 중심을 찾고 가장 가까운 클러스터에 샘플을 할당한다.

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

# 데이터 생성: 5개의 클러스터로 구성된 데이터셋
X, y = make_blobs(n_samples=1500, centers=[(-3, 3), (-3, 1.5), (-2, 2), (0, 2), (1, 2)],
                  cluster_std=[0.2, 0.2, 0.5, 0.7, 0.7], random_state=42)
k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
y_pred = kmeans.fit_predict(X)
# 데이터 시각화
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], s=10, color='navy')
plt.xlabel("$X_1$")
plt.ylabel("$X_2$")
plt.title("샘플 덩어리 다섯 개로 이루어진 레이블 없는 데이터셋")
plt.grid(True)
plt.show()
```
알고리즘이 찾을 클러스터 개수 k를 지정한다. 여기서는 데이터를 보고 k를 5로 지정해야 한다고 알 수 있으나, 일반적으로 어렵다. 
각 샘플은 다섯 개의 클러스터 중 하나에 할당된다. 군집에서 각 샘플의 **레이블**은 알고리즘이 샘플에 할당한 클러스터의 인덱스다. KMeans 클래스의 인스턴스는 labels_ 인스턴스 변수에 훈련된 샘플의 예측 레이블을 가지고 있다.
```python
print (y_pred)
print (y_pred is kmeans.labels_)
```
이 알고리즘이 찾은 센트로이드 다섯 개도 확인해보자.
```python
kmeans.cluster_centers_
```
새로운 샘플에 가장 가까운 센트로이드의 클러스터를 할당할 수 있다.
```python
import numpy as np
X_new = np.array([[0,2],[3,2],[-3,3],[-3,2.5]])
kmeans.predict(X_new)
```
클러스터의 결정 경계를 그려보면 보로노이 다이어그램을 얻을 수 있다.

![image](https://github.com/user-attachments/assets/9c8a6dec-d846-489d-898f-c34faaa23665)

**샘플**은 대부분 적절한 클러스터에 잘 할당되었다. 하지만 샘플 몇 개는 레이블이 잘못 부여되었다. (왼쪽 위 클러스터와 가운데 클러스터의 경계 부근)
실제 k-평균 알고리즘은 클러스터의 크기가 많이 다르면 잘 작동하지 않는다. 샘플을 클러스터에 할당할 때 센트로이드까지 거리를 고려하는 것이 전부이기 때문이다.

샘플을 하나의 클러스터에 할당하는 **하드 군집**보다 클러스터마다 샘플에 점수를 부여하는 **소프트 군집**이 유용할 수 있다. 점수는 샘플과 센트로이드 사이의 거리 또는 유사도 점수 (친화성 점수)가 될 수 있다. KMeans 클래스의 transfrom() 메소드는 샘플과 각 센트로이드 사이의 거리를 반환한다.
```python
kmeans.transform(X_new).round(2)
```
이 결과에서는 하나의 리스트에 5개의 요소가 들어가는데, 각각은 n번째 샘플이 m번째 센트로이드에서 얼마인지를 나타낸다.

**k-평균 알고리즘**
센트로이드가 주어진다고 가정해보자. 데이터셋에 있는 모든 샘플에 가장 가까운 센트로이드의 클러스터를 할당할 수 있다. 반대로 모든 샘플의 레이블이 주어진다면 각 클러스터에 속한 샘플의 평균을 계산하여 모든 센트로이드를 쉽게 구할 수 있다. 
하지만, 레이블이나 센트로이드가 주어지지 않는다면?
처음에는 센트로이드를 랜덤하게 선정한다. 그다음 샘플에 레이블을 할당하고 센트로이드를 업데이트하고, 샘플에 레이블을 할당하고 센트로이드를 업데이트를 하는 식으로 센트로이드에 변화가 없을 때까지 계속한다. 이 알고리즘은 제한된 횟수 안에 수렴하는 것을 보장한다. 샘플과 가장 가까운 센트로이드 사이의 평균 제곱거리는 각 단계마다 내려갈 수만 있고 음수는 없기에 수렴성이 보장된다.

![image](https://github.com/user-attachments/assets/9534bafc-eccd-49f3-98a0-3e830584fef6)

알고리즘의 작동을 보면 위의 그림과 같다. 위 그림은 반복 세 번만에 최적으로 보이는 클러스터에 도달했다.
이 알고리즘의 수렴은 보장되나 지역 최적점으로 수렴하는 등 적절한 솔루션으로 수렴하지 못할 수 있다. 이 여부는 센트로이드 초기화에 달려있다. 아래 그림의 두 가지 예는 랜덤한 초기화 단계에 운이 없을 때 알고리즘이 수렴할 수 있는 최적이 아닌 솔루션을 보여준다.

![image](https://github.com/user-attachments/assets/c64f6e98-2938-419b-88e9-8785be63b6e0)

센트로이드 초기화를 개선하여 이런 위험을 줄일 수 있는 방법을 알아보자.

**센트로이드 초기화 방법**
센트로이드 위치를 근사하게 알 수 있다면 init 매개변수에 센트로이드 리스트를 담은 넘파이 배열을 지정하고 n_init을 1로 설정할 수 있다.
```python
good_init = np.array([[-3,3],[-3,2],[-3,1],[-1,2],[0,2]])
kmeans = KMeans(n_clusters=5, init=good_init, n_init=1, random_state=42)
kmeans.fit(X)
```
또 다른 방법은 랜덤 초기화를 다르게 하여 여러 번 알고리즘을 실행하고 가장 좋은 솔루션을 선택하는 것이다. 랜덤 초기화 횟수는 n_init 매개변수로 조절한다. 기본값은 10이다. 이는 fit() 메소드를 호출할 때 앞ㄷ서 설명한 전체 알고리즘이 10번 실행된다는 뜻이다. 사이킷런은 이 중에 최선의 솔루션을 반환한다. 어떻게? 
사용하는 성능지표가 있다. 이를 모델의 inertia라고 부른다. (각 샘플과 가장 가까운 센트로이드 사이의 제곱거리 합)
KMeans 클래스는 알고리즘을 n_init번 실행하여 이너셔가 가장 낮은 모델을 반환한다.
score() 메소드는 이너셔의 음숫값을 반환한다. 예측기의 score() 메소드는 사이킷런의 '큰 값이 좋은 것이다.'라는 규칙을 따라야하기 때문이다. 

**k-평균 속도 개선과 미니배치 k-평균**
클러스터가 많은 일부 대규모 데이터셋에서 불필요한 거리 계산을 피함으로써 알고리즘의 속도를 높일 수 있는 k-평균 속도 계산을 알아보자. 이를 위해서는 삼각 부등식을 사용한다. (두 점 사이의 직선은 항상 짧은 거리가 됨) 그리고 샘플과 센트로이드 사이의 거리를 위한 하한선과 상한선을 유지한다. 
그러나 이 알고리즘이 항상 훈련 속도를 높일 수 있는 건 아니고, 때로는 훈련 속도가 상당히 느려질 수 있다. 사용해보고 싶다면 algorithm = 'elkan'으로 지정할 것.

k-평균 알고리즘의 또 다른 중요한 변형이 제시되었다. 이 알고리즘은 전체 데이터셋을 사용해 반복하지 않고 각 반복마다 미니배치를 사용해 센트로이드를 조금씩 이동한다. 이는 속도를 높이고, 메모리에 들어가지 않는 대량의 데이터셋에 군집 알고리즘을 적용할 수 있다. 사이킷런의 MiniBatchKMeans 클래스에 이 알고리즘이 구현되어 있다.
```python
from sklearn.cluster import MiniBatchKMeans

minibatch_kmeans = MiniBatchKMeans (n_clusters = 5, random_state = 42) 
minibatch_kmeans.fit(X)
```
데이터셋이 메모리에 들어가지 않는 경우 가장 간단한 방법은 점진적 PCA 방법처럼 memmap클래스를 이용하는 것이다.
또는 MiniBatchKMeans 클래스의 partial_fit() 메소드에 한 번에 하나의 미니배치를 전달할 수 있다. 하지만 초기화를 여러 번 수행하고 만들어진 결과에서 가장 좋은 것을 직접 골라야 해서 할 일이 많다.

미니배치 k-평균 알고리즘이 일반보다는 빨라도 이너셔는 일반적으로 좀 더 나쁘다.

![image](https://github.com/user-attachments/assets/bb44941a-ac25-437e-b8f0-3268b5fc7a64)

**최적의 클러스터 개수 찾기**


