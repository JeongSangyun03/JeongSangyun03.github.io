---
layout : post
title : 3. 서포트 벡터 머신 & 결정 트리
categories : ML 기초
---
# 쿠다 ML 기초 3주차

이 장에서는 서포트 벡터 머신과 결정 트리에 대해 알아보려고 한다.

우선, 서포트 벡터 머신의 핵심 개념과 사용법, 작동 원리를 살펴보겠다. 

서포트 벡터 머신은 선형, 비선형 분류, 회귀, 특이치 탐지에도 사용할 수 있는 다목적 머신러닝 모델이다. 특히 이는 분류 작업에서 빛을 발한다.

## 5.1 선형 SVM 분류
SVM의 기본 아이디어부터 살펴보려고 한다. 저번 주차에서 사용한 붓꽃 데이터셋의 일부를 나타낸 그림을 살펴보자.

![Image-1 (30)](https://github.com/user-attachments/assets/9b578618-cd5d-4c69-9756-0a9c4523561b)

그림을 보면 두 클래스가 직선으로 확실히 잘 나뉘어 있다. 왼쪽 그래프에 세 개의 선형 분류기에서 만들어진 결정 경계가 보인다. 점선으로 나타난 결정 경계를 만든 모델은 클래스를 적절히 분류하지 못하나, 다른 두 모델은 훈련 세트에 대해 완벽하게 작동한다. 하지만 결정 경계가 샘플에 너무 가까워 새 샘플에 대해서는 잘 작동하지 않을 것이다. 오른쪽 그래프의 실선은 SVM 분류기의 결정 경계이다. 이는 두 클래스를 나누고 있고, 제일 가까운 훈련 샘플로부터 가능한 한 멀리 떨어져 있다. 즉, SVM 분류기를 **클래스 사이에 가장 폭이 넓은 도로를 찾는 것**으로 생각할 수 있고, 이를 **라지 마진 분류**라고 한다.

도로 바깥쪽에 훈련 샘플을 더 추가해도 결정 경계에는 영향을 전혀 미치지 않는다. 도로 경계에 위치한 샘플에 의해 전적으로 결정된다. 이를 **서포트 벡터**라고 한다. 

**서포트 벡터 : (그래프의 회색 동그라미) 결정 경계에 가장 가까이 위치한 훈련 샘플들**

SVM은 특성의 스케일에 민감하다. 아래 그림의 왼쪽 그래프에서는 수직축의 스케일이 수평축보다 커서 가장 넓은 도로가 거의 수평에 가깝다. 이를 조정하면 결정 경계가 오른쪽 그래프처럼 훨씬 좋아진다.

![Image-1 (31)](https://github.com/user-attachments/assets/e48832d5-c66e-4df9-968c-052062ca27a2)

### 5.1.1 소프트 마진 분류
모든 샘플이 도로 바깥에 올바르게 분류되어 있다면 이를 **하드 마진 분류**라고 한다. 이 분류에는 두 가지 문제점이 있다. 데이터가 선형적으로 구분되어야 작동하며, 이상치에 민감하다. 아래 그림은 이상치가 하나 있는 붓꽃 데이터셋이다. 왼쪽 그래프에서는 하드 마진을 찾을 수 없다. 오른쪽의 결정 경계는 이상치가 없는, 앞서 언급했던 첫 번째 그림의 결정 경계와 매우 다르고 모델이 일반화 되기 어렵다.

![Image-1 (32)](https://github.com/user-attachments/assets/c3d76fa8-36de-4d19-a777-21ec0cf70347)

어떻게 해야할까? 유연한 모델이 필요하다. 도로의 폭을 가능한 한 넓게 유지하는 것과 샘플이 도로 중간이나 반대에 있는 경우를 말하는 **마진 오류** 사이의 적절한 밸런스가 필요하다. 이를 **소프트 마진 분류**라고 한다.

사이킷런의 SVM 모델에서 여러 하이퍼파라미터를 지정할 수 있는데, 이를 낮게 설정하면 아래 그림의 왼쪽 그래프와 같은 모델을 만들고, 높게 설정하면 오른쪽과 같은 모델을 얻는다. C (하이퍼파라미터)를 줄이면 도로가 더 커지지만 더 많은 마진 오류가 발생한다. C를 줄이면 도로를 지지하는 샘플이 더 많아지므로 과대적합의 위험이 줄어든다. 그러나 너무 많이 줄이면 모델이 과소적합된다. C=100인 모델이 C=1보다 더 잘 일반화될 것이다.

![Image-1 (33)](https://github.com/user-attachments/assets/bb4cf6d1-62c0-4e87-b0cc-355a0017deed)

다음 코드를 살펴보자.

이는 붓꽃 데이터셋을 적재하고, Iris-Virginia 품종을 감지하기 위해 선형 SVM 모델을 훈련시킨다.
```python
from sklearn.datasets import load_iris
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

iris = load_iris(as_frame=True)
X = iris.data[['petal length (cm)','petal width (cm)']].values 
y = (iris.target == 2) 

svm_clf = make_pipeline(StandardScaler(), LinearSVC(C=1,random_state=42))
svm_clf.fit(X,y)
```
이 코드가 위 그림의 왼쪽 그래프이다. (C=1)

그 후, 모델을 사용해 예측을 해보자.

```python
X_new = [[5.5,1.7],[5.0,1.5]]
svm_clf.predict(X_new) 
```

이에 대한 결괏값은 array([ True, False]) 이다.

해석해보자. 첫 번째 꽃은 Iris-Virginica로 분류되지만 두번째는 아니다. SVM이 이러한 예측을 하는 데 사용한 점수를 살펴보자. SVM 모델은 각 샘플과 결정 경계 사이의 거리 (+ or -)를 측정한다.
```python
svm_clf.decision_function(X_new)
```
결과는 다음과 같다.

array([ 0.66163816, -0.22035761])

로지스틱 회귀와 달리 LinearSVC에는 클래스 확률을 추정하는 predict_proba() 메소드가 없다. LinearSVC 대신 SVC 클래스를 사용하고 probability 매개변수를 True로 설정하면 훈련이 끝날 때 SVM 결정함수 점수를 추정확률에 매핑하기 위해 추가적인 모델을 훈련한다. 이후 predict_proba () 및 predict_log_proba () 메소드를 사용할 수 있다.

번외로, 저번 시간 Deep Dive의 내용에 대한 자료 조사를 보충하겠다.

저번 시간에 우리 조는 SVM이 분류 작업에서 각 클래스에 대한 결정 경계를 제공하지만, 확률 분포에 대한 별다른 설계가 되어있지 않아서 분류확률을 제공할 수 없다고 조사했다. 이에 대한 보충 설명을 첨부하겠다.

![image](https://github.com/user-attachments/assets/272f2b59-88bc-4009-8f7c-c98430d2d311)

그러나 주의할 점은 다음과 같다.

![image](https://github.com/user-attachments/assets/b2c05d45-2cfd-427a-ab06-24de9231cb6d)

## 5.2 비선형 SVM 분류

선형적으로 분류할 수 없는 비선형 데이터셋을 다루는 방법은 뭐가 있을까? 앞주차 처럼 다항 특성과 같은 특성을 더 추가하는 방법이 있었다. 아래 그림과 같이 말이다.

![Image-1 (34)](https://github.com/user-attachments/assets/c72de47d-5c69-4522-b083-a98665e3dad4)

사이킷런을 사용하여 이를 구현하기 위해 파이프라인을 만들고, 이를 moons 데이터셋에 적용해보자.

```python
from sklearn.datasets import make_moons
from sklearn.preprocessing import PolynomialFeatures

X, y = make_moons(n_samples = 100, noise = 0.15, random_state = 42)
polynomial_svm_clf = make_pipeline(PolynomialFeatures(degree=3), StandardScaler(), LinearSVC(C=10,max_iter=10_000,random_state=42))
polynomial_svm_clf.fit(X,y)
```
![Image-1 (35)](https://github.com/user-attachments/assets/9218e204-8131-4936-a229-a49d4b4f242e)

### 5.2.1 다항식 커널
낮은 차수의 다항식은 매우 복잡한 데이터셋을 잘 표현하지 못하고 높은 차수의 다항식은 굉장히 많은 특성을 추가하므로 모델을 느리게 만든다.

SVM을 사용할 땐 **커널 트릭**이라는 수학적 기교를 적용할 수 있다. 이는 실제로는 특성을 추가하지 않으면서도 매우 높은 차수의 다항 특성을 많이 추가한 것과 같은 결과를 얻게 해준다. 이는 어떤 특성도 추가하지 않기에 엄청난 수의 특성 조합이 생기지 않는다.
테스트해보자.
```python
from sklearn.svm import SVC
poly_kernel_svm_clf = make_pipeline(StandardScaler(), SVC(kernel="poly", degree=3, coef0=1, C=5))
poly_kernel_svm_clf.fit(X, y)
```
결과는 아래 그림의 왼쪽에 나타냈다. 오른쪽 그림은 10차 다항식 커널을 사용한 또 다른 SVM 분류기이다. 모델이 과대적합이라면 다항식의 차수를 줄여야 한다. 반대로 과소적합이라면 차수를 늘려야 한다. 매개변수 coef0는 모델이 높은 차수와 낮은 차수에 얼마나 영향을 받을지 조절한다.

![Image-1 (36)](https://github.com/user-attachments/assets/9329be9d-badb-42f1-a538-8db305eccfce)

### 5.2.2 유사도 특성
비선형 특성을 다루는 또 다른 기법은 각 샘플이 특정 랜드마크와 얼마나 닮았는지 측정하는 유사도 함수로 계산한 특성을 추가하는 것이다. 

![Image-1 (37)](https://github.com/user-attachments/assets/0c171e50-6ade-4e01-a5d3-a5ec49a547e6)

앞에서 본 1차원 데이터셋에 두 개의 랜드마크 x1=-2, x1=1을 추가하고 감마가 0.3인 가우스 방사 기저 함수 (RBF)를 유사도 함수로 정의해보자. 이 함수의 값은 0 (랜드마크에서 아주 멀리 떨어진 경우)부터 1 (랜드마크와 같은 위치인 경우)까지 변화하며 종 모양으로 나타난다. 

x1=-1 샘플을 살펴보자. 이 샘플은 첫 번째 랜드마크에서 1만큼 떨어져 있고, 두 번째 랜드마크에서 2만큼 떨어져 있다. 그러므로 새로 만든 특성은 x2 = exp(-0.3*1^2) = 0.74, x3 = exp(-0.3*2^2) = 0.30이다. 오른쪽 그래프는 변환된 데이터셋을 보여준다. 이제 선형적으로 구분이 가능하다. 

RBF 변환을 통해 얻을 수 있는 주요한 통찰이 무엇일까?

1. 비선형 데이터를 선형적으로 구분 가능하게 변환
2. 거리에 따른 유사도 변화 (가까울수록 높은 유사도)
3. 고차원 변환을 통해 커널 기반 머신러닝 모델 적용 가능
4. 랜드마크 선택에 따른 영향 분석

**커널 : 낮은 차원의 데이터를 더 높은 차원의 공간으로 변환하여, 비선형적인 데이터도 선형적으로 구분할 수 있도록 해주는 함수**

머신러닝에서 선형 분류기 (선형VM, 로지스틱회귀)는 직선으로 데이터를 구분한다. 하지만 현실에서는 선형적으로 구분되지 않는 데이터들이 많다고 앞에서 언급했다. 이를 고차원으로 변환을 직접하지 않고, 데이터 포인트 간의 유사도를 고차원에서 계산하는 함수인 커널을 사용하면 된다.

랜드마크를 어떻게 선택할까? 간단한 방법은 데이터셋에 있는 모든 샘플 위치에 랜드마크를 설정하는 것인데, 이렇게 하면 차원이 매우 커져 변환된 훈련 세트가 선형적으로 구분될 가능성이 높다. (고차원 공간에서는 데이터가 퍼지면서 선형적으로 구분할 수 있는 가능성이 증가함.)

보충 설명은 아래와 같다.

![image](https://github.com/user-attachments/assets/7ba759d6-a05c-4c88-b08b-8b625110b900)

![image](https://github.com/user-attachments/assets/4677a1eb-702c-4401-964d-1808dcab8780)

![image](https://github.com/user-attachments/assets/d2e38db0-7f5d-4ae2-8f18-c84cd8e204cb)

### 5.2.3 가우스 RBF 커널
유사도 특성 방식도 머신러닝 알고리즘에 유용하게 사용될 수 있다. 추가 특성을 모두 계산하려면 훈련 세트가 큰 경우에 연산 비용이 많이 든다. 여기서 커널 트릭이 한 번 더 SVM의 마법을 만든다. 마치 유사도 특성을 많이 추가하는 것과 같은 비슷한 결과를 얻을 수 있다. 가우스 RBF 커널을 사용한 SVC 모델을 시도해보자.

```python
rbf_kernel_svm_clf = make_pipeline(StandardScaler(), SVC(kernel = 'rbf', gamma = 5, C=0.001))
rbf_kernel_svm_clf.fit(X,y)
```
아래 그림의 왼쪽 아래 그래프에 이 모델을 나타내었다. 다른 그래프들은 하이퍼파라미터 감마와 C를 바꾸어 훈련시킨 모델이다. 살펴보자.

![Image-1 (38)](https://github.com/user-attachments/assets/b6269208-e6ab-4310-8238-afbcd0997e75)

gamma를 증가시키면 종 모양 그래프가 좁아져서 각 샘플의 영향 범위가 작아진다. 결정 경계가 조금 더 불규칙해지고 각 샘플을 따라 구불구불하게 휘어진다. 

gamma를 감소시키면 넓은 종모양 그래프를 만들며 샘플이 넓은 범위에 걸쳐 영향을 주므로 결정경계가 더 부드러워진다. 

결국, 하이퍼파라미터 gamma가 규제의 역할을 한다. 모델이 과대적합일 경우에는 감소시켜야 하고, 과소적합일 경우에는 증가시켜야 한다.

어떤 커널은 특정 데이터 구조에 특화되어 있다. 

**문자열 커널**이 가끔 텍스트 문서나 DNA 서열을 분류할 때 사용된다.

### 5.2.4 계산 복잡도
LinearSVC 파이썬 클래스는 선형 SVM을 위한 최적화된 알고리즘을 구현한 liblinear 라이브러리를 기반으로 한다. 이 라이브러리는 커널 트릭을 지원하지 않지만 훈련 샘플과 특성 수에 거의 선형적으로 늘어난다. 이 알고리즘의 훈련 시간 복잡도는 대략 O(m*n)이다. 

![image](https://github.com/user-attachments/assets/b8431c2f-27ee-4cda-8204-4268be8f278a)

정밀도를 높이면 알고리즘의 수행 시간이 길어진다. 이는 허용 오차 하이퍼파라미터로 조절한다. (사이킷런의 매개변수는 tol). 대부분의 분류 문제는 허용 오차를 기본값으로 두면 잘 작동한다.

SVC는 커널 트릭 알고리즘을 구현한 libsvm 라이브러리를 기반으로 한다. 훈련의 시간 복잡도는 보통 O(m^2*n)과 O(m^3*n) 사이다. 불행하게도 이는 훈련 샘플 수가 커지면 엄청나게 느려진다. 따라서 중소규모의 비선형 훈련 세트에 이 알고리즘이 잘 맞는다. 하지만 특성의 수에 대해서는, 특히 희소 특성인 경우에는 잘 확장된다. 이런 경우 알고리즘의 성능이 샘플이 가진 특성의 평균 수에 거의 비례한다.

SGDClassifier 클래스는 기본적으로 라지 마진 분류를 수행하여 하이퍼파라미터, 특히 규제 하이퍼파라미터와 learning_rate를 조정하여 선형 SVM과 유사한 결과를 생성할 수 있다. 훈련을 위해 점진적 학습이 가능하고, 메모리를 거의 사용하지 않는 확률적 경사 하강법을 사용하므로 대규모 데이터셋에서 모델을 훈련할 수 있다. 

분류에 대해 설명했고, 이제 SVM 알고리즘을 선형 및 비선형 회귀에 어떻게 사용할 수 있는지 알아보자.

## 5.3 SVM 회귀
목표를 바꿔보자. 일정한 마진 오류 안에서 (일부 샘플이 마진을 침범하거나 결정 경계를 잘못된 위치에 두는 경우, margin = 여백) 도로 안에 가능한 한 많은 샘플이 들어가도록 학습한다. 도로의 폭은 하이퍼파라미터 ε로 조절한다.

![image](https://github.com/user-attachments/assets/f05df24c-2b80-4b5a-a018-2160fb713261)

분류와 회귀에 사용되는 하이퍼파라미터가 다르다는 것을 인지하자.

아래의 그림은 랜덤으로 생성한 선형 데이터셋에 훈련시킨 두 개의 선형 SVM 회귀 모델이다. 하나는 마진을 작게, 다른 하나는 마진을 크게 만들었다.

![Image-1 (39)](https://github.com/user-attachments/assets/2229101d-54b1-451d-b228-278ec15807b1)

엡실론을 줄이면 서포트 벡터의 수가 늘어나서 모델이 규제된다. 마진 안에서는 훈련 샘플이 추가되어도 모델의 예측에는 영향이 없다. 그래서 이 모델을 엡실론에 민감하지 않다고 말한다. 

사이킷런의 LinearSVR을 사용해 선형 SVM 회귀를 적용해보겠다. 다음 코드는 위 그림의 왼쪽 그래프에 해당하는 모델을 만든다.
```python
from sklearn.svm import LinearSVR
svm_reg = make_pipeline (StandardScaler(), LinearSVR(epsilon=1.5, random_state=42))
svm_reg.fit(X,y)
```

비선형 회귀 작업을 처리하려면 커널 SVM 모델을 사용한다. 아래 그림은 임의의 2차 방정식 형태의 훈련세트에 2차 다항 커널을 사용한 SVM 회귀를 보여준다. 왼쪽 그래프는 규제가 약간 있고, 오른쪽 그래프는 규제가 훨씬 적다. (C의 차이)

엡실론은 도로의 너비라고 생각하자. C는 도로 바깥에 있는 샘플을 얼마나 허용할 것인지를 결정한다고 생각하자. 마치 도로를 벗어난 차들에 대한 벌금의 정도를 나타낸다고 생각하면 된다.

![Image-1 (40)](https://github.com/user-attachments/assets/638ef933-efc7-4184-99b6-3d93082cd322)

아래의 코드는 사이킷런의 SVR을 이용해 위 그림의 왼쪽 그래프에 해당하는 모델을 만든다.
```python
from sklearn.svm import SVR
svm_poly_reg = make_pipeline(StandardScaler(), SVR(kernel="poly", degree=2, C=0.01, epsilon=0.1))
svm_poly_reg.fit(X,y)
```

SVR은 SVC의 회귀버전이고, LinearSVR은 LinearSVC의 회귀 버전이다. LinearSVR은 필요한 시간이 훈련 세트의 크기에 비례해서 선형적으로 늘어난다. 하지만 SVR은 훈련 세트가 커지면 훨씬 느려진다.

이번에는 결정 트리에 대해 알아보겠다.

**결정 트리**는 분류와 회귀 작업, 그리고 다중 출력 작업까지 가능한 다목적 머신러닝 알고리즘이다. 또한 매우 복잡한 데이터셋도 학습할 수 있는 강력한 알고리즘이다. (DecisionTree)

결정 트리는 최근에 자주 사용되는 강력한 머신러닝 알고리즘인 랜덤 포레스트의 기본 구성 요소이기도 하다. 

이 장에서는 결정 트리의 훈련, 시각화, 예측 방법을 먼저 살펴보려고 한다. 그 후 사이킷런의 CART 훈련 알고리즘을 둘러보고 트리에 규제를 가하는 방법과 회귀 문제에 적용하는 방법을 배우겠다. 마지막으로는 결정 트리의 제약 사항에 관해 이야기 하겠다.

## 6.1 결정 트리 학습과 시각화

지난 번에 이용했던 붓꽃 데이터셋을 이용하여 DecisionTreeClassifier을 훈련하여, 어떻게 예측을 하는지 살펴보겠다.

```python
from sklearn.datasets import load_iris
from sklearn.tree import DecisionTreeClassifier
iris = load_iris(as_frame = True)
X_iris = iris.data [['petal length (cm)','petal width (cm)']].values
y_iris = iris.target

tree_clf = DecisionTreeClassifier (max_depth = 2, random_state = 42)
tree_clf.fit(X_iris,y_iris) 
```
이를 export_graphviz () 함수를 이용하여 그래프 정의를 파일로 출력하여 훈련된 결정 트리를 시각화할 수 있다.
이에 대한 결괏값은 다음과 같다.

![image](https://github.com/user-attachments/assets/c5fb8972-35da-4091-8f4b-a35999ca4c5c)

## 6.2 예측

![Image-1 (43)](https://github.com/user-attachments/assets/307f06a5-1286-4fb7-a619-9497e4ef20af)

위의 결괏값에 각 단계를 나타낸 새로운 그림 자료이다. 이를 토대로 트리가 어떻게 예측을 만들어내는지 살펴보겠다.꽃잎을 기준으로 하여 새로 발견한 붓꽃의 품종을 분류하려 한다고 가정하겠다.

먼저, **루트 노드** (깊이가 0인 맨 꼭대기의 노드)에서 시작한다. 이 노드는 꽃잎의 길이가 2.45cm보다 짧은지 검사한다. 만약 참이라면 루ㅡ 노드에서 왼쪽의 자식 노드 (깊이1, 왼쪽 노드)로 이동한다. 

이 경우, 이 노드가 **리프 노드** (자식 노드를 가지지 않는 노드)이므로 추가적인 검사를 하지 않는다. 그저 품종을 Iris-Setosa라고 예측한다.

반대로, 2.45cm보다 길다면 오른쪽 자식노드로 이동 => 이는 리프 노드가 아니고 분할 노드이다. 따라서 여기서 추가적으로 1.75cm보다 작은지 검사한다. 여기서도 크기에 따라 리프 노드로 이동하여 종료된다.

**용어 정리**
노드 : 트리 구조에서 각각의 분기점 또는 종착점을 의미한다. 트리는 계층적인 구조를 가지며, 머신러닝의 의사결정트리나 랜덤 포레스트 같은 알고리즘에서 데이터를 분류하고 예측하는 데 활용된다.

value 속성 : 

설명 : 해당 노드가 가지고 있는 클래스별 샘플 개수를 나타냄

형식 : 배열 형태의 값으로, 각 클래스 (레이블)에 속한 샘플의 수를 저장

ex : 결정 트리에서 특정 노드가 어느 클래스에 더 많은 샘플을 포함하고 있는지 확인

sample 속성 :

설명 : 해당 노드에 존재하는 전체 샘플의 개수

형식 : 단일 정수 값 (해당 노드에 속한 데이터 포인트의 수)

ex : 특정 노드에서 얼마나 많은 샘플을 포함하고 있는지 확인할 때

gini 속성 :

gini 속성은 지니 불순도를 나타내며, 해당 노드가 얼마나 혼합된 상태인지를 측정한다.

지니 불순도는 0~1의 값을 가지며, 값이 작을수록 해당 노드가 더 순수하다는 의미이다.

특정 노드에서 샘플들이 하나의 클래스로만 이루어져 있다면, 지니 불순도 = 0 (완전 순수한 노드)가 된다.

반대로 여러 클래스가 비슷한 비율로 섞여 있을수록, 지니 불순도 값이 커진다.

![image](https://github.com/user-attachments/assets/088e4490-111c-41b9-879a-9bac88c6cc2a)

이 결정 트리의 결정 경계를 나타내보았다.

![Image-1 (44)](https://github.com/user-attachments/assets/811a824f-a254-41a8-85bf-c725c9f4f3f3)

여기서 굵은 수직선은 루트노드 (깊이0)의 결정경계 (꽃잎 길이 2.45cm)이다. 왼쪽은 클래스 1개인 순수 노드이므로, 나눌 수 없고, 오른쪽 영역은 순수노드가 아니므로 파선에 의해 꽃잎 너비 1.75cm에서 나눠진다. max_depth를 더 키우면 깊이 2의 두 노드가 결정경계를 추가로 만들어 두 개의 수직 점선이 형성된다.

**모델 해석 : 화이트박스와 블랙박스**

화이트박스 : 직관적이고 결정 방식을 이해하기 쉬움 (해석가능한 ML) => 결정 트리 (=사람이 이해할 수 있는 방식으로 모델의 결정을 설명할 수 있는 ML 시스템을 만드는 것)

블랙박스 : 성능 뛰어남 + 예측을 만드는 연산 과정 쉽게 확인 가능, 그러나 예측이 일어나게 된 경위를 파악하기 어려움
=> 랜덤 포레스트, 신경망

## 6.3 클래스 확률 추정

결정 트리는 한 샘플이 특정 클래스 k에 속할 확률을 추정할 수도 있다. 

1. 샘플의 리프 노드를 찾기 위해 트리를 탐색한다.
2. 그 노드에 있는 클래스 k의 훈련 샘플 비율을 반환한다.

ex : 길이 5cm, 너비 1.5cm인 꽃잎을 발견했다.
==> 위의 트리 구조에서, 1.75cm보다 작은 54개의 샘플. 깊이 2의 왼쪽노드를 탐색 -> 해당 확률 출력

=> Iris-Setosa 0% (0/54), Iris-Versicolor 90.7% (49/54), Iris-Virginica 9.3% (5/54)

여기서 가장 높은 확률을 가진 Iris-Versicolor 클래스 1을 출력할 것이다.

```python
print (tree_clf.predict_proba([[5,1.5]]).round(3))
print (tree_clf.predict([[5,1.5]]))
```
이에 대한 결괏값은 아래와 같다.

![image](https://github.com/user-attachments/assets/56a8c952-6900-48b2-ac08-08b2d5d99ce8)

참고로, 일반적으로 사이킷런의 결정 트리 모델에서는 클래스 라벨이 0부터 시작한다.

## 6.4 CART 훈련 알고리즘
사이킷런은 CART 알고리즘을 사용하여 결정 트리를 훈련시킨다.

이는 하나의 특성 k의 임곗값 tk를 사용해 두 개의 서브셋으로 나눈다. 

ex : k = 꽃잎의 길이, tk = 2.45cm ==> k<=tk, k>tk

이 k와 tk를 찾으려면 가장 순수한 서브셋으로 나눌 수 있어야 하고, 이는 비용함수의 최소화가 필요하다.

그 식이 아래와 같다.

![Image-1 (45)](https://github.com/user-attachments/assets/361e38a8-117e-4503-9e82-c61a23adb5d2)

이 과정이 성공했다면, 같은 방식으로 서브셋을 또 나누고, 또 나누고, 반복한다. 이는 최대 깊이가 되거나 불순도를 줄이는 분할을 찾을 수 없을 때 종료된다. 다른 몇 개의 매개변수도 중지 조건에 관여한다. 후에 살펴보자.

## 6.5 계산 복잡도
예측을 하려면 결정 트리를 루트 노드에서부터 리프 노드까지 탐색해야 한다. 이를 위해서는 약 O(log2(m))개의 노드를 거쳐야 한다. 

각 노드는 하나의 특성값만 확인하기 때문에 예측에 필요한 전체 복잡도는 특성 수와 무관하다.

훈련 알고리즘은 각 노드에서 모든 훈련 샘플의 모든 특성을 비교한다. 각 노드에서 모든 샘플의 모든 특성을 비교하면 훈련 복잡도는 O(n*log2(m))이 된다.

## 6.6 지니 불순도 또는 엔트로피?

DecisionTreeClassifier 클래스는 지니 불순도를 사용하는 게 기본이지만 엔트로피 불순도를 사용하기도 한다. 두 개 모두 큰 차이가 없다. 즉, 둘 다 비슷한 트리를 만들어낸다. *(criterion = 'entropy')로 지정*

지니 불순도가 계산이 더 빠르기에 기본값으로 좋으나, 다른 트리가 만들어지는 경우 가장 빈도 높은 클래스를 한쪽 가지로 고립시키는 경향이 있다. 반면, 엔트로피 불순도는 조금 더 균형 잡힌 트리를 만든다.

엔트로피는 무질서도이다. 이 개념이 정보이론에도 연결된다. 어떤 세트가 한 클래스의 샘플만 담는다면 엔트로피가 0이다. 식을 살펴보자.

![image](https://github.com/user-attachments/assets/bd099a5b-89b4-4d37-8622-72aca53ed52a)

## 6.7 규제 매개변수
결정 트리는 훈련 데이터에 대한 제약 사항이 거의 없다. 그러나, 제한을 두지 않으면 트리가 훈련 데이터에 아주 가깝게 맞추려고 해서 과대적합되기 쉽다. 이 의미가 무엇일까? **결정 트리는 데이터를 완벽하게 분리하려고 한다. 그렇다 보니, 각 개별 샘플을 완벽하게 분리하는 기준을 찾으려 하다 보면, 복잡한 분할이 새로운 데이터에서도 잘 작동할지 보장할 수 없어 과대적합의 위험이 커진다.** 또한, **노드가 계속 분할되면서 너무 세부적인 패턴을 학습한다. 이는 각 리프 노드에 남는 샘플 수가 극단적으로 줄어들어, 새로운 데이터에서 일반화되지 않는 과대적합의 정의 문제를 초래한다.**

결정 트리는 모델 파라미터가 있다. 근데, 훈련되기 전에 파라미터 수가 결정되지 않아 **비파라미터 모델**이라고 한다. 그래서 모델 구조가 데이터에 맞춰져서 고정되지 않고 자유롭다. 반면, 선형 모델 같은 **파라미터 모델**은 모델 파라미터 수가 정해져 있어 ***자유도가 제한***된다. 그래서 선형 모델에서는 과대 적합 위험이 줄어든다.

자, 그럼 결정 트리에 대한 규제가 필요해보인다. 규제 매개변수는 사용하는 알고리즘에 따라 다르나, 결정 트리 최대 깊이는 max_depth로 제어 가능하다. 기본값은 제한이 없는 None이다. 이를 이용하면 모델 규제가 가능하며 과대적합의 위험이 감소한다. 결정트리 형태를 제한하는 다른 하이퍼파라미터 몇 개를 알아보자.

![Image-1 (46)](https://github.com/user-attachments/assets/3e9c631e-4a33-496a-aa58-688d2c483757)

min 시작 매개변수를 증가시키거나 max 시작 매개변수를 감소시키면 모델에 대한 규제가 커진다.

moons 데이터셋에서 규제를 해보자.

1. 규제 없이 결정 트리 훈련
2. min_samples_leaf = 5로 다른 결정 트리 훈련 (리프 노드가 생성되기 위해 가지고 있어야 할 최소 샘플 5)

```python
from sklearn.datasets import make_moons
X_moons, y_moons = make_moons (n_samples = 150, noise = 0.2, random_state = 42) 

tree_clf1 = DecisionTreeClassifier (random_state = 42)
tree_clf2 = DecisionTreeClassifier (min_samples_leaf=5,random_state = 42)  
tree_clf1.fit(X_moons,y_moons)
tree_clf2.fit(X_moons,y_moons)
```
이에 대한 결괏값을 아래 그림으로 시각화해보았다.

![Image-1 (47)](https://github.com/user-attachments/assets/76b6d647-3093-4977-9a93-5133085eb0a4)

딱 봐도 규제 없는 왼쪽 모델이 과대적합되는 걸 확인할 수 있다. 이를 평가해서 확인해보자.

```python
X_moons_test, y_moons_test = make_moons (n_samples = 1000, noise = 0.2, random_state = 43)
print (tree_clf1.score(X_moons_test,y_moons_test))
print (tree_clf2.score(X_moons_test,y_moons_test))
```
각각 0.898과 0.92가 나온다. 2 모델이 훨씬 점수가 높다는 것을 확인할 수 있다.

## 6.8 회귀

결정트리는 회귀 문제에도 사용된다.

사이킷런의 결정트리를 사용해 잡음이 섞인 2차 함수 형태의 데이터셋에서 max_depth = 2 설정으로 회귀 트리를 만들어보자.

```python
import numpy as np
from sklearn.tree import DecisionTreeRegressor
X_quad = np.random.rand(200,1) - 0.5 # 랜덤한 하나의 입력 특성
y_quad = X_quad ** 2 + 0.025 * np.random.randn (200,1)
tree_reg = DecisionTreeRegressor (max_depth=2, random_state=42)
tree_reg.fit(X_quad,y_quad) 
```
이에 대한 트리는 다음과 같이 만들어진다.

![Image-1 (48)](https://github.com/user-attachments/assets/7eec1d97-f89f-4fb2-afd7-41648e8d13c6)

앞서 만든 분류 트리와 매우 비슷하나, 차이는 클래스를 예측하지 않고 어떤 값을 예측한다는 것이다. 이 예측값을 사용하여 노드 안의 훈련 샘플에 대한 평균 제곱 오차를 계산한다.

위 모델의 예측을 아래 그림에 나타내었다.

![Image-1 (49)](https://github.com/user-attachments/assets/0721e61c-f9e1-4e9d-af46-04704900d2c9)

각 영역의 예측값은 항상 그 영역의 타깃값 평균이 된다. 알고리즘은 예측값과 가능한 한 많은 샘플이 가까이 있도록 영역을 분할한다.

CART 알고리즘은 훈련 세트를 불순도 최소화의 방향으로 분할하는 대신 MSE를 최소화하도록 분할하는 것을 제외하면 앞의 설명과 비슷하게 작동한다. (두 개의 서브셋으로 분할)

![Image-1 (50)](https://github.com/user-attachments/assets/5c7d005f-0e8f-4f5e-9960-d00730e031be)

위 식은 CART 알고리즘의 최소화 비용함수 식을 나타낸 것이다.

분류와 마찬가지로 회귀도 규제가 없다면 과대적합되기 쉽다. 아래 그림을 보자.

![Image-1 (51)](https://github.com/user-attachments/assets/95a7050d-a737-4598-a934-5fc555742a84)

왼쪽은 예측값이 너무 많다. 음, 확실히 과대적합이다. 이를 규제하면 오른쪽 그래프와 같다.

정리를 해보자. 분류와 회귀의 목적이 무엇일까? 둘 다 예측이다. 

분류의 목적은, 입력 데이터가 속하는 범주를 예측하는 것이다. 즉, 명확하게 구분된 그룹 중 하나를 선택하는 문제를 해결하는 것이 목표이다.

ex : 30세, 178cm, 75kg이면 운동 가능할까? (가능/불가능)

분류에서 과대적합 발생 시 문제점은 다음과 같다. 

![image](https://github.com/user-attachments/assets/9d4bd349-11b2-4ba6-b1c5-abdc90e4aeac)

회귀의 목적은, 연속적인 수치 값을 예측하는 것이다. 즉, 입력 데이터를 기반으로 숫자 값을 출력하는 것이 목표이다.

ex : 결정 트리 회귀의 목표는 예측 값 (예 : 3600만원)을 추정하는 것 

회귀에서 과대적합 발생 시 문제점은 다음과 같다.

![image](https://github.com/user-attachments/assets/c3c56e4a-6d0f-4c4c-b85f-2e7c49ca90f1)

## 6.8 축 방향에 대한 민감성

결정 트리는 앞서 본 것과 같이 장점이 많지만, 몇 가지 제한 사항이 있다. 이는 계단 모양의 결정 경계를 만든다. (모든 분할은 축에 수직). 그래서 데이터 방향에 민감하다. 그림을 보자.

![Image-1 (52)](https://github.com/user-attachments/assets/f292af02-0a50-442f-83e9-f29a4901a7a8)

왼쪽은 쉽게 데이터셋을 구분하지만, 오른쪽 결정 트리는 결정경계가 불필요하게 구불구불해져 잘 일반화될 것 같지 않다.

(두 결정 트리 모두 훈련 세트를 완벽하게 학습한다.)

어떻게 해결할까?

=> 데이터의 스케일을 조정한 후, 주성분 분석 (PCA) 변환을 적용

주성분 분석은 후에 자세히 알아볼 예정이다. 특성 간의 상관관계를 줄이는 방식으로 데이터를 회전하여 결정 트리를 더 쉽게 만들 수 있는 것에 주안점을 두자.

방법 :

데이터 스케일 조정 + PCA 사용 => 작은 파이프라인 생성 => DecisionTreeClassifier 훈련

```python
from sklearn.decomposition import PCA
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

pca_pipeline = make_pipeline(StandardScaler(), PCA())
X_iris_rotated = pca_pipeline.fit_transform(X_iris) 
tree_clf_pca = DecisionTreeClassifier(max_depth=2, random_state=42)
tree_clf_pca.fit(X_iris_rotated, y_iris)
```
시각화 그림을 살펴보면, 결정 트리의 결정 경계를 보여준다. 여기서 볼 수 있듯이 회전을 통해 원래 꽃잎 길이와 선형 함수인 x1 특성 하나만 사용하여 데이터셋을 매우 잘 학습할 수 있다.

## 6.10 결정 트리의 분산 문제

결정 트리의 주요 문제는 분산이 상당히 크다는 점이다. 즉, 하이퍼파라미터나 데이터를 조금만 변경해도 매우 다른 모델이 생성될 수 있다. 사이킷런에서 사용하는 훈련 알고리즘은 확률적이므로, (각 노드에서 평가할 특성 집합 랜덤 선택) 정확히 동일한 데이터에서 동일한 결정 트리를 재훈련하더라도 아래와 같이 매우 다른 모델이 생성될 수 있다.

![Image-1 (54)](https://github.com/user-attachments/assets/c306a8c5-a573-48f1-a3d3-c99f5a730a7c)

음, 여러 결정 트리의 예측을 평균하면 분산을 크게 줄일 수 있다. 이러한 결정 트리의 앙상블을 **랜덤 포레스트**라고 한다. 아주 강력한 종류의 모델이다.

# 3주차 DEEP DIVE
## 1. 커널 기법
### 1-1. 다항식 커널과 RBF 커널의 차이는 무엇인가? 각각의 정확한 개념과  둘 사이의 유사점, 차이점은 무엇인가?
=>
### 1-2. 다항식 커널과 RBF 커널의 하이퍼파라미터 각각 d와 γ은 커질수록 오버피팅을 야기할 수 있다. 그 이유는 무엇인가?

## 2. 가장 좋은 하이퍼파라미터
### 2-1. 하이퍼파라미터를 튜닝하는 전통적인 방식에는 어떤 방법들이 있으며 이들은 어떤 로직으로 작동할까?
=>
### 2-2. AutoML이란 무엇이고, 이것이 항상 전통적인 방식의 하이퍼파라미터 튜닝보다 좋을까?
=>
### 2-3. 항상 그렇지 않다면 언제 그런지, 그리고 그 이유는 무엇일까?

## 3. 나무를 잘 기르는 법
### 3-1. 나무의 깊이는 오버피팅과 직결된다. 이를 방지하기 위한 방식 중 프루닝은 무엇이며, 프리 프루닝과 포스트 프루닝의 차이는 무엇인가?
=>
### 3-2. 앙상블 기법이 무엇인가? 이 기법이 결정트리에 끼치는 효용과, 그 결과로 볼 수 있는 모델들을 살펴보자.
=>
### 3.3 탐욕 알고리즘은 트리 계열 모델의 핵심이다. 그렇다면 다른 머신러닝 모델들은 탐욕 알고리즘을 사용하지 않을까? 어떤 모델들이 탐욕 알고리즘을 사용하는지, 그 이유와 특징을 살펴보자.












