---
layout : post
title : 3. 서포트 벡터 머신 & 결정 트리
categories : ML 기초
---
# 쿠다 ML 기초 3주차

이 장에서는 서포트 벡터 머신과 결정 트리에 대해 알아보려고 한다.

우선, 서포트 벡터 머신의 핵심 개념과 사용법, 작동 원리를 살펴보겠다. 

서포트 벡터 머신은 선형, 비선형 분류, 회귀, 특이치 탐지에도 사용할 수 있는 다목적 머신러닝 모델이다. 특히 이는 분류 작업에서 빛을 발한다.

## 5.1 선형 SVM 분류
SVM의 기본 아이디어부터 살펴보려고 한다. 저번 주차에서 사용한 붓꽃 데이터셋의 일부를 나타낸 그림을 살펴보자.

![Image-1 (30)](https://github.com/user-attachments/assets/9b578618-cd5d-4c69-9756-0a9c4523561b)

그림을 보면 두 클래스가 직선으로 확실히 잘 나뉘어 있다. 왼쪽 그래프에 세 개의 선형 분류기에서 만들어진 결정 경계가 보인다. 점선으로 나타난 결정 경계를 만든 모델은 클래스를 적절히 분류하지 못하나, 다른 두 모델은 훈련 세트에 대해 완벽하게 작동한다. 하지만 결정 경계가 샘플에 너무 가까워 새 샘플에 대해서는 잘 작동하지 않을 것이다. 오른쪽 그래프의 실선은 SVM 분류기의 결정 경계이다. 이는 두 클래스를 나누고 있고, 제일 가까운 훈련 샘플로부터 가능한 한 멀리 떨어져 있다. 즉, SVM 분류기를 **클래스 사이에 가장 폭이 넓은 도로를 찾는 것**으로 생각할 수 있고, 이를 **라지 마진 분류**라고 한다.

도로 바깥쪽에 훈련 샘플을 더 추가해도 결정 경계에는 영향을 전혀 미치지 않는다. 도로 경계에 위치한 샘플에 의해 전적으로 결정된다. 이를 **서포트 벡터**라고 한다. 

**서포트 벡터 : (그래프의 회색 동그라미) 결정 경계에 가장 가까이 위치한 훈련 샘플들**

SVM은 특성의 스케일에 민감하다. 아래 그림의 왼쪽 그래프에서는 수직축의 스케일이 수평축보다 커서 가장 넓은 도로가 거의 수평에 가깝다. 이를 조정하면 결정 경계가 오른쪽 그래프처럼 훨씬 좋아진다.

![Image-1 (31)](https://github.com/user-attachments/assets/e48832d5-c66e-4df9-968c-052062ca27a2)

### 5.1.1 소프트 마진 분류
모든 샘플이 도로 바깥에 올바르게 분류되어 있다면 이를 **하드 마진 분류**라고 한다. 이 분류에는 두 가지 문제점이 있다. 데이터가 선형적으로 구분되어야 작동하며, 이상치에 민감하다. 아래 그림은 이상치가 하나 있는 붓꽃 데이터셋이다. 왼쪽 그래프에서는 하드 마진을 찾을 수 없다. 오른쪽의 결정 경계는 이상치가 없는, 앞서 언급했던 첫 번째 그림의 결정 경계와 매우 다르고 모델이 일반화 되기 어렵다.

![Image-1 (32)](https://github.com/user-attachments/assets/c3d76fa8-36de-4d19-a777-21ec0cf70347)

어떻게 해야할까? 유연한 모델이 필요하다. 도로의 폭을 가능한 한 넓게 유지하는 것과 샘플이 도로 중간이나 반대에 있는 경우를 말하는 **마진 오류** 사이의 적절한 밸런스가 필요하다. 이를 **소프트 마진 분류**라고 한다.

사이킷런의 SVM 모델에서 여러 하이퍼파라미터를 지정할 수 있는데, 이를 낮게 설정하면 아래 그림의 왼쪽 그래프와 같은 모델을 만들고, 높게 설정하면 오른쪽과 같은 모델을 얻는다. C (하이퍼파라미터)를 줄이면 도로가 더 커지지만 더 많은 마진 오류가 발생한다. C를 줄이면 도로를 지지하는 샘플이 더 많아지므로 과대적합의 위험이 줄어든다. 그러나 너무 많이 줄이면 모델이 과소적합된다. C=100인 모델이 C=1보다 더 잘 일반화될 것이다.

![Image-1 (33)](https://github.com/user-attachments/assets/bb4cf6d1-62c0-4e87-b0cc-355a0017deed)

다음 코드를 살펴보자.

이는 붓꽃 데이터셋을 적재하고, Iris-Virginia 품종을 감지하기 위해 선형 SVM 모델을 훈련시킨다.
```python
from sklearn.datasets import load_iris
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.svm import LinearSVC

iris = load_iris(as_frame=True)
X = iris.data[['petal length (cm)','petal width (cm)']].values 
y = (iris.target == 2) 

svm_clf = make_pipeline(StandardScaler(), LinearSVC(C=1,random_state=42))
svm_clf.fit(X,y)
```
이 코드가 위 그림의 왼쪽 그래프이다. (C=1)

그 후, 모델을 사용해 예측을 해보자.

```python
X_new = [[5.5,1.7],[5.0,1.5]]
svm_clf.predict(X_new) 
```

이에 대한 결괏값은 array([ True, False]) 이다.

해석해보자. 첫 번째 꽃은 Iris-Virginica로 분류되지만 두번째는 아니다. SVM이 이러한 예측을 하는 데 사용한 점수를 살펴보자. SVM 모델은 각 샘플과 결정 경계 사이의 거리 (+ or -)를 측정한다.
```python
svm_clf.decision_function(X_new)
```
결과는 다음과 같다.

array([ 0.66163816, -0.22035761])

로지스틱 회귀와 달리 LinearSVC에는 클래스 확률을 추정하는 predict_proba() 메소드가 없다. LinearSVC 대신 SVC 클래스를 사용하고 probability 매개변수를 True로 설정하면 훈련이 끝날 때 SVM 결정함수 점수를 추정확률에 매핑하기 위해 추가적인 모델을 훈련한다. 이후 predict_proba () 및 predict_log_proba () 메소드를 사용할 수 있다.

번외로, 저번 시간 Deep Dive의 내용에 대한 자료 조사를 보충하겠다.

저번 시간에 우리 조는 SVM이 분류 작업에서 각 클래스에 대한 결정 경계를 제공하지만, 확률 분포에 대한 별다른 설계가 되어있지 않아서 분류확률을 제공할 수 없다고 조사했다. 이에 대한 보충 설명을 첨부하겠다.

![image](https://github.com/user-attachments/assets/272f2b59-88bc-4009-8f7c-c98430d2d311)

그러나 주의할 점은 다음과 같다.

![image](https://github.com/user-attachments/assets/b2c05d45-2cfd-427a-ab06-24de9231cb6d)

## 5.2 비선형 SVM 분류

선형적으로 분류할 수 없는 비선형 데이터셋을 다루는 방법은 뭐가 있을까? 앞주차 처럼 다항 특성과 같은 특성을 더 추가하는 방법이 있었다. 아래 그림과 같이 말이다.

![Image-1 (34)](https://github.com/user-attachments/assets/c72de47d-5c69-4522-b083-a98665e3dad4)

사이킷런을 사용하여 이를 구현하기 위해 파이프라인을 만들고, 이를 moons 데이터셋에 적용해보자.

```python
from sklearn.datasets import make_moons
from sklearn.preprocessing import PolynomialFeatures

X, y = make_moons(n_samples = 100, noise = 0.15, random_state = 42)
polynomial_svm_clf = make_pipeline(PolynomialFeatures(degree=3), StandardScaler(), LinearSVC(C=10,max_iter=10_000,random_state=42))
polynomial_svm_clf.fit(X,y)
```
![Image-1 (35)](https://github.com/user-attachments/assets/9218e204-8131-4936-a229-a49d4b4f242e)

### 5.2.1 다항식 커널
낮은 차수의 다항식은 매우 복잡한 데이터셋을 잘 표현하지 못하고 높은 차수의 다항식은 굉장히 많은 특성을 추가하므로 모델을 느리게 만든다.

SVM을 사용할 땐 **커널 트릭**이라는 수학적 기교를 적용할 수 있다. 이는 실제로는 특성을 추가하지 않으면서도 매우 높은 차수의 다항 특성을 많이 추가한 것과 같은 결과를 얻게 해준다. 이는 어떤 특성도 추가하지 않기에 엄청난 수의 특성 조합이 생기지 않는다.
테스트해보자.
```python
from sklearn.svm import SVC
poly_kernel_svm_clf = make_pipeline(StandardScaler(), SVC(kernel="poly", degree=3, coef0=1, C=5))
poly_kernel_svm_clf.fit(X, y)
```
결과는 아래 그림의 왼쪽에 나타냈다. 오른쪽 그림은 10차 다항식 커널을 사용한 또 다른 SVM 분류기이다. 모델이 과대적합이라면 다항식의 차수를 줄여야 한다. 반대로 과소적합이라면 차수를 늘려야 한다. 매개변수 coef0는 모델이 높은 차수와 낮은 차수에 얼마나 영향을 받을지 조절한다.

![Image-1 (36)](https://github.com/user-attachments/assets/9329be9d-badb-42f1-a538-8db305eccfce)

### 5.2.2 유사도 특성
비선형 특성을 다루는 또 다른 기법은 각 샘플이 특정 랜드마크와 얼마나 닮았는지 측정하는 유사도 함수로 계산한 특성을 추가하는 것이다. 

![Image-1 (37)](https://github.com/user-attachments/assets/0c171e50-6ade-4e01-a5d3-a5ec49a547e6)

앞에서 본 1차원 데이터셋에 두 개의 랜드마크 x1=-2, x1=1을 추가하고 감마가 0.3인 가우스 방사 기저 함수 (RBF)를 유사도 함수로 정의해보자. 이 함수의 값은 0 (랜드마크에서 아주 멀리 떨어진 경우)부터 1 (랜드마크와 같은 위치인 경우)까지 변화하며 종 모양으로 나타난다. 

x1=-1 샘플을 살펴보자. 이 샘플은 첫 번째 랜드마크에서 1만큼 떨어져 있고, 두 번째 랜드마크에서 2만큼 떨어져 있다. 그러므로 새로 만든 특성은 x2 = exp(-0.3*1^2) = 0.74, x3 = exp(-0.3*2^2) = 0.30이다. 오른쪽 그래프는 변환된 데이터셋을 보여준다. 이제 선형적으로 구분이 가능하다. 

RBF 변환을 통해 얻을 수 있는 주요한 통찰이 무엇일까?

1. 비선형 데이터를 선형적으로 구분 가능하게 변환
2. 거리에 따른 유사도 변화 (가까울수록 높은 유사도)
3. 고차원 변환을 통해 커널 기반 머신러닝 모델 적용 가능
4. 랜드마크 선택에 따른 영향 분석

**커널 : 낮은 차원의 데이터를 더 높은 차원의 공간으로 변환하여, 비선형적인 데이터도 선형적으로 구분할 수 있도록 해주는 함수**

머신러닝에서 선형 분류기 (선형VM, 로지스틱회귀)는 직선으로 데이터를 구분한다. 하지만 현실에서는 선형적으로 구분되지 않는 데이터들이 많다고 앞에서 언급했다. 이를 고차원으로 변환을 직접하지 않고, 데이터 포인트 간의 유사도를 고차원에서 계산하는 함수인 커널을 사용하면 된다.

랜드마크를 어떻게 선택할까? 간단한 방법은 데이터셋에 있는 모든 샘플 위치에 랜드마크를 설정하는 것인데, 이렇게 하면 차원이 매우 커져 변환된 훈련 세트가 선형적으로 구분될 가능성이 높다. (고차원 공간에서는 데이터가 퍼지면서 선형적으로 구분할 수 있는 가능성이 증가함.)

보충 설명은 아래와 같다.

![image](https://github.com/user-attachments/assets/7ba759d6-a05c-4c88-b08b-8b625110b900)

![image](https://github.com/user-attachments/assets/4677a1eb-702c-4401-964d-1808dcab8780)

![image](https://github.com/user-attachments/assets/d2e38db0-7f5d-4ae2-8f18-c84cd8e204cb)

### 5.2.3 가우스 RBF 커널
유사도 특성 방식도 머신러닝 알고리즘에 유용하게 사용될 수 있다. 추가 특성을 모두 계산하려면 훈련 세트가 큰 경우에 연산 비용이 많이 든다. 여기서 커널 트릭이 한 번 더 SVM의 마법을 만든다. 마치 유사도 특성을 많이 추가하는 것과 같은 비슷한 결과를 얻을 수 있다. 가우스 RBF 커널을 사용한 SVC 모델을 시도해보자.

```python
rbf_kernel_svm_clf = make_pipeline(StandardScaler(), SVC(kernel = 'rbf', gamma = 5, C=0.001))
rbf_kernel_svm_clf.fit(X,y)
```
아래 그림의 왼쪽 아래 그래프에 이 모델을 나타내었다. 다른 그래프들은 하이퍼파라미터 감마와 C를 바꾸어 훈련시킨 모델이다. 살펴보자.

![Image-1 (38)](https://github.com/user-attachments/assets/b6269208-e6ab-4310-8238-afbcd0997e75)

gamma를 증가시키면 종 모양 그래프가 좁아져서 각 샘플의 영향 범위가 작아진다. 결정 경계가 조금 더 불규칙해지고 각 샘플을 따라 구불구불하게 휘어진다. 

gamma를 감소시키면 넓은 종모양 그래프를 만들며 샘플이 넓은 범위에 걸쳐 영향을 주므로 결정경계가 더 부드러워진다. 

결국, 하이퍼파라미터 gamma가 규제의 역할을 한다. 모델이 과대적합일 경우에는 감소시켜야 하고, 과소적합일 경우에는 증가시켜야 한다.

어떤 커널은 특정 데이터 구조에 특화되어 있다. 

**문자열 커널**이 가끔 텍스트 문서나 DNA 서열을 분류할 때 사용된다.

### 5.2.4 계산 복잡도
LinearSVC 파이썬 클래스는 선형 SVM을 위한 최적화된 알고리즘을 구현한 liblinear 라이브러리를 기반으로 한다. 이 라이브러리는 커널 트릭을 지원하지 않지만 훈련 샘플과 특성 수에 거의 선형적으로 늘어난다. 이 알고리즘의 훈련 시간 복잡도는 대략 O(m*n)이다. 

![image](https://github.com/user-attachments/assets/b8431c2f-27ee-4cda-8204-4268be8f278a)

정밀도를 높이면 알고리즘의 수행 시간이 길어진다. 이는 허용 오차 하이퍼파라미터로 조절한다. (사이킷런의 매개변수는 tol). 대부분의 분류 문제는 허용 오차를 기본값으로 두면 잘 작동한다.






