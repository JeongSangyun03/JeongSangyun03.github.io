---
layout : post
title : 쿠다ML기초 3주차
categories : 쿠다ML 기초
---
# RNN과 CNN을 사용한 시퀀스 처리
## 15.1 순환 뉴런과 순환 층
순환 신경망 (RNN) : 입력 층에서 출력 층 방향으로 흐르거나, 뒤쪽으로 순환하는 연결도 가능한 신경망

![image](https://github.com/user-attachments/assets/6776e2e7-5f71-45b3-af93-81bb60224c57)

1. 각 순환 뉴런은 입력을 위한 가중치와 이전 타임 스텝의 출력을 위한 가중치를 가짐 
2. 각 타임 스텝마다 X(t)와 이전 타임 스텝의 출력인 y(t-1)을 입력 받음 (첫 번째 타임스텝은 0으로 설정)

![image](https://github.com/user-attachments/assets/7b9dc5fe-8ae1-4718-b654-1345b6b1db75)

### 15.1.1 메모리 셀

*메모리 셀의 개념*

→ 순환 신경망(RNN)의 기본 요소로, 일정한 메모리 형태를 가지고 있음.

→ 이전 타임스텝의 모든 입력에 대한 함수로 표현될 수 있음.

→ 하나의 순환 뉴런 또는 순환 뉴런 층(스택) 으로 구성됨.

*메모리 셀의 역할*

→ 단기 패턴 학습:

보통 10 스텝 정도의 길이에 대한 패턴을 학습 가능.

문제의 특성에 따라 다르게 적용될 수 있음.

장기 패턴 학습:

10 스텝 이상의 긴 패턴도 학습할 수 있는 더욱 강력한 셀 구조가 필요함.

*셀의 상태 업데이트*

일반적으로 셀의 상태 ℎ(𝑡)는 현재 타임스텝의 입력과 이전 타임스텝의 상태를 반영하여 결정됨.

수식으로 표현하면:

![image](https://github.com/user-attachments/assets/6bedf92a-0386-4bdc-878a-0d7d0c9febfd)

### 15.1.2 입력과 출력 시퀀스
RNN은 입력 시퀀스를 받아서 출력 시퀀스를 생성하는 모델 (입력과 출력의 관계에 따라 다양한 구조의 네트워크가 존재)

|네트워크 유형|설명|예시|
|---|---|---|
|시퀀스-투-시퀀스|입력과 출력이 모두 시퀀스|전력 소비량 예측|
|시퀀스-투-벡터|입력은 시퀀스, 출력은 고정 벡터|감정 분석 (긍/부정)|
|벡터-투-시퀀스|입력은 벡터, 출력은 시퀀스|이미지 → 문장 설명|
|인코더-디코더|입력을 인코딩 후 디코딩하여 변환|기계 번역|

![image](https://github.com/user-attachments/assets/f4b619a7-a9b2-4d4b-885b-ea368897c7ab)


