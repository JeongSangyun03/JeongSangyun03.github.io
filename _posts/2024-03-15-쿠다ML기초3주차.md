---
layout : post
title : 쿠다ML기초 3주차
categories : 쿠다ML 기초
---
# RNN과 CNN을 사용한 시퀀스 처리
## 15.1 순환 뉴런과 순환 층
순환 신경망 (RNN) : 입력 층에서 출력 층 방향으로 흐르거나, 뒤쪽으로 순환하는 연결도 가능한 신경망

![image](https://github.com/user-attachments/assets/6776e2e7-5f71-45b3-af93-81bb60224c57)

1. 각 순환 뉴런은 입력을 위한 가중치와 이전 타임 스텝의 출력을 위한 가중치를 가짐 
2. 각 타임 스텝마다 X(t)와 이전 타임 스텝의 출력인 y(t-1)을 입력 받음 (첫 번째 타임스텝은 0으로 설정)

![image](https://github.com/user-attachments/assets/7b9dc5fe-8ae1-4718-b654-1345b6b1db75)

### 15.1.1 메모리 셀

*메모리 셀의 개념*

→ 순환 신경망(RNN)의 기본 요소로, 일정한 메모리 형태를 가지고 있음.

→ 이전 타임스텝의 모든 입력에 대한 함수로 표현될 수 있음.

→ 하나의 순환 뉴런 또는 순환 뉴런 층(스택) 으로 구성됨.

*메모리 셀의 역할*

→ 단기 패턴 학습:

보통 10 스텝 정도의 길이에 대한 패턴을 학습 가능.

문제의 특성에 따라 다르게 적용될 수 있음.

장기 패턴 학습:

10 스텝 이상의 긴 패턴도 학습할 수 있는 더욱 강력한 셀 구조가 필요함.

*셀의 상태 업데이트*

일반적으로 셀의 상태 ℎ(𝑡)는 현재 타임스텝의 입력과 이전 타임스텝의 상태를 반영하여 결정됨.

수식으로 표현하면:

![image](https://github.com/user-attachments/assets/6bedf92a-0386-4bdc-878a-0d7d0c9febfd)

### 15.1.2 입력과 출력 시퀀스
RNN은 입력 시퀀스를 받아서 출력 시퀀스를 생성하는 모델 (입력과 출력의 관계에 따라 다양한 구조의 네트워크가 존재)

|네트워크 유형|설명|예시|
|---|---|---|
|시퀀스-투-시퀀스|입력과 출력이 모두 시퀀스|전력 소비량 예측|
|시퀀스-투-벡터|입력은 시퀀스, 출력은 고정 벡터|감정 분석 (긍/부정)|
|벡터-투-시퀀스|입력은 벡터, 출력은 시퀀스|이미지 → 문장 설명|
|인코더-디코더|입력을 인코딩 후 디코딩하여 변환|기계 번역|

![image](https://github.com/user-attachments/assets/f4b619a7-a9b2-4d4b-885b-ea368897c7ab)

※ 인코더-디코더 이중 단계 모델에서 디코더의 초기 입력값을 0으로 설정해도, 무엇을 출력해야 할지 힌트가 필요하므로, 인코더의 마지막 은닉 상태 h(T) 를 디코더의 초기 은닉 상태로 사용

## 15.2 RNN 훈련하기
BPTT : RNN을 훈련하기 위한 기법으로, 타임 스텝으로 네트워크를 펼치고 보통의 역전파를 사용하는 기법

![image](https://github.com/user-attachments/assets/2ae10abe-52d8-4c10-be86-a80f3f1e2597)

→ 네트워크가 시간에 따라 펼쳐진 형태로 존재

1. 순전파 (Forward Pass)
→ 입력 X를 각 타임스텝에서 처리하여 은닉 상태를 업테이트
→ 마지막 타임스텝까지 도달하면 예측 Y가 생성됨
2. 손실함수 𝐿 계산
→ 출력 Y와 실제 값 Y 비교하여 손실 계산 (그림에서는 L(Y2,Y3,Y4) 손실 계산됨)
3. 역전파 (Backward Pass)
→ 계산된 손실을 시간을 거슬러서 각 타임스텝의 가중치에 대해 미분 & 모든 가중치 W와 b (이하 파라미터)를 업데이트

※ 케라스가 싹 다 처리해줌

## 15.3 시계열 예측하기
시계열 데이터 로드 및 정제
```python
import pandas as pd
from pathlib import Path

path = Path ('datasets/ridership/CTA_-_Ridership_-_Daily_Boarding_Totals.csv')
df = pd.read_csv (path, parse_dates["service_date"])
df.columns = ['date', 'day_type','bus','rail','total'] # 짧은 이름
df = df.drop('total', axis = 1) # total은 단순히 bus+rail이므로 제거
df = df.drop_duplicates () #중복 월 삭제 (2011-10과 2014-07)
```
단순 예측 및 결과 시각화

단순 에측 : 과거 정보(가장 최근값)를 이용하여 미래 값을 직접 예측하는 방법 (패턴이 매우 강력할 때)

```python
diff_7 = df[['bus,'rail']].diff(7)['2019-03' : '2019-05']

fig, axs = plt.subplots(2,1,sharex = True, figsize = (8,5))
df.plot(ax=axs[0], legend = False, marker = '.') #원본 시계열
df.shift(7).plot(ax=axs[0], grid = True, legend = False, linestyle = ':') # 지연된 시계열
diff_7.plot(ax=axs[1], grid = True, marker = '.') # 7일 간의 차이
plt.show()
```

자기상관 : 시계열이 시간이 지연된 자기자신과 상관관계를 가지는 것

MAE와 MAPE를 사용한 예측 오류 평가
```python
# MAE 
diff_7.abs().mean()
```
MAE (평균 절대 오차) : 예측값과 실제값의 차이를 평균낸 값 (단위가 데이터와 동일)

MAE가 크다는 것은 오차가 크다는 의미!

```python
targets = df[['bus','rail']]['2019-03':'2019-05']
(diff_7/targets).abs().mean()
```
MAPE (평균 절대 비율 오차) : 예측값과 실제값의 차이를 실제값으로 나눈 비율 평균 (비율로 나타내므로 단위에 영향 X)

연간 계절성이 나타나는지에 대해서 그래프를 생성하고 분석해보자.

```python
period = slice('2001','2019')
df_monthly = df.resample('M').mean() #월 평균 계산
rolling_average_12_months = df_monthly[period].rolling(window=12).mean()

fig, ax = plt.subplots(figsize = (8,4))
df_monthly[period].plot(ax=ax, marker='.')
rolling_average_12_months.plot(ax=ax, grid = True, legend = False)
plt.show()
```
그리고 차분을 살펴보자.
```python
df_monthly.diff(12)[period].plot(grid=True, marker = '/', figsize(8,3))
plt.show()
```
![image](https://github.com/user-attachments/assets/ec8b561b-129e-43e5-b26d-9bedf46490f1)

![image](https://github.com/user-attachments/assets/6b7b99be-db80-457d-84af-75e4fc7f39e8)

차분 : 시계열 데이터에서 트렌드와 계절성을 제거하기 위해 사용되는 기법 (각각의 시점에서 이전 시점과의 차이를 계산하여 새로운 시계열 생성) → 시간이 지나도 평균과 분산이 일정한 **정상 시계열** 생성 가능 (변동 폭 일정한 패턴으로 변화)

예측에서 차분이 왜 중요한가? 

트렌드를 고려하여 매년 평균적으로 감소하는 값 (책의 승객 이용 데이터 예시)을 반영하면 예측 정확도를 높일 수 있음

☆ 차분을 사용하면 단기 패턴을 쉽게 포착할 수 있고, 트렌드를 고려하면 장기적인 예측 성능 개선 가능

### 15.3.1 ARMA 모델

**ARMA (자기 회귀 이동 평균)** = AR (자기 회귀) + MA (이동 평균) : 지연된 값의 간단한 가중치 합 계산 + 이동 평균 합 → 예측 수정

※ 마지막 몇 개의 예측 오차의 가중치 합 사용하여 이동 평균 계산

***이 모델은 정상 시계열을 가정한다!***

즉, 평균과 분산이 시간이 지나도 일정한 시계열 데이터를 가정한다. but, 실제 데이터는 비정상 시계열이 대부분.

→ 차분 활용하여 정상 시계열로 변환.

차분 : 미분의 근사값과 유사. 각 타임 스텝에서 시계열의 기울기 (연속적 데이터일 때)

(원본 시계열이 d차 방정식 형태 트렌드를 가질 때 : 연속적인 d번의 차분을 수행하여 시계열의 d차 도함수를 근사. d차 다항식 트렌드 제거 가능. 여기서 d를 **누적 차수**라고 함)

**ARIMA (자기 회귀 누적 이동 평균)** = AR + I (차분) + MA : d번의 차분을 수행하여 시계열을 정상 상태로 만들고, 일반적인 ARMA 모델 적용 → 차분으로 뺐던 값을 다시 더함 (원래 시계열 예측값. 원래 스케일로 복원)

**SARIMA (계절성 ARIMA)** = 주어진 빈도에 대한 계절 항을 추가로 모델링하는 ARIMA의 유형 : 총 7개의 하이퍼파라미터를 가짐

SARIMA 모델의 하이퍼파라미터 살펴보기

SARIMA 모델의 하이퍼파라미터는 ARIMA 모델을 위한 p,d,q 하이퍼파라미터, 계절성 패턴을 모델링 하기 위한 P,D,Q 하이퍼파라미터, 계절성 패턴의 간격인 s가 존재한다. (계절성 패턴은 단위별로 상이함)

|하이퍼파라미터|설명|
|---|---|
|p|자기회귀(AR)차수|
|d|차분 횟수|
|q|이동평균(MA) 차수|
|P|계절적 자기회귀 차수|
|D|계절적 차분 횟수|
|Q|계절적 이동평균 차수|
|m|계절 주기|

ARIMA 클래스의 statsmodels 라이브러리를 활용한 SARIMA 모델 적용
```python
from statsmodels.tsa.arima.model import ARIMA

origin, today = '2019-01-01', '2019-05-31'
rail_series = df.loc[origin:today]['rail'].asfreq('D') #여기서 D를 통해 빈도를 일자별로 설정
model = ARIMA (rail_series, order = (1,0,0), seasonal_order = (0,1,1,7))
model = model.fit()
y_pred = model.forecast ()
# 위 코드는 예측력이 좋지 않다. 따라서 3,4,5월의 모든 날에 대한 예측을 만들고 전체 기간에 대한 MAE를 계산하겠다.
origin, start_date, end_date = '2019-01-01', '2019-03-01', '2019-05-31'
time_period = pd.date_range (start_date, end_date)
rail_series = df.loc[origin:end_date]['rail'].asfreq('D')
y_preds = []
for today in time_period.shift(-1) :
  model = ARIMA (rail_series[origin:today], order = (1,0,0), seasonal_order = (0,1,1,7)) # today까지 데이터로 훈련
  model = model.fit()
  y_pred = model.forecast()[0]
  y_preds.append(y_pred)

y_preds = pd.Series (y_preds, index = time_period)
mae = (y_preds - rail_series[time_period].abs().mean()
결과를 표시하지 않았지만 성능이 높아졌다는 것을 확인할 수 있다.
```

SARIMA 모델의 하이퍼파라미터를 선택하는 방법

가장 쉽고 시작하기 좋은 방법 : 단순 그리드 서치 (여러 조합 실험, 하이퍼파라미터 값만 바꾸면서 코드 실행)

### 15.3.2 머신러닝 모델을 위한 데이터 준비하기
간단한 선형 모델로 예측을 먼저 시도해보겠다.

**훈련, 검증, 테스트 세트 분할 후, 훈련 세트와 검증 세트를 위한 데이터셋 생성**
```python
rail_train = df['rail']['2016-01':'2018-12'] / 1e6 # 기본 가중치 초기화와 학습률의 원활한 작동을 위한 나눗셈
rail_valid = df['rail']['2019-01':'2019-05'] / 1e6
rail_test = df['rail']['2019-06':] / 1e6

seq_length = 56 #윈도
train_ds = tf.keras.utils.timeseries_dataset_from_array (
  rail_train.to_numpy(),
  targets = rail_train[seq_length:],
  sequence_length = seq_length
  batch_size = 32,
  shuffle = True,
  seed = 42
)
valid_ds = tf.keras.utils.timeseries_dataset_from_array (
  rail_valid.to_numpy(),
  targets = rail_valid[seq_length:],
  sequence_length = seq_length,
  batch_size = 32
)
```
### 15.3.3 선형모델로 예측하기
**후버 손실을 이용한 손실 최소화 및 조기 종료 사용**

※ REMIND

후버 손실 : MAE와 MSE의 장점을 결합한 손실함수. 이상치에 강건하여 큰 오차에서 MAE처럼 선형적으로 증가하여 이상치 영향 줄이고, 작은 오차에서는 MSE처럼 작동하여 미분 가능.

```python
tf.random.set_seed(42)
model = tf.keras.Sequential([
  tf.keras.layers.Dense(1,input_shape = [seq_length])
])
early_stopping_cb = tf.keras.callbacks.EarlyStopping(
  monitor = 'val_mae', patience = 50, restore_best_weights = True)
opt = tf.keras.optimizers.SGD(learning_rate = 0.02, momentum = 0.9)
model.compile(loss=tf.keras.losses.Huber(), optimizer = opt, metrics = ['mae'])
history = model.fit (train_ds, validation_data = valid_ds, epochs = 500, callbacks = [early_stopping_cb])
# 과적합 방지 위한 조기종료
```

### 15.3.4 간단한 RNN으로 예측하기
SARIMA 통계 모델이 아닌, RNN 딥러닝 모델을 이용하여 예측해보자. 여기서는 가장 간단한 RNN을 사용할 것이다.
```python
univar_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 1]),
    tf.keras.layers.Dense(1)  # 기본적으로 활성화 함수가 없음. (회귀라서)
])
```
### 15.3.5 심층 RNN으로 예측하기

![image](https://github.com/user-attachments/assets/41fd5046-3696-4ff0-929b-c756c68c6929)

RNN은 셀을 여러 층으로 쌓는 것이 일반적이다. 이를 **심층 RNN**이라고 한다.

```python
#SimpleRNN 3개를 쌓아서 만든다
deep_model = tf.keras.Sequential ([
  tf.keras.layers.SimpleRNN(32, return_sequences = True, input_shape = [None,1]), #시퀀스-투-시퀀스
  tf.keras.layers.SimpleRNN(32, return_sequences = True), #시퀀스-투-시퀀스
  tf.keras.layers.SimpleRNN(32), #시퀀스-투-벡터
  tf.keras.layers.Dense(1) #모델의 예측 만듦
])
```

### 15.3.6 다변량 시계열 예측하기
***버스와 열차 데이터를 입력으로 사용**
```python
#버스와 열차 시리즈를 입력으로 사용
df_mulvar = df[['vus','rail']] / 1e6
# 내일의 요일 유형 사전에 미리 알고 있음 (미리 결정된 정보)
df_mulvar = ['next_day_type'] = df['day_type'].shift(-1)
#원-핫 인코딩
df_mulvar = pd.get_dummies(df_mulvar)

#df_mulvar은 버스, 열차, 날의 요일 유형 W,A,U의 다섯 개의 열을 가진 데이터프레임이다.
```

**훈련, 검증, 테스트 세트 분할**
```python
mulvar_train = df_mulvar ['2016-01':'2018-12']
mulvar_valid = df_mulvar['2019-01' : '2019-05']
mulvar_test = df_mulvar['2019-06' :]
```
**데이터셋 생성**
```python
train_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_train.to_numpy(),  # 5개의 열을 모두 입력으로 사용
    targets=mulvar_train["rail"][seq_length:],  # "rail" 열만 예측값(타겟)
    [...]
)

valid_mulvar_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_valid.to_numpy(),
    targets=mulvar_valid["rail"][seq_length:],
    [...]
)
```
**RNN 모델 생성**
```python
mulvar_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),
    tf.keras.layers.Dense(1)
])
```

다변량 시계열 예측 모델 개선 및 다중 출력 RNN 특징

1. 새로운 RNN 모델은 5개의 입력 변수 사용하여 예측 수행 (다변량 입력을 통한 성능 향상)
2. 기존 모델은 하나의 출력 값(rail 예측)만 생성했지만, 버스와 열차 승객 수를 모두 예측할 수 있도록 모델을 변경할 수 있음.
3. 관련 있는 여러 작업을 단일 RNN 모델에서 함께 학습하는 것이 유리할 수도 있음

### 15.3.7 여러 타임 스텝 앞 예측하기
앞서 다음 타임 스텝의 값만 예측하였지만, 타깃을 적절히 바꾸어 여러 타임 스텝 앞의 값을 손쉽게 예측할 수 있다.

예제 : 현시점부터 2주 뒤의 승객 수 예측 시 14일 앞의 값을 타깃으로 사용

방법 1. 한 번에 1개의 미래 값을 예측한 후, 그 값을 다시 입력하여 다음 값을 예측하는 방식
```python
import numpy as np
X = rail_valid.to_numpy()[np.newaxis, :seq_length, np.newaxis]
for step_ahead in range(14):
    y_pred_one = univar_model.predict(X)  # 모델을 사용하여 1일 뒤 값 예측
    X = np.concatenate([X, y_pred_one.reshape(1, 1, 1)], axis=1)  # 예측값을 입력 데이터에 추가
```
방법 2. 한 번에 다음 14개 값 예측하는 RNN 훈련 (S2V 다중 출력)
```python
def split_inputs_and_targets(mulvar_series, ahead=14, target_col=1):
    return mulvar_series[:, :-ahead], mulvar_series[:, -ahead:, target_col]
    #ahead=14: 14일 앞까지 예측해야 하므로 타깃을 14일 이동

ahead_train_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_train.to_numpy(),
    targets=None,
    sequence_length=seq_length + 14
    # sequence_length=seq_length + 14 : 입력 시퀀스를 만들 때 14일을 추가로 고려하여 출력(타깃) 데이터까지 포함하도록 설정.
).map(split_inputs_and_targets)

ahead_valid_ds = tf.keras.utils.timeseries_dataset_from_array(
    mulvar_valid.to_numpy(),
    targets=None,
    sequence_length=seq_length + 14,
    batch_size=32
).map(split_inputs_and_targets)
# RNN 모델 생
ahead_model = tf.keras.Sequential([
    tf.keras.layers.SimpleRNN(32, input_shape=[None, 5]),
    tf.keras.layers.Dense(14) #Dense(14): 출력층에서 14개의 값 예측 (14일 앞까지 예측).
])
```

### 15.3.8 시퀀스-투-시퀀스 모델로 예측하기
**기존 방식 (S2V)**

마지막 타임 스텝에서만 14개 값 예측 : RNN의 마지막 출력만 사용하므로, 중간 단계의 정보를 충분히 활용하지 못할 수 있음

**개선된 방식 (S2S)**

모든 타임 스텝에서 예측 수행 (각 타임 스텝에서 별도의 출력 생성하여 14개 미래 값 예측) : 각 타임 스텝의 출력을 포함하여 전체적인 패턴을 더 잘 학습 가능

**→ 왜 S2S 방식이 더 좋은가?**

모든 타임 스텝의 출력을 사용하므로, RNN의 학습 정보가 더 풍부해짐.

Gradient(기울기)가 마지막 타임 스텝으로 집중되지 않고, 전체적인 패턴을 학습할 수 있음.

오차 역전파(Backpropagation) 과정에서 마지막 타임 스텝만 고려하는 것이 아니라, 모든 타임 스텝에서 오류를 조정 가능 → 학습 안정성 & 속도 향상.

**어떻게 만드는가?**

시계열 데이터를 사용할 때, 모델이 학습할 수 있도록 일정한 크기의 입력 윈도와 출력 윈도를 생성해야 함.

이를 위해 to_window() 함수를 두 번 실행하여 연속적인 윈도 시퀀스를 생성함. (comma 없이 각 윈도가 앞 윈도와 겹쳐져 있는 자연스러운 윈도)

윈도(Window)로 변환된 시계열 데이터를 입력(X)과 타깃(Y)으로 나누기 (map() 함수 이용)

※ 인과 모델 : 과거 정보만을 사용하여 미래를 예측하는 모델 (미래를 미리 알면 안된다.)

위의 방식은 RNN 등 시계열 모델에서 일반적으로 사용되는 인과 모델 학습 방식이다.

S2S에 사용할 데이터셋 준비 과정 - 다른 유틸리티 함수 생성 (셔플링과 배치 처리 담당)

```python
def to_seq2seq_dataset (series, seq_length = 56, ahead = 14, target_col = 1, batch_size = 32, shuffle =      False, seed = None)
  ds = to_windows (tf.data.Dataset.from_tensor_slices(series), ahead + 1)
  ds = to_windows (ds, seq_length).map(lambda S : (S[:,0], S[:, 1:, 1]))

  if shuffle :
    ds = ds.shuffle (8 * batch_size, seed = seed)
  return ds.batch(batch_size)
```

```python
# 위 함수를 이용한 데이터셋 생성
seq2seq_train = to_seq2seq_dataset(mulvar_train, shuffle = True, seed = 42)
seq2seq_valid = to_seq2seq_dataset (mulvar_valid)

# S2S 모델 생성
seq2seq_model = tf.keras.Sequential ([
  tf.keras.layers.SimpleRNN (32, return_sequences = True, input_shape = [None, 5]),
  tf.keras.layers.Dense(14)
]}
```
위 코드에서,

***return_sequences = True의 기능***

: 마지막 타임 스텝에서 하나의 벡터를 출력하는 것 X, 크기가 32인 벡터의 시퀀스 출력

***Dense층의 기능***

: 시퀀스 입력을 처리. (32차원 벡터를 입력으로 받고 14차원 벡터를 출력하는 식으로 매 타임스텝 적용)

(RNN의 마지막 은닉 상태가 최종 출력층을 통과하면서 14차원 벡터가 된다.)

단순한 RNN의 특징과 한계점
1. 특징 : 시계열 예측 또는 다른 종류의 시퀀스 처리에 매우 탁월
2. 한계 : 긴 시계열이나 시퀀스에서는 성능 ↓

## 15.4 긴 시퀀스 다루기
긴 시퀀스로 RNN 훈련 시 많은 타임 스텝에 걸쳐 실행해야 함 : 펼친 RNN 매우 깊은 네트워크 형성 → 그레이디언트 소실, 그레이디언트 폭주 문제 발생

1. 훈련 시 아주 오랜 시간 소모
2. 불안정한 훈련 발생
3. 입력의 초기 부분 소실

### 15.4.1 불안정한 그레이디언트 문제와 싸우기
폭주 문제 : 수렴하지 않는 활성화 함수 (ReLU) 사용 시 출력 폭주, 그레이디언트 자체 폭주하는 문제 → 훈련 불안정해짐

SOL) 작은 학습률 사용 or 수렴하는 활성화 함수 사용 (tanh). 훈련 불안정 판단 시 그레이디언트 클리핑 사용

**RNN에서 잘 맞는 정규화는 배치 정규화인가 층 정규화인가?**

BATCH 정규화 : RNN에 효율적 적용 어려움

1. 시간 종속성 : BN은 미니배치 내에서 평균과 분산을 계산하지만, RNN은 각 타임스텝마다 은닉 변화 → 각 t마다 다른 분산과 평균이 필요

2. 미니배치 크기 한계 : RNN 학습 시 미니배치 크기 감소 → BN의 평균, 분산 추정 능력 감소

3. 기울기 흐름 : 기울기는 t마다 변함

***결과적으로, 배치 정규화는 타임 스텝 사이에 사용할 수 없으며, 순환 층 사이에서만 사용 가능!***

층 정규화 : 한개의 샘플에서 모든 뉴런을 정규화 → 시간 축으로부터 독립적 : t마다 변하는 은닉에도 영향을 받지 않음

Layer Normalization을 적용한 사용자 정의 RNN 셀 구현

```python
class LNSimpleRNNCell(tf.keras.layers.Layer):
def __init__(self, units, activation="tanh", **kwargs):
  super().__init__(**kwargs)
  self.state_size = units
  self.output_size = units
  self.simple_rnn_cell = tf.keras.layers.SimpleRNNCell(units, activation=None) #기본적인 RNN 연산 수행 (출력 및 새로운 은닉 상태 계산)

  self.layer_norm = tf.keras.layers.LayerNormalization() #층 정규화
  self.activation = tf.keras.activations.get(activation) #활성화 함수 사용

def call(self, inputs, states):
  outputs, new_states = self.simple_rnn_cell(inputs, states)
  norm_outputs = self.activation(self.layer_norm(outputs))
  return norm_outputs, [norm_outputs]
```

타임스텝 사이에 드롭아웃을 적용하는 방식 (케라스에서 dropout 매개변수(입력)와 recurrent_dropout 매개변수(은닉) 지원)

은닉 업데이트 시 일정 뉴런 dropout → 기울기 흐름 조절 : 특정 뉴런 의존 ↓

Deep dive

***드롭아웃 마스크 유지?***

일반 드롭아웃은 각 t마다 새로운 마스크가 생성되어 패턴이 불규칙해진다. (무직위로 뉴런을 drop함) 그렇다면 훈련이 불안정해지지 않는가? 이를 위해 일반 드롭아웃이 아닌 드롭아웃 마스크 유지 기법을 사용한다. 모든 타임 스텝에서 drop을 유지하여 기울기가 유지되고 안정적인 훈련이 가능해진다. 일반 드롭아웃과 드롭아웃 마스크 유지의 차이점에 대해서 살펴보자.

(Recurrent Dropout에












