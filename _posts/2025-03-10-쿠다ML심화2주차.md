---
layout : post
title : 쿠다 ML 심화 2주차
categories : 쿠다 ML 심화
---
# 14. 합성곱 신경망을 사용한 컴퓨터 비전
합성곱 신경망 (CNN) : 이미지 검색 서비스, 자율 주행 자동차, 영상 자동 분류 시스템, 음성 인식, NLP 등에 사용

본 장에서는 **시각적인 애플리케이션**에 초점을 맞춤 → (CNN이 어디에서 왔는가. 구성요소는 무엇인가. 케라스로 사용하는 방법은 무엇인가.)

## 14.1 시각 피질 구조
시각 피질에 관한 연구 → **합성곱 신경망** 연구 

시각 피질에 관한 연구 중요 특징 : 

1. 시각 피질 내 많은 뉴런 : 작은 **국부 수용장** (시야의 일부 범위 안에 있는 시각 자극에 반응)
2. 동일한 수용장을 가진 뉴런 내에서도 다른 요소에 반응
3. 어떤 뉴런 : 큰 수용장을 가져 저수준 패턴 조합된 더 복잡한 패턴에 반응 (이웃한 저수준 뉴런의 출력에 기반한 고수준 뉴런)

**합성곱 신경망** 연구 : 

**LeNet-5 구조** 사용 (완전 연결 층 & 시그모이드 활성화 함수 포함 & **합성곱 층**, **플링 층** 포함)

## 14.2 합성곱 층
합성곱 층 : CNN의 가장 중요한 구성 요소

첫 번째 합성곱 층의 뉴런 : 합성곱 층 뉴런의 수용장 안에 있는 픽셀에만 연결

(☞네트워크가 첫 번째 은닉 층에서는 작은 저수준 특성에 집중. 후에 더 큰 고수준 특성으로 조합해 나감)

어떤 층 i행 j열에 있는 한 뉴런 : 이전층의 i에서 i + fh -1 까즤의 행 & j + fw - 1까지의 열에 있는 뉴런의 출력에 연결. (높이와 너비)

**제로 패딩** :높이와 너비를 이전 층과 같게 하기 위해 입력 주위에 0 추가하는 것

**스트라이드** : 한 수용장 & 다음 수용장 사이의 수평 또는 수직 방향 스텝 크기

스트라이드를 이용하여 차원 축소를 진행하고 계산 복잡도를 줄일 수 있음. (수용장 사이에 간격을 두어 큰 입력 층을 훨씬 작은 층에 연결)

(※ 스트라이드의 가로세로 크기가 동일할 수도 있고, 다를 수도 있다.)

### 14.2.1 필터
![image](https://github.com/user-attachments/assets/b2dd6380-4eab-46cb-a57f-126fa905afce)

이미지를 통해 이해해보자.

**필터** : (합성곱 커널 또는 커널이라고 불림). 이는 두 개의 가중치 세트. 

첫 번째 필터 : 가운데 흰 수직선(열이 1로 채워짐)과 그 외(열이 0으로 채워짐)가 있는 검은 사각형 (7*7 행렬 구성)

두 번째 필터 : 가운데 흰 수평선이 있는 검은 사각형

(※ 1의 특성을 가진 흰색 부분을 제외하면 수용장 안의 모든 것을 무시할 것)

층의 전체 뉴런에 적용된 하나의 필터 ==> 하나의 **특성 맵** 만듦

(훈련 중 합성곱 츠이 자동으로 해당 문제에 가장 유용한 필터를 찾고 상위 층은 이들을 연결하여 더 복잡한 패턴 학습)

### 14.2.2 여러 가지 특성 맵 쌓기
실제 합성곱 층 : 여러 가지 필터 갖고 필터마다 하나의 특성 맵 출력 (3D 출력이 일반적)

각 특성 맵의 픽셀 = 하나의 뉴런. 

뉴런 전체 집합 : 하나의 필터 => 하나의 특성 맵이므로, 하나의 특성 맵 안에서 모든 뉴런이 같은 파라미터 (동일한 가중치와 편향)을 공유. 다른 특성 맵에 있는 뉴런은 다른 파라미터 사용

하나의 합성곱 층이 입력에 여러 필터 동시 적용하여 입력에 있는 여러 특성 감지 가능

### 14.2.3 케라스로 합성곱 층 구현하기
샘플 로드 및 전처리 과정
```python
from sklearn.datasets import load_sample_images
import tensorflow as tf

images = load_sample_images()['images']
images = tf.keras.layers.CenterCrop(height=70, width=120)(images)
images = tf.keras.layers.Rescaling(scale = 1/255)(images)
```
이는 4D 텐서. (1차원 : 두 개의 샘플 이미지. 2&3차원 : 이미지 크기. 4차원 : 컬러 채널 3개)

합성곱 층 생성 + 이미지 주입 
```python
conv_layer = tf.keras.layers.Conv2D (filters = 32, kernel_size = 7)
fmaps = conv_layer(images)
fmaps.shape
```
층의 가중치 살펴보기 (커널과 편향)
```python
kernels, biases = conv_layer.get_weights()
print (kernels.shape)
print (biases.shape)
```

### 14.2.4 메모리 요구 사항
CNN : 모델의 합성곱 층이 많으면 많은 양의 RAM 필요 → 훈련 시 역전파 알고리즘에서 계산량 多 (더 많은 공간 필요)

ex) 5*5 필터 200개 사용하는 합성곱 층 설명 

입력 150*100 RGB 이미지인 경우, 파라미터 수 15200개 비교적 少

각 뉴런이 75개의 입력과 가중치 합을 계산해야 하므로 총 2억 2천 5백만 개의 실수 곱셈 연산 필요

출력이 200*150*100*32비트일 경우, RAM 사용량 12MB

but, 훈련 배치 100개 샘플일 경우, 해당 층에서만 1.2GB RAM 사용

추론 시에는 필요한 RAM이 훈련보다 적지만, 훈련 중에는 전체 합산된 RAM 양이 필요함

다양한 최적화 방법 고려 필요

## 14.3 풀링 층
목적 : 계산량, 메모리 사용량, 파라미터 수 감소를 위해 입력 이미지의 축소본을 만드는 것

풀링 층의 각 뉴런 => 이전 층의 작은 사각 영역 수용장 안에 있는 뉴런의 출력과 연결 (합성곱 층과 마찬가지, 크기, 스트라이드, 패딩 유형 지정해야 하지만 가중치 X)

최대 풀링(Max Pooling)

주어진 영역에서 가장 큰 값을 선택.

특징을 강조하면서도 크기를 줄임.

엣지(edge)나 텍스처 같은 중요한 정보 보존.

평균 풀링(Average Pooling)

주어진 영역에서 평균 값을 선택.

전체적으로 부드럽게 변환되며, 세부 정보를 덜 강조.

입력 이미지가 변형될 때, CNN이 이를 어떻게 처리하는지를 설명하는 용어 (불변성, 등변성)

1. 불변성 : 입력이 변화해도 출력이 일정한 성질 (위치, 크기, 각도 변화에 대해 결과 일정하게 유지)

2. 등변성 : 입력이 변형되면, 출력도 같은 방식으로 변하는 성질 (CNN이 입력 이미지의 변화 (회전, 이동)를 반영하여 출력도 같은 방식으로 변함)

## 14.4 케라스로 풀링 층 구현하기
1) 최대 풀링(Max Pooling)

MaxPooling2D 또는 MaxPool2D를 사용하여 2×2 커널 기반의 풀링 층 생성.

기본적으로 스트라이드는 커널 크기와 동일하며, "valid" 패딩 사용(즉, 패딩 없음).

특징: 각 영역에서 최대값을 선택하여 정보를 보존하며, 중요한 특징을 강조.

2) 평균 풀링(Average Pooling)

AveragePooling2D 또는 AvgPool2D 사용.

특징: 특정 영역 내 평균값을 계산하여 다운샘플링 수행.

최대 풀링보다 부드러운 결과를 제공하지만, 중요한 특징이 사라질 수 있음.

연산 비용이 더 큼.

3) 깊이 방향 최대 풀링(Depth Pooling)

일반적인 MaxPooling2D는 너비와 높이 방향의 풀링만 수행하지만, 깊이(채널) 방향으로도 최대 풀링이 가능함.

케라스는 기본적으로 제공하지 않으므로 사용자 정의 층(Custom Layer) 으로 구현 가능.

tf.keras.layers.Layer 클래스를 상속받아 DepthPool 클래스를 정의.

입력 텐서를 원하는 크기로 그룹화한 후, tf.reduce_max()를 사용하여 각 그룹의 최대값을 계산.

4) 전역 평균 풀링(global average pooling layer)

각 특성 맵의 평균 계산

## 14.5 CNN 구조
전형적인 CNN 구조 : 합성곱 층 몇 개 쌓음 & 풀링 층 쌓음 & 합성곱 층 몇 개 더 쌓음 & 다시 풀링 층 쌓음 => 가장 위층에는 몇 개의 완전 연결 층으로 구성된 일반적인 피드포워드 신경망 추가 & 마지막 층에서 예측 출력

### 14.5.1 LeNet-5
가장 널리 알려진 CNN 구조 : 합성곱 층 & 풀링 층 적재 => 밀집 층 뒤따름

활성화 함수로 tanh 대신 ReLU 사용. RBF 대신 소프트맥스 사용


### 14.5.2 AlexNet
AlexNet 구조는 더 크고 깊을 뿐 LeNet-5와 유사 & 합성곱 층 위에 풀링층 쌓지 않고 바로 합성곱 층끼리 적재

과대적합 방지 위해 두 가지 규제 기법 사용

1. 훈련 중 F9(완전 연결) & F10(완전연결) 출력에 드롭아웃 50% 비율 적용

2. 훈련 이미지를 랜덤하게 여러 간격으로 이동 or 수평으로 뒤집고 조명 바꾸는 식 => **데이터 증식** 수행

구조:

5개의 합성곱 층 (Convolutional Layers)

3개의 완전 연결 층 (Fully Connected Layers)

활성화 함수: ReLU

Max Pooling 사용.

### 14.5.3 GoogLeNet
2014년 ILSVRC에서 우승한 모델.

AlexNet보다 더 깊고 효율적인 네트워크 구조를 가짐.

핵심 아이디어: 인셉션 모듈 (Inception Module)

여러 개의 필터(1×1, 3×3, 5×5)를 동시에 적용하여 다양한 크기의 특징을 추출.

1×1 합성곱을 활용해 차원을 축소하여 연산량을 줄임.

Concatenate 층을 이용해 깊이 방향으로 연결.

```python
from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, concatenate, Input
from tensorflow.keras.models import Model

def inception_module(x, filters):
    f1, f2_in, f2_out, f3_in, f3_out, f4_out = filters
    
    path1 = Conv2D(f1, (1,1), padding='same', activation='relu')(x)
    
    path2 = Conv2D(f2_in, (1,1), padding='same', activation='relu')(x)
    path2 = Conv2D(f2_out, (3,3), padding='same', activation='relu')(path2)
    
    path3 = Conv2D(f3_in, (1,1), padding='same', activation='relu')(x)
    path3 = Conv2D(f3_out, (5,5), padding='same', activation='relu')(path3)
    
    path4 = MaxPooling2D((3,3), strides=(1,1), padding='same')(x)
    path4 = Conv2D(f4_out, (1,1), padding='same', activation='relu')(path4)
    
    return concatenate([path1, path2, path3, path4], axis=-1)

input_layer = Input(shape=(224, 224, 3))
x = inception_module(input_layer, [64, 96, 128, 16, 32, 32])
model = Model(input_layer, x)
model.summary()
```

### 14.5.4 VGGNet
ILSVRC 2014 준우승 모델.

구조가 단순하고 깊음 → 3×3 필터만 사용하여 깊이를 증가시킴.

16~19개의 합성곱 층 사용 (VGG16, VGG19).

단점:

파라미터 수가 많아 연산량이 큼.

모델이 깊어질수록 기울기 소실(Vanishing Gradient) 문제 발생.
```python
from tensorflow.keras.applications import VGG16

model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
model.summary()
```
### 14.5.5 ResNet
2015년 ILSVRC 우승 모델.

잔차 학습(Residual Learning) 개념을 도입하여 매우 깊은 신경망에서도 학습이 가능.

핵심 개념: 스킵 연결(Skip Connection)

네트워크의 출력이 입력값을 그대로 유지하도록 

F(x)+x 형태로 잔차 유닛(Residual Unit)을 구성.

기울기 소실 문제를 해결하고 학습 속도를 개선.
```python
from tensorflow.keras.layers import Add, BatchNormalization, ReLU

def residual_block(x, filters):
    res = Conv2D(filters, (3,3), padding='same', activation='relu')(x)
    res = BatchNormalization()(res)
    res = Conv2D(filters, (3,3), padding='same')(res)
    res = BatchNormalization()(res)
    
    x = Add()([x, res])
    return ReLU()(x)
```

### 14.5.6 Xception
GoogleNet의 인셉션 구조를 개선한 모델.

핵심 개념: 심층 분리 합성곱 (Depthwise Separable Convolution)

**공간 필터링(Depthwise Convolution)** 과 **특징 조합(Pointwise Convolution)** 을 분리하여 연산량을 줄이고 효율성을 증가.

MobileNet에서도 같은 기법 사용.

```python
from tensorflow.keras.applications import Xception

model = Xception(weights='imagenet', include_top=False, input_shape=(299, 299, 3))
model.summary()
```

### 14.5.7 SENet
2017년 ILSVRC 우승 모델.

네트워크에서 채널별 중요도를 조정하는 메커니즘 도입.

핵심 개념: Squeeze-and-Excitation 블록

각 채널별 특징을 학습하고 중요한 채널에 가중치를 부여.

기존 CNN 모델(ResNet, Inception 등)에 적용 가능.
```python
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Reshape, Multiply

def se_block(x, ratio=16):
    filters = x.shape[-1]
    squeeze = GlobalAveragePooling2D()(x)
    excitation = Dense(filters//ratio, activation='relu')(squeeze)
    excitation = Dense(filters, activation='sigmoid')(excitation)
    excitation = Reshape((1, 1, filters))(excitation)
    return Multiply()([x, excitation])
```
### 14.5.8 주목할만한 다른 구조
ResNeXt

ResNet의 확장 버전.

여러 개의 그룹 컨볼루션을 사용하여 성능 개선.

DenseNet

각 층의 출력을 모든 이후 층과 연결.

파라미터 수를 줄이고 효율적인 특징 학습 가능.

MobileNet

경량화된 CNN 모델로 모바일 및 임베디드 환경에 적합.

심층 분리 합성곱(Depthwise Separable Convolution) 사용.

```python
from tensorflow.keras.applications import MobileNetV2

model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
model.summary()
```
EfficientNet

모델 크기(깊이, 너비, 해상도)를 균형 있게 조정하는 복합 스케일링(compound scaling) 사용.

```python
from tensorflow.keras.applications import EfficientNetB0

model = EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
model.summary()
```
### 14.5.9 올바른 CNN 구조 선택
정확도, 모델 속도, CPU의 추론 속도, GPU에서 추론 속도 등에 따라 뭘 중요하게 여기는지를 기준으로 가장 잘 맞는 구조를 선택할 수 있다.
## 14.6 케라스로 ResNet-34 CNN 구현하기
직접 ResNet-34 모델 구현
```python
from tensorflow.keras import layers, models

class ResidualUnit(layers.Layer):
    def __init__(self, filters, strides=1, activation='relu', **kwargs):
        super().__init__(**kwargs)
        self.activation = layers.Activation(activation)
        self.main_layers = [
            layers.Conv2D(filters, kernel_size=3, strides=strides, padding='same'),
            layers.BatchNormalization(),
            self.activation,
            layers.Conv2D(filters, kernel_size=3, strides=1, padding='same'),
            layers.BatchNormalization()
        ]
        self.skip_layers = []
        if strides > 1:
            self.skip_layers = [
                layers.Conv2D(filters, kernel_size=1, strides=strides, padding='same'),
                layers.BatchNormalization()
            ]

    def call(self, inputs):
        z = inputs
        for layer in self.main_layers:
            z = layer(z)
        skip_z = inputs
        for layer in self.skip_layers:
            skip_z = layer(skip_z)
        return layers.Activation('relu')(z + skip_z)
```

## 14.7 케라스의 사전 훈련 모델 사용하기
tf.keras.applications에서 제공하는 모델 로드 및 예측 실행
```python
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
import numpy as np

model = ResNet50(weights='imagenet')

# 예제 이미지 로드 및 처리
image = load_sample_images()["images"]
image_resized = tf.keras.layers.Resizing(height=224, width=224)(image)
inputs = preprocess_input(image_resized)

# 예측 수행
Y_proba = model.predict(inputs)
top_K = decode_predictions(Y_proba, top=3)
```
## 14.8 사전 훈련된 모델을 사용한 전이 학습
tf_flowers 데이터셋 활용하여 모델 학습
```python
import tensorflow_datasets as tfds

dataset, info = tfds.load("tf_flowers", as_supervised=True, with_info=True)
train_set_raw, valid_set_raw, test_set_raw = dataset['train[:70%]'], dataset['train[70%:85%]'], dataset['train[85%:]']

# 데이터 증강 및 모델 학습
train_set = train_set_raw.map(lambda x, y: (preprocess_input(x), y)).shuffle(1000).batch(32)
model.fit(train_set, validation_data=valid_set_raw.batch(32), epochs=10)
```
### 14.10.1 완전 합성곱 신경망 (FCN)
CNN의 밀집 층을 제거하고 합성곱만 사용하여 이미지 크기 유지

다양한 응용 분야에서 활용 가능

```python
from tensorflow.keras.layers import Conv2DTranspose
upsample = Conv2DTranspose(filters=64, kernel_size=3, strides=2, padding='same')
```

이와 같은 방식으로 객체 탐지, 객체 추적, 시맨틱 분할 및 YOLO를 활용한 모델 구축이 가능함.
## 14.10.2 YOLO
빠르고 정확한 객체 탐지 모델

PASCAL VOC 데이터셋으로 학습

YOLOv3, YOLOv5, YOLO9000 등 다양한 변형 존재

```python
from tensorflow.keras.applications import YOLOv3

model = YOLOv3(weights='imagenet')
image = load_sample_images()["images"]
predictions = model.predict(image)
```
mAP (Mean Average Precision)

객체 탐지에서 널리 사용되는 평가 지표

AP 값을 IoU 기준으로 평균 내어 계산

## 14.11 객체 추적

물체의 위치를 시간에 따라 추적하는 기술

대표적인 기법: DeepSORT

```python
from deep_sort_realtime.deepsort_tracker import DeepSort
tracker = DeepSort()

# 프레임별 객체 탐지 후 추적 수행
for frame in video_frames:
    detections = detect_objects(frame)
    tracks = tracker.update_tracks(detections, frame=frame)
```

## 14.12 시맨틱 분할
픽셀 단위로 객체를 분류하는 기법

Fully Convolutional Network (FCN) 기반
```python
from tensorflow.keras.applications import DeepLabV3
model = DeepLabV3(weights='pascal_voc')
```

### 토론 주제
1. CNN 모델에 대해 책에서 언급했을 때 메모리 요구량이 크다는 점이 나왔다. 이러한 사실에 입각하여 CNN 모델을 훈련할 때 필요한 메모리 요구량을 줄이기 위한 방법은 무엇인지 더 자세하게 토론하고 싶었다.

2. 책에서는 인간의 시각적 인식에 대하여 간략하게 설명하였다. CNN이 인간의 시각 시스템을 어느 정도 모방한다고 하지만, 실제로 인간의 시각 인식과 CNN의 처리 방식에는 어떤 근본적인 차이점이 존재하는지 책의 내용보다 더 구체적으로 알아보고 싶었다.



