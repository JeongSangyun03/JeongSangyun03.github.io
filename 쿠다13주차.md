---
layout : post
title : "심화 13주차"
---


# 7장 추천 시스템 평가

## 7.1 3가지 평가 방법

![image](https://github.com/user-attachments/assets/b221974e-ecff-4f81-9e75-d1588d211bc4)

![image](https://github.com/user-attachments/assets/298235a8-b2a4-40fe-9575-974d0025181c)

## 7.2 오프라인 평가

### 7.2.1 모델 정밀도 평가

추천 시스템에서 모델의 주요 목적:

과거의 사용자 행동을 학습하고 미지의 사용자 행동에 대해 높은 정밀도로 예측을 수행하는 것

### 7.2.2 모델 밸리데이션

![image](https://github.com/user-attachments/assets/f0853442-b3ee-4990-9e8c-06ed1220370e)

밸리데이션: 미지의 데이터에 대한 일반화 성능을 검증하는 것

### 7.2.3 모델 튜닝

모델 튜닝: 예측 성능이 높아지도록 모델이 가진 파라미터를 조정하는 것

### 7.2.4 평가 지표

![image](https://github.com/user-attachments/assets/1cd35da9-0907-44c6-ba19-64a345170e9b)

MAE: 평균 절댓값 오차

MSE: 평균 제곱 오차

RMSE: 평균 제곱근 오차

Precision: 적합률

Recall: 재현율

F1-measure:

![image](https://github.com/user-attachments/assets/edfc4fe6-6d2d-4171-b2e9-232e5afbec35)

PR 곡선

![image](https://github.com/user-attachments/assets/1ffd91cd-4a76-4447-8a4c-7d443fecf92b)

![image](https://github.com/user-attachments/assets/18a72625-c993-447f-9fa4-eef18be36210)

MRR@K: 사용자 순위에 대해 최초의 적합 아이템이 순위에서 얼마나 상위에 위치하는지

![image](https://github.com/user-attachments/assets/5f696347-57d4-4241-8437-82e5d9bff1b4)

AP@K: 순위의 K번째까지에 대해 각 적합 아이템까지의 Precision을 평균한 값

![image](https://github.com/user-attachments/assets/a083a9a0-db18-495d-ae68-838083171ad2)

MAP@K: 중앙 평균 정밀도, AP를 각 사용자에 대해 평균한 값

![image](https://github.com/user-attachments/assets/be17b006-b4b8-4786-ba09-a9f32e4f3c36)

![image](https://github.com/user-attachments/assets/4e683880-07b8-4fdf-8cf8-028c2802fc47)

![image](https://github.com/user-attachments/assets/27d52a96-fc86-4895-a8c4-29703a03bf02)

nDCG: DCG라고 불리는 값을 순위가 이상적으로 나열됐을 때의 DCG로 나누는 것

![image](https://github.com/user-attachments/assets/f2aa585c-e851-4af9-a87d-be3228055f79)

![image](https://github.com/user-attachments/assets/ff2c000d-2645-44a2-b064-810f92dcb0ba)

카탈로그 커버리지: 실제로 추천된 아이템 집합 / 모든 아이템 집합

-> 인기 상품에 치우친 추천 검출

![image](https://github.com/user-attachments/assets/7d28c959-45d3-4aab-a0cb-342dcfd9324b)

사용자 커버리지: 실제로 추천이 수행된 사용자 집합 / 모든 사용자 집합

![image](https://github.com/user-attachments/assets/2319cb4b-48f9-42c7-a45f-95a606927731)

-> 콜드 스사트 문제 검출

신규성: 순위에 대한 추천 아이템이 정말로 새로운지

다양성: 순위의 각 아이템 간 유사도 거리의 평균값에 따라 정의, 유사도가 낮은 경우 다양성 상승

흥미로움: 순위에 대해 의외성을 가지면서도 유용한 아이템의 비율 측정

웹 검색에서 사용자의 행동 의도 3가지

![image](https://github.com/user-attachments/assets/0122d6a2-1585-4d07-a8d2-6eb2a2101e6d)

![image](https://github.com/user-attachments/assets/89eee27f-c3b3-4291-9364-fb268435cf0b)

ERR: 주목한 아이템의 순위에 위치를 추가해 해당 아이템보다 위에 있는 아이템으로부터 정의된 정지 확률

## 7.3 온라인 평가

### 7.3.1 A/B 테스트

무작위 비교 시험(RCT) 중 하나로 테스트 대상의 기능에 변경을 추가한 결과를 보여 주는 실험군과 변경하지 않은 결과를 보여주는 대조군으로 사용자를 나눈 후 평가를 수행하는 방법

![image](https://github.com/user-attachments/assets/391b1a91-a453-4c90-9f2f-82e8e7e5bada)

그룹 편향: 테스트 전부터 그룹에 차이가 있어 테스트가 올바르게 수행되지 않음

로그 혼합: 학습 데이터 부족 등의 원인으로 뒤섞이 행동 로그에 따라 학습할 때 발생

집계 기간: 특정 요율, 계절 조건에서 효과가 있거나 단/장기간에 따라 다를 때

OEC 지표: A/B 테스트의 성공과 실패를 최종적으로 판단하는 지표, 개선하고자 함

가드레일 지표: 저하되어서는 안 되는 제약 표시

지표 설계 방침

1. 감도: 이동 확률(얼마나 빈번하게 변동하는가), 통계력(효과에 변동이 있을 떄 그것을 얼마나 정확하게 특정할 수 있는가)

2. 신뢰성: 로그 사양과 실측값 간의 괴리가 적을수록 좋음

3. 효율성: 시간, 복잡성, 비용

4. 디버깅 가능성과 액션 가능성

5. 해석 가능성과 방향성

### 7.3.2 인터리빙

A/B 테스트에서 테스트 대상이 늘어남에 따라 테스트 사용자 수도 선형적으로 늘어나 평가 효율이 문제됨을 극복

사용자 그룹을 나누지 않고 평가 대상의 각 순위를 하나의 순위로 섞어 사용자에게 제시

뒤섞인 순위에 대한 클릭으로 원래 순위끼리 평가

멀티리빙: 3개 이상의 순위 섞어 평가하기

![image](https://github.com/user-attachments/assets/bc595557-0f89-4dfe-b9dd-a6e50500b729)

## 7.4 사용자 스터디를 통한 평가

### 7.4.1 조사 설계

참가자 선정 - 참가자 수 - 조사 시점 - 피험자 내 측정/피험자 간 측정

### 7.4.2 설문 조사 예

![image](https://github.com/user-attachments/assets/77da726b-ad38-449c-9e7b-f44d608a9af9)

